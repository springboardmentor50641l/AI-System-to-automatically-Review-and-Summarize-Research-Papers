# ğŸ“„ AI Research Paper Analysis System (Powered by LangGraph)

![Project Status](https://img.shields.io/badge/Status-Active-brightgreen) ![Python](https://img.shields.io/badge/Python-3.8%2B-blue) ![LangGraph](https://img.shields.io/badge/Orchestration-LangGraph-purple) ![Gemini](https://img.shields.io/badge/AI-Gemini%203.0-blueviolet)

An AI-powered web application that orchestrates advanced research paper analysis using **LangGraph** and **Google Gemini 3.0**. It allows users to search for papers via Semantic Scholar, upload documents, and generate deep comparative reports through a multi-step agentic workflow.

---

## ğŸš€ Project Overview

This project evolves traditional document analysis by using **Graph-based AI workflows**. Instead of simple text summarization, it employs a **LangGraph StateGraph** to:
1.  **Search & Acquire:** Fetch relevant academic papers automatically.
2.  **Individual Analysis:** Summarize each paper independently to extract methodology and metrics.
3.  **Comparative Synthesis:** aggregate insights to produce a structured comparative literature review.

It is designed for **Data Scientists**, **Researchers**, and **Students** who need to synthesize multiple papers quickly.

---

## ğŸ§  Key Features

* **ğŸ¤– LangGraph Orchestration:** Uses a stateful graph to manage the workflow between individual paper summarization and final comparative reporting.
* **ğŸŒ Automated Paper Search:** Integrated with the **Semantic Scholar API** to search and download open-access PDFs directly.
* **ğŸ“¤ Smart Upload System:** Supports PDF, DOCX, and text file uploads.
* **ğŸ” Deep Extraction:** Extracts core problems, methodologies, quantitative results, and limitations.
* **ğŸ“Š Comparative Reports:** Generates Markdown-formatted tables comparing multiple papers side-by-side.
* **ğŸ’¬ Natural Language Query:** (Coming Soon) Ask specific questions across the document set.

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
| :--- | :--- |
| **Orchestration** | **LangGraph**, LangChain |
| **LLM Engine** | **Google Gemini 3.0 Flash** |
| **Backend** | Python, Flask |
| **External APIs** | **Semantic Scholar API** |
| **Frontend** | HTML, CSS, JavaScript |
| **Processing** | PyPDF2, python-docx |

---

## ğŸ“‚ Project Directory Structure

```text
Final Project/
â”‚
â”œâ”€â”€ app.py                     # Main Flask app with LangGraph workflow
â”œâ”€â”€ requirements.txt           # Project dependencies
â”œâ”€â”€ .env                       # API Keys (Gemini & Semantic Scholar)
â”œâ”€â”€ README.md                  # Project documentation
â”‚
â”œâ”€â”€ papers/                    # Storage for uploaded/downloaded PDFs
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html             # Home & Upload
â”‚   â”œâ”€â”€ analysis.html          # Results visualization
â”‚   â””â”€â”€ query.html             # Chat interface
â”‚
â””â”€â”€ static/
    â”œâ”€â”€ css/style.css
    â””â”€â”€ js/script.js

```

---

## âš™ï¸ Prerequisites

Before running the project, ensure you have:

1. **Python 3.9+** (Recommended for modern LangChain/LangGraph support).
2. **Google Cloud API Key** (Access to Gemini models).
3. **Semantic Scholar API Key** (Optional, but recommended for higher rate limits).

---

## ğŸ“¦ Installation & Setup

### ğŸ”¹ Step 1: Clone the Repository

```bash
git clone <repository-url>
cd "Final Project"

```

### ğŸ”¹ Step 2: Create Virtual Environment

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux / Mac
source venv/bin/activate

```

### ğŸ”¹ Step 3: Install Dependencies

```bash
pip install -r requirements.txt

```

### ğŸ”¹ Step 4: Configure Environment Variables

Create a `.env` file in the root directory and add your keys:

```ini
# Required for Analysis
GEMINI_API_KEY=your_google_gemini_key_here

# Optional (for better Paper Search limits)
SEMANTIC_API_KEY=your_semantic_scholar_key_here

FLASK_DEBUG=True

```

---

## â–¶ï¸ Execution Steps

### ğŸ”¹ Step 5: Start the Application

```bash
python app.py

```

### ğŸ”¹ Step 6: Access the Interface

Open your browser and visit: `http://127.0.0.1:5000/`

---

## ğŸ”„ LangGraph Workflow Architecture

The application uses a **StateGraph** to ensure high-quality output. The workflow follows these nodes:

1. **Input State:** Raw text is loaded from multiple PDF/DOCX files.
2. **Node 1: Summarize (`summarize_individual`)**:
* The LLM iterates through every paper individually.
* It extracts strict structured data (Problem, Method, Metrics).
* *Self-Correction:* Includes JSON cleaning logic to handle Gemini 3.0 outputs.


3. **Node 2: Synthesize (`generate_comparison`)**:
* The graph passes the structured summaries to a second LLM call.
* The model generates a "Comparative Analysis" including an Executive Summary and Comparison Table.


4. **Output:** A rendered HTML report.

---

## ğŸ›¡ï¸ Error Handling

* **API Failures:** Graceful fallback if Semantic Scholar is down or API keys are invalid.
* **Content Cleaning:** Custom regex logic (`get_clean_content`) ensures clean Markdown output even if the model returns raw JSON.
* **File Parsing:** Robust handling for encrypted PDFs or corrupted files.

---

## ğŸ‘¨â€ğŸ’» Author

**Pranjal Upadhyay**

* *B.Tech CSE (AI & Data Science)*
* *Aspiring Data Scientist & AI Engineer*
