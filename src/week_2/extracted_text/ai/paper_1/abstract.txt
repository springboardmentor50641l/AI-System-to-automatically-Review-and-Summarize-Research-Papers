As AI systems become more capable, we would like to enlist their help to supervise
other AIs. We experiment with methods for training a harmless AI assistant through selfimprovement, without any human labels identifying harmful outputs. The only human
oversight is provided through a list of rules or principles, and so we refer to the method as
‘Constitutional AI’. The process involves both a supervised learning and a reinforcement
learning phase. In the supervised phase we sample from an initial model, then generate
self-critiques and revisions, and then finetune the original model on revised responses. In
the RL phase, we sample from the finetuned model, use a model to evaluate which of the
two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we
use ‘RL from AI Feedback’ (RLAIF). As a result we are able to train a harmless but nonevasive AI assistant that engages with harmful queries by explaining its objections to them.
Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the
human-judged performance and transparency of AI decision making. These methods make
it possible to control AI behavior more precisely and with far fewer human labels.
_∗_ Correspondence to: {yuntao,jared}@anthropic.com
Author contributions are detailed in 7.
**Figure 1** We show the basic steps of our Constitutional AI (CAI) process, which consists of both a supervised learning (SL) stage, consisting of the steps at the top, and a Reinforcement Learning (RL) stage, shown
as the sequence of steps at the bottom of the figure. Both the critiques and the AI feedback are steered by
a small set of principles drawn from a ‘constitution’. The supervised stage significantly improves the initial
model, and gives some control over the initial behavior at the start of the RL phase, addressing potential
exploration problems. The RL stage significantly improves performance and reliability.