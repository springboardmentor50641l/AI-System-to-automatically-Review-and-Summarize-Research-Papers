{
  "total_papers": 3,
  "extraction_date": "2026-01-29T22:01:09.622279",
  "papers": [
    {
      "file_name": "A_survey_on_Image_Data_Augmentation_for_Deep_Learn_20260129.pdf",
      "file_path": "C:\\Users\\ASUS\\Documents\\GitHub\\AI-System-to-automatically-Review-and-Summarize-Research-Papers\\data\\papers\\A_survey_on_Image_Data_Augmentation_for_Deep_Learn_20260129.pdf",
      "file_size": 4276196,
      "extraction_date": "2026-01-29T22:01:09.433171",
      "pdf_type": "text_based",
      "total_pages": 48,
      "total_chars": 135679,
      "total_words": 20564,
      "extraction_method": "pymupdf",
      "pages": [
        {
          "page_number": 1,
          "text": "A survey on Image Data Augmentation \nfor Deep Learning\nConnor Shorten* and Taghi M. Khoshgoftaar\nIntroduction\nDeep Learning models have made incredible progress in discriminative tasks. This has \nbeen fueled by the advancement of deep network architectures, powerful computation, \nand access to big data. Deep neural networks have been successfully applied to Computer Vision tasks such as image classification, object detection, and image segmentation thanks to the development of convolutional neural networks (CNNs). These neural \nnetworks utilize parameterized, sparsely connected kernels which preserve the spatial \ncharacteristics of images. Convolutional layers sequentially downsample the spatial \nresolution of images while expanding the depth of their feature maps. This series of \nconvolutional transformations can create much lower-dimensional and more useful representations of images than what could possibly be hand-crafted. The success of CNNs \nhas spiked interest and optimism in applying Deep Learning to Computer Vision tasks.\nAbstract \nDeep convolutional neural networks have performed remarkably well on many \nComputer Vision tasks. However, these networks are heavily reliant on big data to \navoid overfitting. Overfitting refers to the phenomenon when a network learns a \nfunction with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical \nimage analysis. This survey focuses on Data Augmentation, a data-space solution to \nthe problem of limited data. Data Augmentation encompasses a suite of techniques \nthat enhance the size and quality of training datasets such that better Deep Learning \nmodels can be built using them. The image augmentation algorithms discussed in this \nsurvey include geometric transformations, color space augmentations, kernel filters, \nmixing images, random erasing, feature space augmentation, adversarial training, \ngenerative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In \naddition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final \ndataset size, and curriculum learning. This survey will present existing methods for Data \nAugmentation, promising developments, and meta-level decisions for implementing \nData Augmentation. Readers will understand how Data Augmentation can improve \nthe performance of their models and expand limited datasets to take advantage of the \ncapabilities of big data.\nKeywords: Data Augmentation, Big data, Image data, Deep Learning, GANs\nOpen Access\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n( iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \n\n*Correspondence: \n \nDepartment of Computer \nand Electrical Engineering \nand Computer Science, \nFlorida Atlantic University, \nBoca Raton, USA",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 2,
          "text": "Page 2 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nThere are many branches of study that hope to improve current benchmarks by applying deep convolutional networks to Computer Vision tasks. Improving the generalization ability of these models is one of the most difficult challenges. Generalizability refers \nto the performance difference of a model when evaluated on previously seen data (training data) versus data it has never seen before (testing data). Models with poor generalizability have overfitted the training data. One way to discover overfitting is to plot the \ntraining and validation accuracy at each epoch during training. The graph below depicts \nwhat overfitting might look like when visualizing these accuracies over training epochs \n(Fig. 1).\nTo build useful Deep Learning models, the validation error must continue to decrease \nwith the training error. Data Augmentation is a very powerful method of achieving this. \nThe augmented data will represent a more comprehensive set of possible data points, \nthus minimizing the distance between the training and validation set, as well as any \nfuture testing sets.\nData Augmentation, the focus of this survey, is not the only technique that has been \ndeveloped to reduce overfitting. The following few paragraphs will introduce other solutions available to avoid overfitting in Deep Learning models. This listing is intended to \ngive readers a broader understanding of the context of Data Augmentation.\nMany other strategies for increasing generalization performance focus on the model’s \narchitecture itself. This has led to a sequence of progressively more complex architectures from AlexNet [1] to VGG-16 [2], ResNet [3], Inception-V3 [4], and DenseNet [5]. \nFunctional solutions such as dropout regularization, batch normalization, transfer learning, and pretraining have been developed to try to extend Deep Learning for application \non smaller datasets. A brief description of these overfitting solutions is provided below. \nA complete survey of regularization methods in Deep Learning has been compiled by \nKukacka et al. [6]. Knowledge of these overfitting solutions will inform readers about \nother existing tools, thus framing the high-level context of Data Augmentation and Deep \nLearning.\n• Dropout [7] is a regularization technique that zeros out the activation values of randomly chosen neurons during training. This constraint forces the network to learn \nmore robust features rather than relying on the predictive capability of a small subset \nof neurons in the network. Tompson et al. [8] extended this idea to convolutional \nFig. 1 The plot on the left shows an inflection point where the validation error starts to increase as the \ntraining rate continues to decrease. The increased training has caused the model to overfit to the training \ndata and perform poorly on the testing set relative to the training set. In contrast, the plot on the right shows \na model with the desired relationship between training and testing error",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 3,
          "text": "Page 3 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nnetworks with Spatial Dropout, which drops out entire feature maps rather than \nindividual neurons.\n• Batch normalization [9] is another regularization technique that normalizes the set \nof activations in a layer. Normalization works by subtracting the batch mean from \neach activation and dividing by the batch standard deviation. This normalization \ntechnique, along with standardization, is a standard technique in the preprocessing \nof pixel values.\n• Transfer Learning [10, 11] is another interesting paradigm to prevent overfitting. \nTransfer Learning works by training a network on a big dataset such as ImageNet \n[12] and then using those weights as the initial weights in a new classification task. \nTypically, just the weights in convolutional layers are copied, rather than the entire \nnetwork including fully-connected layers. This is very effective since many image \ndatasets share low-level spatial characteristics that are better learned with big data. \nUnderstanding the relationship between transferred data domains is an ongoing \nresearch task [13]. Yosinski et al. [14] find that transferability is negatively affected \nprimarily by the specialization of higher layer neurons and difficulties with splitting \nco-adapted neurons.\n• Pretraining [15] is conceptually very similar to transfer learning. In Pretraining, the \nnetwork architecture is defined and then trained on a big dataset such as ImageNet \n[12]. This differs from Transfer Learning because in Transfer Learning, the network \narchitecture such as VGG-16 [2] or ResNet [3] must be transferred as well as the \nweights. Pretraining enables the initialization of weights using big datasets, while still \nenabling flexibility in network architecture design.\n• One-shot and Zero-shot learning [16, 17] algorithms represent another paradigm for \nbuilding models with extremely limited data. One-shot learning is commonly used \nin facial recognition applications [18]. An approach to one-shot learning is the use of \nsiamese networks [19] that learn a distance function such that image classification is \npossible even if the network has only been trained on one or a few instances. Another \nvery popular approach to one-shot learning is the use of memory-augmented networks [20]. Zero-shot learning is a more extreme paradigm in which a network uses \ninput and output vector embeddings such as Word2Vec [21] or GloVe [22] to classify \nimages based on descriptive attributes.\nIn contrast to the techniques mentioned above, Data Augmentation approaches \noverfitting from the root of the problem, the training dataset. This is done under the \nassumption that more information can be extracted from the original dataset through \naugmentations. These augmentations artificially inflate the training dataset size by \neither data warping or oversampling. Data warping augmentations transform existing images such that their label is preserved. This encompasses augmentations such as \ngeometric and color transformations, random erasing, adversarial training, and neural \nstyle transfer. Oversampling augmentations create synthetic instances and add them to \nthe training set. This includes mixing images, feature space augmentations, and generative adversarial networks (GANs). Oversampling and Data Warping augmentations do \nnot form a mutually exclusive dichotomy. For example, GAN samples can be stacked \nwith random cropping to further inflate the dataset. Decisions around final dataset size,",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 4,
          "text": "Page 4 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \ntest-time augmentation, curriculum learning, and the impact of resolution are covered \nin this survey under the “Design considerations for image Data Augmentation” section. \nDescriptions of individual augmentation techniques will be enumerated in the “Image \nData Augmentation techniques” section. A quick taxonomy of the Data Augmentations \nis depicted below in Fig. 2.\nBefore discussing image augmentation techniques, it is useful to frame the context of \nthe problem and consider what makes image recognition such a difficult task in the first \nplace. In classic discriminative examples such as cat versus dog, the image recognition \nsoftware must overcome issues of viewpoint, lighting, occlusion, background, scale, and \nmore. The task of Data Augmentation is to bake these translational invariances into the \ndataset such that the resulting models will perform well despite these challenges.\nIt is a generally accepted notion that bigger datasets result in better Deep Learning \nmodels [23, 24]. However, assembling enormous datasets can be a very daunting task \ndue to the manual effort of collecting and labeling data. Limited datasets is an especially \nprevalent challenge in medical image analysis. Given big data, deep convolutional networks have been shown to be very powerful for medical image analysis tasks such as skin \nlesion classification as demonstrated by Esteva et al. [25]. This has inspired the use of \nCNNs on medical image analysis tasks [26] such as liver lesion classification, brain scan \nanalysis, continued research in skin lesion classification, and more. Many of the images \nstudied are derived from computerized tomography (CT) and magnetic resonance imaging (MRI) scans, both of which are expensive and labor-intensive to collect. It is especially difficult to build big medical image datasets due to the rarity of diseases, patient \nFig. 2 A taxonomy of image data augmentations covered; the colored lines in the figure depict which data \naugmentation method the corresponding meta-learning scheme uses, for example, meta-learning using \nNeural Style Transfer is covered in neural augmentation [36]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 5,
          "text": "Page 5 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nprivacy, the requirement of medical experts for labeling, and the expense and manual \neffort needed to conduct medical imaging processes. These obstacles have led to many \nstudies on image Data Augmentation, especially GAN-based oversampling, from the \napplication perspective of medical image classification.\nMany studies on the effectiveness of Data Augmentation utilize popular academic \nimage datasets to benchmark results. These datasets include MNIST hand written digit \nrecognition, CIFAR-10/100, ImageNet, tiny-imagenet-200, SVHN (street view house \nnumbers), Caltech-101/256, MIT places, MIT-Adobe 5K dataset, Pascal VOC, and Stanford Cars. The datasets most frequently discussed are CIFAR-10, CIFAR-100, and ImageNet. The expansion of open-source datasets has given researchers a wide variety of \ncases to compare performance results of Data Augmentation techniques. Most of these \ndatasets such as ImageNet would be classified as big data. Many experiments constrain \nthemselves to a subset of the dataset to simulate limited data problems.\nIn addition to our focus on limited datasets, we will also consider the problem of class \nimbalance and how Data Augmentation can be a useful oversampling solution. Class \nimbalance describes a dataset with a skewed ratio of majority to minority samples. Leevy \net al. [27] describe many of the existing solutions to high-class imbalance across data \ntypes. Our survey will show how class-balancing oversampling in image data can be \ndone with Data Augmentation.\nMany aspects of Deep Learning and neural network models draw comparisons with \nhuman intelligence. For example, a human intelligence anecdote of transfer learning is \nillustrated in learning music. If two people are trying to learn how to play the guitar, and \none already knows how to play the piano, it seems likely that the piano-player will learn \nto play the guitar faster. Analogous to learning music, a model that can classify ImageNet images will likely perform better on CIFAR-10 images than a model with random \nweights.\nData Augmentation is similar to imagination or dreaming. Humans imagine different scenarios based on experience. Imagination helps us gain a better understanding \nof our world. Data Augmentation methods such as GANs and Neural Style Transfer \ncan ‘imagine’ alterations to images such that they have a better understanding of them. \nThe remainder of the paper is organized as follows: A brief “Background” is provided \nto give readers a historical context of Data Augmentation and Deep Learning. “Image \nData Augmentation techniques” discusses each image augmentation technique in detail \nalong with experimental results. “Design considerations for image Data Augmentation” \ndiscusses additional characteristics of augmentation such as test-time augmentation \nand the impact of image resolution. The paper concludes with a “Discussion” of the presented material, areas of “Future work”, and “Conclusion”.\nBackground\nImage augmentation in the form of data warping can be found in LeNet-5 [28]. This \nwas one of the first applications of CNNs on handwritten digit classification. Data \naugmentation has also been investigated in oversampling applications. Oversampling \nis a technique used to re-sample imbalanced class distributions such that the model \nis not overly biased towards labeling instances as the majority class type. Random \nOversampling (ROS) is a naive approach which duplicates images randomly from the",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 6,
          "text": "Page 6 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nminority class until a desired class ratio is achieved. Intelligent oversampling techniques date back to SMOTE (Synthetic Minority Over-sampling Technique), which \nwas developed by Chawla et al. [29]. SMOTE and the extension of Borderline-SMOTE \n[30] create new instances by interpolating new points from existing instances via \nk-Nearest Neighbors. The primary focus of this technique was to alleviate problems \ndue to class imbalance, and SMOTE was primarily used for tabular and vector data.\nThe AlexNet CNN architecture developed by Krizhevsky et al. [1] revolutionized \nimage classification by applying convolutional networks to the ImageNet dataset. \nData Augmentation is used in their experiments to increase the dataset size by a \nmagnitude of 2048. This is done by randomly cropping 224 × 224 patches from the \noriginal images, flipping them horizontally, and changing the intensity of the RGB \nchannels using PCA color augmentation. This Data Augmentation helped reduce \noverfitting when training a deep neural network. The authors claim that their augmentations reduced the error rate of the model by over 1%.\nSince then, GANs were introduced in 2014 [31], Neural Style Transfer [32] in 2015, \nand Neural Architecture Search (NAS) [33] in 2017. Various works on GAN extensions such as DCGANs, CycleGANs and Progressively-Growing GANs [34] were published in 2015, 2017, and 2017, respectively. Neural Style Transfer was sped up with \nthe development of Perceptual Losses by Johnson et al. [35] in 2016. Applying metalearning concepts from NAS to Data Augmentation has become increasingly popular \nwith works such as Neural Augmentation [36], Smart Augmentation [37], and AutoAugment [38] published in 2017, 2017, and 2018, respectively.\nApplying Deep Learning to medical imaging has been a popular application for \nCNNs since they became so popular in 2012. Deep Learning and medical imaging \nbecame increasingly popular with the demonstration of dermatologist-level skin cancer detection by Esteva et al. [25] in 2017.\nThe use of GANs in medical imaging is well documented in a survey by Yi et al. \n[39]. This survey covers the use of GANs in reconstruction such as CT denoising [40], \naccelerated magnetic resonance imaging [41], PET denoising [42], and the application of super-resolution GANs in retinal vasculature segmentation [43]. Additionally, \nYi et al. [39] cover the use of GAN image synthesis in medical imaging applications \nsuch as brain MRI synthesis [44, 45], lung cancer diagnosis [46], high-resolution skin \nlesion synthesis [47], and chest x-ray abnormality classification [48]. GAN-based \nimage synthesis Data Augmentation was used by Frid-Adar et al. [49] in 2018 for liver \nlesion classification. This improved classification performance from 78.6% sensitivity and 88.4% specificity using classic augmentations to 85.7% sensitivity and 92.4% \nspecificity using GAN-based Data Augmentation.\nMost of the augmentations covered focus on improving Image Recognition models. Image Recognition is when a model predicts an output label such as ‘dog’ or ‘cat’ \ngiven an input image.\nHowever, it is possible to extend results from image recognition to other Computer \nVision tasks such as Object Detection led by the algorithms YOLO [50], R-CNN [51], \nfast R-CNN [52], and faster R-CNN [53] or Semantic Segmentation [54] including \nalgorithms such as U-Net [55].",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 7,
          "text": "Page 7 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nImage Data Augmentation techniques\nThe earliest demonstrations showing the effectiveness of Data Augmentations come \nfrom simple transformations such as horizontal flipping, color space augmentations, and \nrandom cropping. These transformations encode many of the invariances discussed earlier that present challenges to image recognition tasks. The augmentations listed in this \nsurvey are geometric transformations, color space transformations, kernel filters, mixing \nimages, random erasing, feature space augmentation, adversarial training, GAN-based \naugmentation, neural style transfer, and meta-learning schemes. This section will explain \nhow each augmentation algorithm works, report experimental results, and discuss disadvantages of the augmentation technique.\nData Augmentations based on basic image manipulations\nGeometric transformations\nThis section describes different augmentations based on geometric transformations and \nmany other image processing functions. The class of augmentations discussed below \ncould be characterized by their ease of implementation. Understanding these transformations will provide a useful base for further investigation into Data Augmentation \ntechniques.\nWe will also describe the different geometric augmentations in the context of their \n‘safety’ of application. The safety of a Data Augmentation method refers to its likelihood \nof preserving the label post-transformation. For example, rotations and flips are generally safe on ImageNet challenges such as cat versus dog, but not safe for digit recognition tasks such as 6 versus 9. A non-label preserving transformation could potentially \nstrengthen the model’s ability to output a response indicating that it is not confident \nabout its prediction. However, achieving this would require refined labels [56] post-augmentation. If the label of the image after a non-label preserving transformation is something like [0.5 0.5], the model could learn more robust confidence predictions. However, \nconstructing refined labels for every non-safe Data Augmentation is a computationally \nexpensive process.\nDue to the challenge of constructing refined labels for post-augmented data, it is \nimportant to consider the ‘safety’ of an augmentation. This is somewhat domain dependent, providing a challenge for developing generalizable augmentation policies, (see AutoAugment [38] for further exploration into finding generalizable augmentations). There \nis no image processing function that cannot result in a label changing transformation \nat some distortion magnitude. This demonstrates the data-specific design of augmentations and the challenge of developing generalizable augmentation policies. This is an \nimportant consideration with respect to the geometric augmentations listed below.\nFlipping\nHorizontal axis flipping is much more common than flipping the vertical axis. This augmentation is one of the easiest to implement and has proven useful on datasets such \nas CIFAR-10 and ImageNet. On datasets involving text recognition such as MNIST or \nSVHN, this is not a label-preserving transformation.",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 8,
          "text": "Page 8 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nColor space\nDigital image data is usually encoded as a tensor of the dimension (height × width × color \nchannels). Performing augmentations in the color channels space is another strategy \nthat is very practical to implement. Very simple color augmentations include isolating \na single color channel such as R, G, or B. An image can be quickly converted into its \nrepresentation in one color channel by isolating that matrix and adding 2 zero matrices \nfrom the other color channels. Additionally, the RGB values can be easily manipulated \nwith simple matrix operations to increase or decrease the brightness of the image. More \nadvanced color augmentations come from deriving a color histogram describing the \nimage. Changing the intensity values in these histograms results in lighting alterations \nsuch as what is used in photo editing applications.\nCropping\nCropping images can be used as a practical processing step for image data with mixed \nheight and width dimensions by cropping a central patch of each image. Additionally, \nrandom cropping can also be used to provide an effect very similar to translations. \nThe contrast between random cropping and translations is that cropping will reduce \nthe size of the input such as (256,256) → (224, 224), whereas translations preserve the \nspatial dimensions of the image. Depending on the reduction threshold chosen for \ncropping, this might not be a label-preserving transformation.\nRotation\nRotation augmentations are done by rotating the image right or left on an axis \nbetween 1° and 359°. The safety of rotation augmentations is heavily determined by \nthe rotation degree parameter. Slight rotations such as between 1 and 20 or − 1 to \n− 20 could be useful on digit recognition tasks such as MNIST, but as the rotation \ndegree increases, the label of the data is no longer preserved post-transformation.\nTranslation\nShifting images left, right, up, or down can be a very useful transformation to avoid \npositional bias in the data. For example, if all the images in a dataset are centered, \nwhich is common in face recognition datasets, this would require the model to be \ntested on perfectly centered images as well. As the original image is translated in a \ndirection, the remaining space can be filled with either a constant value such as 0 s or \n255 s, or it can be filled with random or Gaussian noise. This padding preserves the \nspatial dimensions of the image post-augmentation.\nNoise injection\nNoise injection consists of injecting a matrix of random values usually drawn from a \nGaussian distribution. Noise injection is tested by Moreno-Barea et al. [57] on nine \ndatasets from the UCI repository [58]. Adding noise to images can help CNNs learn \nmore robust features.\nGeometric transformations are very good solutions for positional biases present in \nthe training data. There are many potential sources of bias that could separate the",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 9,
          "text": "Page 9 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \ndistribution of the training data from the testing data. If positional biases are present, such as in a facial recognition dataset where every face is perfectly centered in \nthe frame, geometric transformations are a great solution. In addition to their powerful ability to overcome positional biases, geometric transformations are also useful because they are easily implemented. There are many imaging processing libraries \nthat make operations such as horizontal flipping and rotation painless to get started \nwith. Some of the disadvantages of geometric transformations include additional \nmemory, transformation compute costs, and additional training time. Some geometric transformations such as translation or random cropping must be manually \nobserved to make sure they have not altered the label of the image. Finally, in many of \nthe application domains covered such as medical image analysis, the biases distancing \nthe training data from the testing data are more complex than positional and translational variances. Therefore, the scope of where and when geometric transformations \ncan be applied is relatively limited.\nColor space transformations\nImage data is encoded into 3 stacked matrices, each of size height × width. These matrices represent pixel values for an individual RGB color value. Lighting biases are amongst \nthe most frequently occurring challenges to image recognition problems. Therefore, the \neffectiveness of color space transformations, also known as photometric transformations, is fairly intuitive to conceptualize. A quick fix to overly bright or dark images is to \nloop through the images and decrease or increase the pixel values by a constant value. \nAnother quick color space manipulation is to splice out individual RGB color matrices. \nAnother transformation consists of restricting pixel values to a certain min or max value. \nThe intrinsic representation of color in digital images lends itself to many strategies of \naugmentation.\nColor space transformations can also be derived from image-editing apps. An image’s \npixel values in each RGB color channel is aggregated to form a color histogram. This histogram can be manipulated to apply filters that change the color space characteristics of \nan image.\nThere is a lot of freedom for creativity with color space augmentations. Altering the \ncolor distribution of images can be a great solution to lighting challenges faced by testing \ndata (Figs. 3, 4).\nImage datasets can be simplified in representation by converting the RGB matrices into a single grayscale image. This results in smaller images, height × width × 1, \nFig. 3 Examples of Color Augmentations provided by Mikolajczyk and Grochowski [72] in the domain of \nmelanoma classification",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 10,
          "text": "Page 10 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nresulting in faster computation. However, this has been shown to reduce performance accuracy. Chatifled et  al. [59] found a ~ 3% classification accuracy drop \nbetween grayscale and RGB images with their experiments on ImageNet [12] and \nthe PASCAL [60] VOC dataset. In addition to RGB versus grayscale images, there \nare many other ways of representing digital color such as HSV (Hue, Saturation, and \nValue). Jurio et al. [61] explore the performance of Image Segmentation on many different color space representations from RGB to YUV, CMY, and HSV.\nSimilar to geometric transformations, a disadvantage of color space transformations is increased memory, transformation costs, and training time. Additionally, \ncolor transformations may discard important color information and thus are not \nalways a label-preserving transformation. For example, when decreasing the pixel \nvalues of an image to simulate a darker environment, it may become impossible to \nsee the objects in the image. Another indirect example of non-label preserving color \ntransformations is in Image Sentiment Analysis [62]. In this application, CNNs try \nto visually predict the sentiment score of an image such as: highly negative, negative, neutral, positive, or highly positive. One indicator of a negative/highly negative \nimage is the presence of blood. The dark red color of blood is a key component to \ndistinguish blood from water or paint. If color space transforms repeatedly change \nthe color space such that the model cannot recognize red blood from green paint, \nthe model will perform poorly on Image Sentiment Analysis. In effect, color space \ntransformations will eliminate color biases present in the dataset in favor of spatial characteristics. However, for some tasks, color is a very important distinctive \nfeature.\nFig. 4 Examples of color augmentations tested by Wu et al. [127]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 11,
          "text": "Page 11 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nGeometric versus photometric transformations\nTaylor and Nitschke [63] provide a comparative study on the effectiveness of geometric \nand photometric (color space) transformations. The geometric transformations studied were flipping, − 30° to 30° rotations, and cropping. The color space transformations \nstudied were color jittering, (random color manipulation), edge enhancement, and PCA. \nThey tested these augmentations with 4-fold cross-validation on the Caltech101 dataset \nfiltered to 8421 images of size 256 × 256 (Table 1).\nKernel filters\nKernel filters are a very popular technique in image processing to sharpen and blur \nimages. These filters work by sliding an n × n matrix across an image with either a Gaussian blur filter, which will result in a blurrier image, or a high contrast vertical or horizontal edge filter which will result in a sharper image along edges. Intuitively, blurring \nimages for Data Augmentation could lead to higher resistance to motion blur during \ntesting. Additionally, sharpening images for Data Augmentation could result in encapsulating more details about objects of interest.\nSharpening and blurring are some of the classical ways of applying kernel filters to \nimages. Kang et  al. [64] experiment with a unique kernel filter that randomly swaps \nthe pixel values in an n × n sliding window. They call this augmentation technique \nPatchShuffle Regularization. Experimenting across different filter sizes and probabilities \nof shuffling the pixels at each step, they demonstrate the effectiveness of this by achieving a 5.66% error rate on CIFAR-10 compared to an error rate of 6.33% achieved without the use of PatchShuffle Regularization. The hyperparameter settings that achieved \nthis consisted of 2 × 2 filters and a 0.05 probability of swapping. These experiments were \ndone using the ResNet [3] CNN architecture (Figs. 5, 6).\nKernel filters are a relatively unexplored area for Data Augmentation. A disadvantage \nof this technique is that it is very similar to the internal mechanisms of CNNs. CNNs \nhave parametric kernels that learn the optimal way to represent images layer-by-layer. \nFor example, something like PatchShuffle Regularization could be implemented with a \nconvolution layer. This could be achieved by modifying the standard convolution layer \nparameters such that the padding parameters preserve spatial resolution and the subsequent activation layer keeps pixel values between 0 and 255, in contrast to something \nlike a sigmoid activation which maps pixels to values between 0 and 1. Therefore kernel \nTable 1 Results of Taylor and Nitschke’s Data Augmentation experiments on Caltech101 \n[63]\nTheir results find that the cropping geometric transformation results in the most accurate classifier\nThe italic value denote high performance according to the comparative metrics\nTop-1 accuracy (%)\nTop-5 accuracy (%)\nBaseline\n48.13 ± 0.42\n64.50 ± 0.65\nFlipping\n49.73 ± 1.13\n67.36 ± 138\nRotating\n50.80 ± 0.63\n69.41 ± 0.48\nCropping\n61.95 + 1.01\n79.10 ± 0.80\nColor Jittering\n49.57 ± 0.53\n67.18 ± 0.42\nEdge Enhancement\n49.29 + 1.16\n66.49 + 0.84\nFancy PCA\n49.41 ± 0.84\n67.54 ± 1.01",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 12,
          "text": "Page 12 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nfilters can be better implemented as a layer of the network rather than as an addition to \nthe dataset through Data Augmentation.\nMixing images\nMixing images together by averaging their pixel values is a very counterintuitive \napproach to Data Augmentation. The images produced by doing this will not look like a \nuseful transformation to a human observer. However, Ionue [65] demonstrated how the \npairing of samples could be developed into an effective augmentation strategy. In this \nexperiment, two images are randomly cropped from 256 × 256 to 224 × 224 and randomly flipped horizontally. These images are then mixed by averaging the pixel values \nfor each of the RGB channels. This results in a mixed image which is used to train a classification model. The label assigned to the new image is the same as the first randomly \nselected image (Fig. 7).\nOn the CIFAR-10 dataset, Ionue reported a reduction in error rate from 8.22 to 6.93% \nwhen using the SamplePairing Data Augmentation technique. The researcher found \neven better results when testing a reduced size dataset, reducing CIFAR-10 to 1000 total \nsamples with 100 in each class. With the reduced size dataset, SamplePairing resulted in \nan error rate reduction from 43.1 to 31.0%. The reduced CIFAR-10 results demonstrate \nthe usefulness of the SamplePairing technique in limited data applications (Fig. 8).\nAnother detail found in the study is that better results were obtained when mixing \nimages from the entire training set rather than from instances exclusively belonging \nto the same class. Starting from a training set of size N, SamplePairing produces a \ndataset of size N2 + N. In addition, Sample Pairing can be stacked on top of other \naugmentation techniques. For example, if using the augmentations demonstrated in \nFig. 5 Examples of applying the PatchShuffle regularization technique [64]\nFig. 6 Pixels in a n × n window are randomly shifted with a probability parameter p",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 2
        },
        {
          "page_number": 13,
          "text": "Page 13 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nthe AlexNet paper by Krizhevsky et al. [1], the 2048 × dataset increase can be further \nexpanded to (2048 × N)2.\nThe concept of mixing images in an unintuitive way was further investigated by \nSummers and Dinneen [66]. They looked at using non-linear methods to combine \nimages into new training instances. All of the methods they used resulted in better \nperformance compared to the baseline models (Fig. 9).\nAmongst these non-linear augmentations tested, the best technique resulted in a \nreduction from 5.4 to 3.8% error on CIFAR-10 and 23.6% to 19.7% on CIFAR-100. \nIn like manner, Liang et al. [67] used GANs to produce mixed images. They found \nthat the inclusion of mixed images in the training data reduced training time and \nincreased the diversity of GAN-samples. Takahashi and Matsubara [68] experiment \nFig. 7 SamplePairing augmentation strategy [65]\nFig. 8 Results on the reduced CIFAR-10 dataset. Experimental results demonstrated with respect to sampling \npools for image mixing [65]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 2
        },
        {
          "page_number": 14,
          "text": "Page 14 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nwith another approach to mixing images that randomly crops images and concatenates the croppings together to form new images as depicted below. The results of \ntheir technique, as well as SamplePairing and mixup augmentation, demonstrate \nthe sometimes unreasonable effectiveness of big data with Deep Learning models \n(Fig. 10).\nAn obvious disadvantage of this technique is that it makes little sense from a human \nperspective. The performance boost found from mixing images is very difficult to \nunderstand or explain. One possible explanation for this is that the increased dataset \nsize results in more robust representations of low-level characteristics such as lines and \nedges. Testing the performance of this in comparisons to transfer learning and pretraining methods is an interesting area for future work. Transfer learning and pretraining are \nother techniques that learn low-level characteristics in CNNs. Additionally, it will be \nFig. 9 Non-linearly mixing images [66]\nFig. 10 Mixing images through random image cropping and patching [68]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 2
        },
        {
          "page_number": 15,
          "text": "Page 15 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \ninteresting to see how the performance changes if we partition the training data such \nthat the first 100 epochs are trained with original and mixed images and the last 50 with \noriginal images only. These kinds of strategies are discussed further in Design Considerations of Data Augmentation with respect to curriculum learning [69]. Additionally, \nthe paper will cover a meta-learning technique developed by Lemley et al. [37] that uses \na neural network to learn an optimal mixing of images.\nRandom erasing\nRandom erasing [70] is another interesting Data Augmentation technique developed \nby Zhong et al. Inspired by the mechanisms of dropout regularization, random erasing \ncan be seen as analogous to dropout except in the input data space rather than embedded into the network architecture. This technique was specifically designed to combat \nimage recognition challenges due to occlusion. Occlusion refers to when some parts of \nthe object are unclear. Random erasing will stop this by forcing the model to learn more \ndescriptive features about an image, preventing it from overfitting to a certain visual feature in the image. Aside from the visual challenge of occlusion, in particular, random \nerasing is a promising technique to guarantee a network pays attention to the entire \nimage, rather than just a subset of it.\nRandom erasing works by randomly selecting an n × m patch of an image and masking \nit with either 0 s, 255 s, mean pixel values, or random values. On the CIFAR-10 dataset \nthis resulted in an error rate reduction from 5.17 to 4.31%. The best patch fill method \nwas found to be random values. The fill method and size of the masks are the only \nparameters that need to be hand-designed during implementation (Figs. 11, 12).\nRandom erasing is a Data Augmentation method that seeks to directly prevent overfitting by altering the input space. By removing certain input patches, the model is forced \nto find other descriptive characteristics. This augmentation method can also be stacked \non top of other augmentation techniques such as horizontal flipping or color filters. Random erasing produced one of the highest accuracies on the CIFAR-10 dataset. DeVries \nand Taylor [71] conducted a similar study called Cutout Regularization. Like the random \nerasing study, they experimented with randomly masking regions of the image (Table 2).\nMikolajcyzk and Grochowski [72] presented an interesting idea to combine random \nerasing with GANs designed for image inpainting. Image inpainting describes the task of \nFig. 11 Example of random erasing on image recognition tasks [70]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 16,
          "text": "Page 16 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nfilling in a missing piece of an image. Using a diverse collection of GAN inpainters, the \nrandom erasing augmentation could seed very interesting extrapolations. It will be interesting to see if better results can be achieved by erasing different shaped patches such \nas circles rather than n × m rectangles. An extension of this will be to parameterize the \ngeometries of random erased patches and learn an optimal erasing configuration.\nA disadvantage to random erasing is that it will not always be a label-preserving transformation. In handwritten digit recognition, if the top part of an ‘8’ is randomly cropped \nout, it is not any different from a ‘6’. In many fine-grained tasks such as the Stanford Cars \ndataset [73], randomly erasing sections of the image (logo, etc.) may make the car brand \nunrecognizable. Therefore, some manual intervention may be necessary depending on \nthe dataset and task.\nA note on combining augmentations\nOf the augmentations discussed, geometric transformations, color space transformations, kernel filters, mixing images, and random erasing, nearly all of these transformations come with an associated distortion magnitude parameter as well. This parameter \nencodes the distortional difference between a 45° rotation and a 30° rotation. With a \nlarge list of potential augmentations and a mostly continuous space of magnitudes, it is \neasy to conceptualize the enormous size of the augmentation search space. Combining \naugmentations such as cropping, flipping, color shifts, and random erasing can result in \nFig. 12 Example of random erasing on object detection tasks [70]\nTable 2 Results of  Cutout Regularization [104], plus  denotes using traditional \naugmentation methods, horizontal flipping and cropping\nA 2.56% error rate is obtained on CIFAR-10 using cutout and traditional augmentation methods\nThe italic value denote high performance according to the comparative metrics\nMethod\nC10\nC10+\nC100\nC100+\nResNetl8 [5]\n10.63 ± 0.26\n4.72 ± 0.21\n36.68 ± 0.57\n22.46 ± 0.31\n-\nResNet18 + cutout\n9.31 ± 0.18\n3.99 ± 0.13\n34.98 ± 0.29\n21.96 ± 0.24\n-\nWideResNet [21]\n6.97 ± 0.22\n3.87 ± 0.08\n26.06 ± 0.22\n18.8 ± 0.08\n1.60 ± 0.05\nWideResNet + cutout\n5.54 ± 0.08\n3.08 ± 0.16\n23.94 ± 0.15\n18.41 ± 0.27\n1.30 ± 0.03\nShake-shake regularization [4]\n-\n2.86\n-\n15.85\n-\nShake-shake regularization + cutout\n-\n2.56 ± 0.07\n-\n15.20 ± 0.21\n-",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 17,
          "text": "Page 17 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nmassively inflated dataset sizes. However, this is not guaranteed to be advantageous. In \ndomains with very limited data, this could result in further overfitting. Therefore, it is \nimportant to consider search algorithms for deriving an optimal subset of augmented \ndata to train Deep Learning models with. More on this topic will be discussed in Design \nConsiderations of Data Augmentation.\nData Augmentations based on Deep Learning\nFeature space augmentation\nAll of the augmentation methods discussed above are applied to images in the input \nspace. Neural networks are incredibly powerful at mapping high-dimensional inputs into \nlower-dimensional representations. These networks can map images to binary classes \nor to n × 1 vectors in flattened layers. The sequential processing of neural networks can \nbe manipulated such that the intermediate representations can be separated from the \nnetwork as a whole. The lower-dimensional representations of image data in fully-connected layers can be extracted and isolated. Konno and Iwazume [74] find a performance \nboost on CIFAR-100 from 66 to 73% accuracy by manipulating the modularity of neural \nnetworks to isolate and refine individual layers after training. Lower-dimensional representations found in high-level layers of a CNN are known as the feature space. DeVries \nand Taylor [75] presented an interesting paper discussing augmentation in this feature \nspace. This opens up opportunities for many vector operations for Data Augmentation.\nSMOTE is a popular augmentation used to alleviate problems with class imbalance. \nThis technique is applied to the feature space by joining the k nearest neighbors to form \nnew instances. DeVries and Taylor discuss adding noise, interpolating, and extrapolating \nas common forms of feature space augmentation (Figs. 13, 14).\nFig. 13 Architecture diagram of the feature space augmentation framework presented by DeVries and Taylor \n[75]\nFig. 14 Examples of interpolated instances in the feature space on the handwritten  character [75]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 2
        },
        {
          "page_number": 18,
          "text": "Page 18 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nThe use of auto-encoders is especially useful for performing feature space augmentations on data. Autoencoders work by having one half of the network, the encoder, \nmap images into low-dimensional vector representations such that the other half of the \nnetwork, the decoder, can reconstruct these vectors back into the original image. This \nencoded representation is used for feature space augmentation.\nDeVries and Taylor [75] tested their feature space augmentation technique by extrapolating between the 3 nearest neighbors per sample to generate new data and compared \ntheir results against extrapolating in the input space and using affine transformations in \nthe input space (Table 3).\nFeature space augmentations can be implemented with auto-encoders if it is necessary \nto reconstruct the new instances back into input space. It is also possible to do feature \nspace augmentation solely by isolating vector representations from a CNN. This is done \nby cutting off the output layer of the network, such that the output is a low-dimensional \nvector rather than a class label. Vector representations are then found by training a CNN \nand then passing the training set through the truncated CNN. These vector representations can be used to train any machine learning model from Naive Bayes, Support Vector Machine, or back to a fully-connected multilayer network. The effectiveness of this \ntechnique is a subject for future work.\nA disadvantage of feature space augmentation is that it is very difficult to interpret the \nvector data. It is possible to recover the new vectors into images using an auto-encoder \nnetwork; however, this requires copying the entire encoding part of the CNN being \ntrained. For deep CNNs, this results in massive auto-encoders which are very difficult \nand time-consuming to train. Finally, Wong et al. [76] find that when it is possible to \ntransform images in the data-space, data-space augmentation will outperform feature \nspace augmentation.\nAdversarial training\nOne of the solutions to search the space of possible augmentations is adversarial \ntraining. Adversarial training is a framework for using two or more networks with \ncontrasting objectives encoded in their loss functions. This section will discuss using \nadversarial training as a search algorithm as well as the phenomenon of adversarial \nattacking. Adversarial attacking consists of a rival network that learns augmentations \nto images that result in misclassifications in its rival classification network. These \nadversarial attacks, constrained to noise injections, have been surprisingly successful \nfrom the perspective of the adversarial network. This is surprising because it completely defies intuition about how these models represent images. The adversarial \nTable 3 Performance results of the experiment with feature vs. input space extrapolation \non MNIST and CIFAR-10 [75]\nThe italic value denote high performance according to the comparative metrics\nModel\nCIFAR-10\nBaseline\n1.093 ± 0.057\n30.65 ± 0.27\nBaseline + input space affine transformations\n1.477 ± 0.068\n-\nBaseline + input space extrapolation\n1.010 ± 0.065\n-\nBaseline + feature space extrapolation\n0.950 ± 0.036\n29.24 ± 0.27",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 19,
          "text": "Page 19 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nattacks demonstrate that representations of images are much less robust than what \nmight have been expected. This is well demonstrated by Moosavi-Dezfooli et al. [77] \nusing DeepFool, a network that finds the minimum possible noise injection needed \nto cause a misclassification with high confidence. Su et al. [78] show that 70.97% of \nimages can be misclassified by changing just one pixel. Zajac et al. [79] cause misclassifications with adversarial attacks limited to the border of images. The success of \nadversarial attacks is especially exaggerated as the resolution of images increases.\nAdversarial attacking can be targeted or untargeted, referring to the deliberation \nin which the adversarial network is trying to cause misclassifications. Adversarial \nattacks can help to illustrate weak decision boundaries better than standard classification metrics can.\nIn addition to serving as an evaluation metric, defense to adversarial attacks, adversarial training can be an effective method for searching for augmentations.\nBy constraining the set of augmentations and distortions available to an adversarial \nnetwork, it can learn to produce augmentations that result in misclassifications, thus \nforming an effective search algorithm. These augmentations are valuable for strengthening weak spots in the classification model. Therefore, adversarial training can be \nan effective search technique for Data Augmentation. This is in heavy contrast to the \ntraditional augmentation techniques described previously. Adversarial augmentations \nmay not represent examples likely to occur in the test set, but they can improve weak \nspots in the learned decision boundary.\nEngstrom et  al. [80] showed that simple transformations such as rotations and \ntranslations can easily cause misclassifications by deep CNN models. The worst out \nof the random transformations reduced the accuracy of MNIST by 26%, CIFAR10 by \n72% and ImageNet (Top 1) by 28%. Goodfellow et al. [81] generate adversarial examples to improve performance on the MNIST classification task. Using a technique for \ngenerating adversarial examples known as the “fast gradient sign method”, a maxout \nnetwork [82] misclassified 89.4% of adversarial examples with an average confidence \nof 97.6%. This test is done on the MNIST dataset. With adversarial training, the error \nrate of adversarial examples fell from 89.4% to 17.9% (Fig. 15).\nLi et al. [83] experiment with a novel adversarial training approach and compare the \nperformance on original testing data and adversarial examples. The results displayed \nFig. 15 Adversarial misclassification example [81]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 20,
          "text": "Page 20 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nbelow show how anticipation of adversarial attacks in the training process can dramatically reduce the success of attacks.\nAs shown in Table 4, the adversarial training in their experiment did not improve the \ntest accuracy. However, it does significantly improve the test accuracy of adversarial \nexamples. Adversarial defense is a very interesting subject for evaluating security and \nrobustness of Deep Learning models. Improving on the Fast Gradient Sign Method, \nDeepFool, developed by Moosavi-Dezfooli et al. [77], uses a neural network to find the \nsmallest possible noise perturbation that causes misclassifications.\nAnother interesting framework that could be used in an adversarial training context is \nto have an adversary change the labels of training data. Xie et al. [84] presented DisturbLabel, a regularization technique that randomly replaces labels at each iteration. This \nis a rare example of adding noise to the loss layer, whereas most of the other augmentation methods discussed add noise into the input or hidden representation layers. On the \nMNIST dataset with LeNet [28] CNN architecture, DisturbLabel produced a 0.32% error \nrate compared to a baseline error rate of 0.39%. DisturbLabel combined with Dropout \nRegularization produced a 0.28% error rate compared to the 0.39% baseline. To translate \nthis to the context of adversarial training, one network takes in the classifier’s training \ndata as input and learns which labels to flip to maximize the error rate of the classification network.\nThe effectiveness of adversarial training in the form of noise or augmentation search is \nstill a relatively new concept that has not been widely tested and understood. Adversarial \nsearch to add noise has been shown to improve performance on adversarial examples, \nbut it is unclear if this is useful for the objective of reducing overfitting. Future work \nseeks to expand on the relationship between resistance to adversarial attacks and actual \nperformance on test datasets.\nGAN‑based Data Augmentation\nAnother exciting strategy for Data Augmentation is generative modeling. Generative modeling refers to the practice of creating artificial instances from a dataset such \nthat they retain similar characteristics to the original set. The principles of adversarial \ntraining discussed above have led to the very interesting and massively popular generative modeling framework known as GANs. Bowles et al. [85] describe GANs as a way \nto “unlock” additional information from a dataset. GANs are not the only generative \nTable 4 Test accuracies showing the  impact of  adversarial training, clean refers \nto  the  original testing data, FGSM refers to  adversary examples derived from  Fast \nGradient Sign Method and  PGD refers to  adversarial examples derived from  Projected \nGradient Descent [83]\nModels\nCIFAR-10\nClean\nClean\nStandard\n0.9939\n0.0922\n0.9306\n0.5524\n0.0256\nAdversarially trained\n0.9932\n0.9492\n0.0612\n0.8755\n0.8526\n0.1043\nOur method\n0.9903\n0.9713\n0.9171\n0.8714\n0.6514\n0.3440",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 21,
          "text": "Page 21 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nmodeling technique that exists; however they are dramatically leading the way in computation speed and quality of results.\nAnother useful strategy for generative modeling worth mentioning is variational \nauto-encoders. The GAN framework can be extended to improve the quality of samples \nproduced with variational auto-encoders [86]. Variational auto-encoders learn a lowdimensional representation of data points. In the image domain, this translates an image \ntensor of size height × width × color channels down into a vector of size n × 1, identical to what was discussed with respect to feature space augmentation. Low-dimensional \nconstraints in vector representations will result in a poorer representation, although \nthese constraints are better for visualization using methods such as t-SNE [87]. Imagine a vector representation of size 5 × 1 created by an autoencoder. These autoencoders \ncan take in a distribution of labeled data and map them into this space. These classes \ncould include ‘head turned left’, ‘centered head’, and ‘head turned right’. The auto-encoder \nlearns a low-dimensional representation of these data points such that vector operations \nsuch as adding and subtracting can be used to simulate a front view-3D rotation of a new \ninstance. Variational auto-encoder outputs can be further improved by inputting them \ninto GANs [31]. Additionally, a similar vector manipulation process can be done on the \nnoise vector inputs to GANs through the use of Bidirectional GANs [88].\nThe impressive performance of GANs has resulted in increased attention on how they \ncan be applied to the task of Data Augmentation. These networks have the ability to generate new training data that results in better performing classification models. The GAN \narchitecture first proposed by Ian Goodfellow [31] is a framework for generative modeling through adversarial training. The best anecdote for understanding GANs is the analogy of a cop and a counterfeiter. The counterfeiter (generator network) takes in some \nform of input. This could be a random vector, another image, text, and many more. The \ncounterfeiter learns to produce money such that the cop (discriminator network) cannot \ntell if the money is real or fake. The real or fake dichotomy is analogous to whether or \nnot the generated instance is from the training set or if it was created by the generator \nnetwork (Fig. 16).\nThe counterfeiter versus robber analogy is a seamless bridge to understand GANs \nin the context of network intrusion detection. Lin et al. [89] use a generator network \nto learn how to fool a black-box detection system. This highlights one of the most \ninteresting characteristics of GANs. Analysis tools derived from game theory such as \nminimax strategy and the Nash Equilibrium [90] suggest that the generator will eventually fool the discriminator. The success of the generator to overcome the discriminator makes it very powerful for generative modeling. GANs are the most promising \ngenerative modeling technique for use in Data Augmentation.\nFig. 16 Illustration of GAN concept provided by Mikolajczyk and Grochowski [72]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 22,
          "text": "Page 22 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nThe vanilla GAN architecture uses multilayer perceptron networks in the generator and discriminator networks. This is able to produce acceptable images on \na simple image dataset such as the MNIST handwritten digits. However, it fails \nto produce quality results for higher resolution, more complicated datasets. In \nthe MNIST dataset, each image is only 28 × 28 × 1 for a total of 784 pixels. GANs \napplied to the MNIST data are able to produce convincing results. However, MNIST \nimages are far less challenging than other image datasets due to low intra-class variance and resolution, to name a couple differences of many. This is in heavy contrast \nwith other datasets studied in most academic Computer Vision papers such as ImageNet or CIFAR-10. For immediate reference, an ImageNet image is of resolution \n256 × 256 × 3, totaling 196,608 pixels, a 250× increase in pixel count compared with \nMNIST.\nMany research papers have been published that modify the GAN framework \nthrough different network architectures, loss functions, evolutionary methods, and \nmany more. This research has significantly improved the quality of samples created by \nGANs. There have been many new architectures proposed for expanding on the concept of GANs and producing higher resolution output images, many of which are out \nof the scope of this paper. Amongst these new architectures, DCGANs, Progressively \nGrowing GANs, CycleGANs, and Conditional GANs seem to have the most application potential in Data Augmentation.\nThe DCGAN [91] architecture was proposed to expand on the internal complexity of the generator and discriminator networks. This architecture uses CNNs for \nthe generator and discriminator networks rather than multilayer perceptrons. The \nDCGAN was tested to generate results on the LSUN interior bedroom image dataset, each image being 64 × 64 × 3, for a total of 12,288 pixels, (compared to 784 in \nMNIST). The idea behind DCGAN is to increase the complexity of the generator \nnetwork to project the input into a high dimensional tensor and then add deconvolutional layers to go from the projected tensor to an output image. These deconvolutional layers will expand on the spatial dimensions, for example, going from \n14 × 14 × 6 to 28 × 28 × 1, whereas a convolutional layer will decrease the spatial \ndimensions such as going from 14 × 14 × 32 to 7 × 7 × 64. The DCGAN architecture \npresents a strategy for using convolutional layers in the GAN framework to produce \nhigher resolution images (Figs. 17, 18).\nFig. 17 DCGAN, generator architecture presented by Radford et al. [91]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 23,
          "text": "Page 23 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nFrid-Adar et  al. [49] tested the effectiveness of using DCGANs to generate liver \nlesion medical images. They use the architecture pictured above to generate 64 × 64 × 1 \nsize images of liver lesion CT scans. Their original dataset contains 182 CT scans, (53 \nCysts, 64 Metastases, and 65 Hemangiomas). After using classical augmentations to \nachieve 78.6% sensitivity and 88.4% specificity, they observed an increase to 85.7% sensitivity and 92.4% specificity once they added the DCGAN-generated samples.\nAnother architecture of interest is known as Progressively Growing GANs [34]. This \narchitecture trains a series of networks with progressive resolution complexity. These \nresolutions range from 4 × 4 to 8 × 8 and so on until outputs of size 1024 × 1024 are \nachieved. This is built on the concept that GANs can accept images as input as well \nas random vectors. Therefore, the series of GANs work by passing samples from a \nlower resolution GAN up to higher-resolution GANs. This has produced very amazing results on facial images.\nIn addition to improving the resolution size of GANs, another interesting architecture \nthat increases the quality of outputs is the CycleGAN [92] proposed by Zhu et al. CycleGAN introduces an additional Cycle-Consistency loss function to help stabilize GAN \ntraining. This is applied to image-to-image translation. Neural Style Transfer [32], discussed further in the section below, learns a single image to single image translation. \nHowever, CycleGAN learns to translate from a domain of images to another domain, \nsuch as horses to zebras. This is implemented via forward and backward consistency loss \nfunctions. A generator takes in images of horses and learns to map them to zebras such \nthat the discriminator cannot tell if they were originally a part of the zebra set or not, as \ndiscussed above. After this, the generated zebras from horse images are passed through \na network which translates them back into horses. A second discriminator determines \nif this re-translated image belongs to the horse set or not. Both of these discriminator \nlosses are aggregated to form the cycle-consistency loss.\nThe use of CycleGANs was tested by Zhu et al. [93] in the task of Emotion Classification. Using the emotion recognition dataset, FER2013 [94], Facial Expression \nRecognition Database, they build a CNN classifier to recognize 7 different emotions: \nangry, disgust, fear, happy, sad, surprise, and neutral. These classes are imbalanced \nand the CycleGAN is used as a method of intelligent oversampling.\nCycleGANs learned an unpaired image-to-image translation between domains. An \nexample of the domains in this problem is neutral to disgust. The CycleGAN learns to \ntranslate an image representing a neutral image into an image representing the disgust emotion (Figs. 19, 20).\nFig. 18 Complete DCGAN architecture used by Frid-Adar et al. [49] to generate liver lesion images",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 24,
          "text": "Page 24 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nUsing CycleGANs to translate images from the other 7 classes into the minority \nclasses was very effective in improving the performance of the CNN model on emotion recognition. Employing these techniquess, accuracy improved 5-10%. To further \nunderstand the effectiveness of adding GAN-generated instances, a t-SNE visualization is used. t-SNE [87] is a visualization technique that learns to map between highdimensional vectors into a low-dimensional space to facilitate the visualization of \ndecision boundaries (Fig. 21).\nAnother interesting GAN architecture for use in Data Augmentation is Conditional \nGANs [95]. Conditional GANs add a conditional vector to both the generator and the \ndiscriminator in order to alleviate problems with mode collapse. In addition to inputting \na random vector z to the generator, Conditional GANs also input a y vector which could \nbe something like a one-hot encoded class label, e.g. [0 0 0 1 0]. This class label targets a \nspecific class for the generator and the discriminator (Fig. 22).\nFig. 19 Some examples of synthetic data created with CycleGANs for emotion classification\nFig. 20 Architecture overview: G and F consist of two separate GANs composing the CycleGAN. Two images \nare taken from the reference and target class and used to generate new data in the target class",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 2
        },
        {
          "page_number": 25,
          "text": "Page 25 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nLucic et al. [96] sought out to compare newly developed GAN loss functions. They \nconducted a series of tests that determined most loss functions can reach similar scores \nwith enough hyperparameter optimization and random restarts. This suggests that \nincreased computational power is a more promising area of focus than algorithmic \nchanges in the generator versus discriminator loss function.\nMost of the research done in applying GANs to Data Augmentation and reporting \nthe resulting classification performance has been done in biomedical image analysis \nFig. 21 t-SNE visualization demonstrating the improved decision boundaries when using \nCycleGAN-generated samples. a original CNN model, b adding GAN-generated disgust images, c adding \nGAN-generated sad images, d adding both GAN-generated disgust and sad images [93]\nFig. 22 Illustration from Mirza and Osindero [95] showing how the conditional y vector is integrated into the \nGAN framework",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 2
        },
        {
          "page_number": 26,
          "text": "Page 26 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \n[39]. These papers have shown improved classification boundaries derived from training with real and generated data from GAN models. In addition, some papers measure \nthe quality of GAN outputs by a visual Turing test. In these tests, the study asks two \nexperts to distinguish between real and artificial images in medical image tasks such \nas skin lesion classification and liver cancer detection. Table 5 shows that the first and \nsecond experts were only able to correctly label 62.5% and 58.6% of the GAN-generated liver lesion images as fake. Labeling images as fake refers to their origin coming \nfrom the generator rather than an actual liver lesion image (Table 6; Fig. 23).\nTable 5 Results of ‘Visual Turing Test’ on DCGAN-generated liver lesion images presented \nby Frid-Adar et al. [139]\nClassification accuracy\nIs ROI real?\nReal (%)\nSynthetic (%)\nTotal score\nTotal score\nExpert 1\n77.5\n235\\302 = 77.8%\n189\\302 = 62.5%\nExpert 2\n69.2\n69.2\n209\\302 = 69.2%\n177\\302 = 58.6%\nTable 6 Results of ‘Visual Turing Test’ on different DCGAN- and WGAN [104]-generated \nbrain tumor MR images presented by Han et al. [140]\nAccuracy (%)\nReal \nselected \nas real\nReal as synt\nSynt as real\nSynt as synt\nT1 (DCGAN, 128 × 128)\n26\n6\nTlc (DCGAN, 128 × 128)\n24\n3\nT2 (DCGAN, 128 × 128)\n22\n8\nFLAIR (DCGAN, 128 × 128)\n12\n8\nConcat (DCGAN, 128 × 128)\n34\n7\nConcat (DCGAN, 64 × 64)\n13\n9\nT1 (WGAN, 128 × 128)\n20\n6\nTlc (WGAN, 128 × 128)\n13\n8\nT2 (WGAN, 128 × 128)\n19\n11\nFLAIR (WGAN, 128 × 128)\n16\n4\nConcat (WGAN, 128 × 128)\n31\n15\nConcat (WGAN, 64 × 64)\n18\n15\nFig. 23 Trends in applying GANs to medical image analysis [39]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 27,
          "text": "Page 27 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nGAN samples can be used as an oversampling technique to solve problems with class \nimbalance. Lim et al. [97] show how GAN samples can be used for unsupervised anomaly detection. By oversampling rare normal samples, which are samples that occur with \nsmall probability, GANs are able to reduce the false positive rate of anomaly detection. \nThey do this using the Adversarial Autoencoder framework proposed by Makhzani et al. \n[98] (Fig. 24).\nAs exciting as the potential of GANs is, it is very difficult to get high-resolution outputs from the current cutting-edge architectures. Increasing the output size of the \nimages produced by the generator will likely cause training instability and non-convergence. Another drawback of GANs is that they require a substantial amount of data to \ntrain. Thus, depending on how limited the initial dataset is, GANs may not be a practical \nsolution. Salimans et al. [99] provide a more complete description of the problems with \ntraining GANs.\nNeural Style Transfer\nNeural Style Transfer [32] is one of the flashiest demonstrations of Deep Learning \ncapabilities. The general idea is to manipulate the representations of images created in \nCNNs. Neural Style Transfer is probably best known for its artistic applications, but it \nalso serves as a great tool for Data Augmentation. The algorithm works by manipulating the sequential representations across a CNN such that the style of one image can be \ntransferred to another while preserving its original content. A more detailed explanation \nof the gram matrix operation powering Neural Style Transfer can be found by Li et al. \n[100] (Fig. 25).\nIt is important to also recognize an advancement of the original algorithm from Gatys \net al. known as Fast Style Transfer [35]. This algorithm extends the loss function from \na per-pixel loss to a perceptual loss and uses a feed-forward network to stylize images. \nThis perceptual loss is reasoned about through the use of another pre-trained net. The \nuse of perceptual loss over per-pixel loss has also shown great promise in the application of super-resolution [101] as well as style transfer. This loss function enhancement \nenables style transfer to run much faster, increasing interest in practical applications. \nAdditionally, Ulyanov et al. [102] find that replacing batch normalization with instance \nnormalization results in a significant improvement for fast stylization (Fig. 26).\nFor the purpose of Data Augmentation, this is somewhat analogous to color space \nlighting transformations. Neural Style Transfer extends lighting variations and enables \nthe encoding of different texture and artistic styles as well. This leaves practitioners of \nData Augmentation with the decision of which styles to sample from when deriving new \nimages via Neural Style Transfer.\nChoosing which styles to sample from can be a challenging task. For applications such as self-driving cars it is fairly intuitive to think of transferring training \nFig. 24 Adversarial autoencoder framework used in DOPING [97]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 28,
          "text": "Page 28 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \ndata into a night-to-day scale, winter-to-summer, or rainy-to-sunny scale. However, \nin other application domains, the set of styles to transfer into is not so obvious. For \nease of implementation, data augmentation via Neural Style Transfer could be done \nby selecting a set of k styles and applying them to all images in the training set. The \nwork of Style Augmentation [103], avoids introducing a new form of style bias into \nthe dataset by deriving styles at random from a distribution of 79,433 artistic images. \nTransferring style in training data has been tested on the transition from simulated \nenvironments to the real-world. This is very useful for robotic manipulation tasks \nusing Reinforcement Learning because of potential damages to hardware when training in the real-world. Many constraints such as low-fidelity cameras cause these \nmodels to generalize poorly when trained in physics simulations and deployed in the \nreal-world.\nTobin et al. [104] explore the effectiveness of using different styles in training simulation and achieve within 1.5 cm accuracy in the real-world on the task of object localization. Their experiments randomize the position and texture of the objects to be detected \nFig. 25 Illustration of style and content reconstructions in Neural Style Transfer [32]\nFig. 26 Illustration of the Fast neural style algorithm by Johnson et al. [35]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 2
        },
        {
          "page_number": 29,
          "text": "Page 29 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \non the table in the simulation, as well as the texture, lighting, number of lights, and random noise in the background. They found that with enough variability in the training \ndata style, the real-world simply appears as another variation to the model. Interestingly, \nthey found that diversity in styles was more effective than simulating in as realistic of an \nenvironment as possible. This is in contrast to the work of Shrivastava et al. [105] who \nused GANs to make their simulated data as realistic as possible (Fig. 27).\nUsing simulated data to build Computer Vision models has been heavily investigated. \nOne example of this is from Richter et al. [106]. They use computer graphics from modern open-world games such as Grand Theft Auto to produce semantic segmentation \ndatasets. The authors highlight anecdotes of the manual annotation costs required to \nbuild these pixel-level datasets. They mention the CamVid dataset [107] requires 60 min \nper image to manually annotate, and the Cityscapes dataset [108] requires 90 min per \nimage. This high labor and time cost motivates the use and development of synthetic \ndatasets. Neural Style Transfer is a very interesting strategy to improve the generalization ability of simulated datasets.\nA disadvantage of Neural Style Transfer Data Augmentation is the effort required to \nselect styles to transfer images into. If the style set is too small, further biases could be \nintroduced into the dataset. Trying to replicate the experiments of Tobin et al. [104] will \nrequire a massive amount of additional memory and compute to transform and store \n79,433 new images from each image. The original algorithm proposed by Gatys et al. \n[32] has a very slow running time and is therefore not practical for Data Augmentation. \nThe algorithm developed by Johnson et al. [35] is much faster, but limits transfer to a \npre-trained set of styles.\nMeta learning Data Augmentations\nThe concept of meta-learning in Deep Learning research generally refers to the concept \nof optimizing neural networks with neural networks. This approach has become very \npopular since the publication of NAS [33] from Zoph and Le. Real et al. [109, 110] also \nFig. 27 Examples of different styles simulated by Tobin et al. [104]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 30,
          "text": "Page 30 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nshow the effectiveness of evolutionary algorithms for architecture search. Salimans et al. \n[111] directly compare evolutionary strategies with Reinforcement Learning. Another \ninteresting alternative to Reinforcement Learning is simple random search [112]. Utilizing evolutionary and random search algorithms is an interesting area of future \nwork, but the meta-learning schemes reviewed in this survey are all neural-network, \ngradient-based.\nThe history of Deep Learning advancement from feature engineering such as SIFT \n[113] and HOG [114] to architecture design such as AlexNet [1], VGGNet [2], and \nInception-V3 [4], suggest that meta-architecture design is the next paradigm shift. \nNAS takes a novel approach to meta-learning architectures by using a recurrent network trained with Reinforcement Learning to design architectures that result in the \nbest accuracy. On the CIFAR-10 dataset, this achieved an error rate of 3.65 (Fig. 28).\nThis section will introduce three experiments using meta-learning for Data Augmentation. These methods use a prepended neural network to learn Data Augmentations via mixing images, Neural Style Transfer, and geometric transformations.\nNeural augmentation The Neural Style Transfer algorithm requires two parameters \nfor the weights of the style and content loss. Perez and Wang [36] presented an algorithm to meta-learn a Neural Style Transfer strategy called Neural Augmentation. The \nNeural Augmentation approach takes in two random images from the same class. The \nprepended augmentation net maps them into a new image through a CNN with 5 \nlayers, each with 16 channels, 3 × 3 filters, and ReLU activation functions. The image \noutputted from the augmentation is then transformed with another random image via \nNeural Style Transfer. This style transfer is carried out via the CycleGAN [92] extension of the GAN [31] framework. These images are then fed into a classification model \nand the error from the classification model is backpropagated to update the Neural \nAugmentation net. The Neural Augmentation network uses this error to learn the optimal weighting for content and style images between different images as well as the \nmapping between images in the CNN (Fig. 29).\nPerez and Wang tested their algorithm on the MNIST and Tiny-imagenet-200 datasets on binary classification tasks such as cat versus dog. The Tiny-imagenet-200 \ndataset is used to simulate limited data. The Tiny-imagenet-200 dataset contains \nonly 500 images in each of the classes, with 100 set aside for validation. This problem \nFig. 28 Concept behind Neural Architecture Search [33]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 31,
          "text": "Page 31 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nlimits this dataset to 2 classes. Thus there are only 800 images for training. Each of \nthe Tiny-imagenet-200 images is 64 × 64 × 3, and the MNIST images are 28 × 28 × 1. \nThe experiment compares their proposed Neural Augmentation [36] approach with \ntraditional augmentation techniques such as cropping and rotation, as well as with \na style transfer approach with a predetermined set of styles such as Night/Day and \nWinter/Summer.\nThe traditional baseline study transformed images by choosing an augmentation \nfrom a set (shifted, zoomed in/out, rotated, flipped, distorted, or shaded with a hue). \nThis was repeated to increase the dataset size from N to 2 N. The GAN style transfer \nbaseline uses 6 different styles to transform images (Cezanne, Enhance, Monet, Ukiyoe, Van Gogh and Winter). The Neural Augmentation techniques tested consist of \nthree levels based on the design of the loss function for the augmentation net (Content loss, Style loss via gram matrix, and no loss computer at this layer). All experiments are tested with a convolutional network consisting of 3 convolutional layers \neach followed by max pooling and batch normalization, followed by 2 fully-connected \nlayers. Each experiment runs for 40 epochs at a learning rate of 0.0001 with the Adam \noptimization technique (Table 7).\nThe results of the experiment are very promising. The Neural Augmentation technique performs significantly better on the Dogs versus Goldfish study and only \nslightly worse on Dogs versus Cats. The technique does not have any impact on the \nMNIST problem. The paper suggests that the likely best strategy would be to combine \nthe traditional augmentations and the Neural Augmentations.\nSmart Augmentation The Smart Augmentation [37] approach utilizes a similar concept as the Neural Augmentation technique presented above. However, the combination of images is derived exclusively from the learned parameters of a prepended CNN, \nrather than using the Neural Style Transfer algorithm.\nSmart Augmentation is another approach to meta-learning augmentations. This is \ndone by having two networks, Network-A and Network-B. Network-A is an augmentation network that takes in two or more input images and maps them into a new \nimage or images to train Network-B. The change in the error rate in Network-B is then \nFig. 29 Illustration of augmentation network [36]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 32,
          "text": "Page 32 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nbackpropagated to update Network-A. Additionally another loss function is incorporated into Network-A to ensure that its outputs are similar to others within the class. \nNetwork-A uses a series of convolutional layers to produce the augmented image. The \nconceptual framework of Network-A can be expanded to use several Networks trained \nin parallel. Multiple Network-As could be very useful for learning class-specific augmentations via meta-learning (Fig. 30).\nSmart Augmentation is similar to SamplePairing [65] or mixed-examples in the sense \nthat a combination of existing examples produces new ones. However, the mechanism of \nSmart Augmentation is much more sophisticated, using an adaptive CNN to derive new \nimages rather than averaging pixels or hand-engineered image combinations.\nThe Smart Augmentation technique was tested on the task of gender recognition. \nOn the Feret dataset, accuracy improved from 83.52 to 88.46%. The audience dataset \nresponded with an improvement of 70.02% to 76.06%. Most interestingly, results from \nanother face dataset increased from 88.15 to 95.66%. This was compared with traditional \naugmentation techniques which increased the accuracy from 88.15 to 89.08%. Additionally, this experiment derived the same accuracy when using two Network-As in the augmentation framework as was found with one Network-A. This experiment demonstrates \nTable 7 Results comparing augmentations [36]\nQuantitative results on dogs vs. goldfish\nDogs vs goldfish\nAugmentation\nVal. acc.\nNone\n0.855\nTraditional\n0.890\nGANs\n0.865\nNeural + no loss\n0.915\nNeural + content loss\n0.900\nNeural + style\n0.890\nControl\n0.840\nQuantitative results on dogs vs cats\nDogs vs cat\nAugmentation\nVal. acc.\nNone\n0.705\nTraditional\n0.775\nGANs\n0.720\nNeural + no loss\n0.765\nNeural + content loss\n0.770\nNeural + style\n0.740\nControl\n0.710\nMNIST 0′s and 8′s\nAugmentation\nVal. acc.\nNone\n0.972\nNeural + no loss\n0.975\nNeural + content loss\n0.968",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 33,
          "text": "Page 33 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nFig. 30 Illustration of the Smart Augmentation architecture [37]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 34,
          "text": "Page 34 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nthe significant performance increase with the Smart Augmentation meta-learning strategy (Fig. 31).\nAutoAugment AutoAugment [38], developed by Cubuk et  al., is a much different \napproach to meta-learning than Neural Augmentation or Smart Augmentation. AutoAugment is a Reinforcement Learning algorithm [115] that searches for an optimal \naugmentation policy amongst a constrained set of geometric transformations with miscellaneous levels of distortions. For example, ‘translateX 20 pixels’ could be one of the \ntransformations in the search space (Table 8).\nIn Reinforcement Learning algorithms, a policy is analogous to the strategy of the \nlearning algorithm. This policy determines what actions to take at given states to achieve \nsome goal. The AutoAugment approach learns a policy which consists of many subpolicies, each sub-policy consisting of an image transformation and a magnitude of \ntransformation. Reinforcement Learning is thus used as a discrete search algorithm of \naugmentations. The authors also suggest that evolutionary algorithms or random search \nwould be effective search algorithms as well.\nAutoAugment found policies which achieved a 1.48% error rate on CIFAR-10. AutoAugment also achieved an 83.54% Top-1 accuracy on the ImageNet dataset. Very interestingly as well, the policies learned on the ImageNet dataset were successful when \ntransferred to the Stanford Cars and FGVC Aircraft image recognition tasks. In this \ncase, the ImageNet policy applied to these other datasets reduced error rates by 1.16% \nand 1.76% respectively.\nGeng et al. [116] expanded on AutoAugment by replacing the Reinforcement Learning \nsearch algorithm with Augmented Random Search (ARS) [112]. The authors point out \nthat the sub-policies learned from AutoAugment are inherently flawed because of the \ndiscrete search space. They convert the probability and magnitude of augmentations into \na continuous space and search for sub-policies with ARS. With this, they achieve lower \nerror rates on CIFAR-10, CIFAR-100, and ImageNet (Table 9).\nMinh et  al. [117] also experimented with using Reinforcement Learning [115] to \nsearch for Data Augmentations. They further explore the effectiveness of learning transformations for individual instances rather than the entire dataset. They find classification accuracy differences of 70.18% versus 74.42% on the CIFAR-10 dataset and 74.61% \nversus 80.35% on the problem of classifying dogs versus cats. Further, they explore the \nrobustness of classifiers with respect to test-time augmentation and find that the model \nFig. 31 On the gender recognition task, the image to the left is an example of an instance produced by \nNetwork-A in Smart Augmentation given the right images as input [37]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 35,
          "text": "Page 35 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \ntrained with Reinforcement Learning augmentation search performs much better. On \nthe CIFAR-10 dataset this results in 50.99% versus 70.06% accuracy when the models are \nevaluated on augmented test data.\nA disadvantage to meta-learning is that it is a relatively new concept and has not been \nheavily tested. Additionally, meta-learning schemes can be difficult and time-consuming \nto implement. Practitioners of meta-learning will have to solve problems primarily with \nvanishing gradients [118], amongst others, to train these networks.\nTable 8 AutoAugment augmentation policy found on the reduced CIFAR-10 dataset [38]\nOperation 1\nOperation 2\nSub-policy 0\n(Invert,0.1,7)\n(Contrast,0.2,6)\nSub-policy 1\n(Rotate,0.7,2)\n(TranslateX,0.3,9)\nSub-policy 2\n(Sharpness,0.8,1)\n(Sharpness,0.9,3)\nSub-policy 3\n(ShearY,0.5,8)\n(TranslateY,0.7,9)\nSub-policy 4\n(AutoContrast,0.5,8)\n(Equalize,0.9,2)\nSub-policy 5\n(ShearY,0.2,7)\n(Posterize,0.3,7)\nSub-policy 6\n(Color,0.4,3)\n(Brightness,0.6,7)\nSub-policy 7\n(Sharpness,0.3,9)\n(Brightness,0.7,9)\nSub-policy 8\n(Equalize,0.6,5)\n(Equalize,0.5,1)\nSub-policy 9\n(Contrast,0.6,7)\n(Sharpness,0.6,5)\nSub-policy 10\n(Color,0.7,7)\n(TranslateX,0.5,8)\nSub-policy 11\n(Equalize,0.3,7)\n(AutoContrast,0.4,8)\nSub-policy 12\n(TranslateY,0.4,3)\n(Sharpness,0.2,6)\nSub-policy 13\n(Brightness,0.9,6)\n(Color,0.2,8)\nSub-policy 14\n(Solarize,0.5,2)\n(Invert,0.0,3)\nSub-policy 15\n(Equalize,0.2,0)\n(AutoContrast,0.6,0)\nSub-policy 16\n(Equalize,0.2,8)\n(Equalize,0.6,4)\nSub-policy 17\n(Color,0.9,9)\n(Equalize,0.6,6)\nSub-policy 18\n(AutoContrast,0.8,4)\n(Solarize,0.2,8)\nSub-policy 19\n(Brightness,0.1,3)\n(Color,0.7,0)\nSub-policy 20\n(Solarize,0.4,5)\n(AutoContrast,0.9,3)\nSub-policy 21\n(TranslateY,0.9,9)\n(TranslateY,0.7,9)\nSub-policy 22\n(AutoContrast,0.9,2)\n(Solarize,0.8,3)\nSub-policy 23\n(Equalize,0.8,8)\n(Invert,0.1,3)\nSub-policy 24\n(TranslateY,0.7,9)\n(AutoContrast,0.9,1)\nTable 9 The performance of ARS on continuous space vs. AutoAugment on discrete space \n[116]\nModel\nAutoAugment\nARS-Aug\nWide-ResNet-28-10\n2.68\n2.33\nShake-Shake (26 2 × 32 days)\n2.47\n2.14\nShake-Shake (26 2 × 96 days)\n1.99\n1.68\nShake-Shake (26 2 × 112 days)\n1.89\n1.59\nAmoebaNet-B (6,128)\n1.75\n1.49\nPyramidNet + ShakeDrop\n1.48\n1.26",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 36,
          "text": "Page 36 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nComparing Augmentations\nAs shown throughout “Design considerations for image Data Augmentation” section, \npossibilities for Data Augmentation. However, there are not many comparative studies \nthat show the performance differences of these different augmentations. One such study \nwas conducted by Shijie et al. [119] which compared GANs, WGANs, flipping, cropping, \nshifting, PCA jittering, color jittering, adding noise, rotation, and some combinations on \nthe CIFAR-10 and ImageNet datasets. Additionally, the comparative study ranged across \ndataset sizes with the small set consisting of 2 k samples with 200 in each class, tthe \nmedium set consisting of 10 k samples with 1 k in each class, and the large set consisting of 50 k samples with 5 k in each class. They also tested with 3 levels of augmentation, \nno augmentation, original plus same size of generated samples, and original plus double \nsize of generated samples. They found that cropping, flipping, WGAN, and rotation generally performed better than others. The combinations of flipping + cropping and flipping + WGAN were the best overall, improving classification performance on CIFAR-10 \nby + 3% and + 3.5%, respectively.\nDesign considerations for image Data Augmentation\nThis section will briefly describe some additional design decisions with respect to Data \nAugmentation techniques on image data.\nTest-time augmentation\nIn addition to augmenting training data, many research reports have shown the effectiveness of augmenting data at test-time as well. This can be seen as analogous to ensemble learning techniques in the data space. By taking a test image and augmenting it in the \nsame way as the training images, a more robust prediction can be derived. This comes \nat a computational cost depending on the augmentations performed, and it can restrict \nthe speed of the model. This could be a very costly bottleneck in models that require \nreal-time prediction. However, test-time augmentation is a promising practice for applications such as medical image diagnosis. Radosavovic et al. [120] denote test-time augmentation as data distillation to describe the use of ensembled predictions to get a better \nrepresentation of the image.\nWang et al. [121] sought out to develop a mathematical framework to formulate testtime augmentation. Testing their test-time augmentation scheme on medical image \nsegmentation, they found that it outperformed the single-prediction baseline and dropout-based multiple predictions. They also found better uncertainty estimation when \nusing test-time augmentation, reducing highly confident but incorrect predictions. \nTheir test-time augmentation method uses a Monte Carlo simulation in order to obtain \nparameters for different augmentations such as flipping, scaling, rotation, and translations, as well as noise injections.\nTest-time augmentation can be found in the Alexnet paper [1], which applies CNNs \nto the ImageNet dataset. In their experiments, they average the predictions on ten randomly cropped patches. These patches consist of one extracted from the center, four \ncorner croppings, and the equivalent regions on the horizontally flipped images. These \npredictions are averaged to form the final output. He et al. [3] use the same 10-crop testing procedure to evaluate their ResNet CNN architecture (Fig. 32).",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 37,
          "text": "Page 37 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nPerez et al. [122] present a study on the effectiveness of test-time augmentation with \nmany augmentation techniques. These augmentations tested include color augmentation, rotation, shearing, scaling, flipping, random cropping, random erasing, elastic, \nmixing, and combinations between the techniques. Table  9 shows the higher performance achieved when augmenting test images as well as training images. Matsunaga \net al. [123] also demonstrate the effectiveness of test-time augmentation on skin lesion \nclassification, using geometric transformations such as rotation, translation, scaling, and \nflipping.\nThe impact of test-time augmentation on classification accuracy is another mechanism \nfor measuring the robustness of a classifier. A robust classifier is thus defined as having a \nlow variance in predictions across augmentations. For example, a prediction of an image \nshould not be much different when that same image is rotated 20°. In their experiments \nsearching for augmentations with Reinforcement Learning, Minh et al. [117] measure \nrobustness by distorting test images with a 50% probability and contrasting the accuracy on un-augmented data with the augmented data. In this study, the performance of \nthe baseline model decreases from 74.61 to 66.87% when evaluated on augmented test \nimages.\nSome classification models lie on the fence in terms of their necessity for speed. This \nsuggests promise in developing methods that incrementally upgrade the confidence \nof prediction. This could be done by first outputting a prediction with little or no testtime augmentation and then incrementally adding test-time augmentations to increase \nthe confidence of the prediction. Different Computer Vision tasks require certain constraints on the test-time augmentations that can be used. For example, image recognition can easily aggregate predictions across warped images. However, it is difficult to \naggregate predictions on geometrically transformed images in object detection and \nsemantic segmentation.\nCurriculum learning\nAside from the study of Data Augmentation, many researchers have been interested \nin trying to find a strategy for selecting training data that beats random selection. In \nthe context of Data Augmentation, research has been published investigating the relationship between original and augmented data across training epochs. Some research \nFig. 32 Impact of test-time data augmentation for skin lesion classification [122]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 38,
          "text": "Page 38 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nsuggests that it is best to initially train with the original data only and then finish training \nwith the original and augmented data, although there is no clear consensus.\nIn the SamplePairing [65] study, one epoch on ImageNet and 100 epochs on other \ndatasets are completed without SamplePairing before mixed image data is added to the \ntraining. Once the SamplePairing images are added to the training set, they run in cycles \nbetween 8:2 epochs, 8 with SamplePairing images, 2 without. Jaderberg et al. [124] train \nexclusively with synthetic data for natural scene text recognition. The synthetic data \nproduced the training data by enumerating through different fonts and augmentations. \nThis produced sets of training images for size 50 k and 90 k lexicons. Mikolajczyk and \nGrochowski [72] draw comparisons from transfer learning. They suggest that training on \naugmented data to learn the initial weights of a deep convolutional network is similar to \ntransferring weights trained on other datasets such as ImageNet. These weights are then \nfine-tuned only with the original training data.\nCurriculum learning decisions are especially important for One-Shot Learning systems such as FaceNet, presented by Schroff et  al. [125]. It is important to find faces \nwhich are somewhat similar to the new face such that the learned distance function is \nactually useful. In this sense, the concept of curriculum learning shares many similarities \nwith adversarial search algorithms or learning only on hard examples.\nCurriculum learning, a term originally coined by Bengio et al. [126], is an applicable \nconcept for all Deep Learning models, not just those constrained with limited data. Plotting out training accuracy over time across different initial training subsets could help \nreveal patterns in the data that dramatically speed up training time. Data Augmentation \nconstructs massively inflated training from combinations such as flipping, translating, \nand randomly erasing. It is highly likely that a subset exists in this set such that training \nwill be faster and more accurate.\nResolution impact\nAnother interesting discussion about Data Augmentation in images is the impact of resolution. Higher resolution images such as HD (1920 × 1080 × 3) or 4 K (3840 × 2160 × 3) \nrequire much more processing and memory to train deep CNNs. However, it seems \nintuitive that next-generation models would be trained on higher resolution images. \nMany current models downsample images from their original resolution to make the \nclassification problem computationally more feasible. However, sometimes this downsampling causes information loss within the image, making image recognition more difficult (Table 10).\nIt is interesting to investigate the nature of this downsampling and resulting performance comparison. Wu et al. [127] compare the tradeoff between accuracy and speed \nTable 10 Comparison of resolution across three very popular open-source image datasets\nDataset\nResolution\nMNIST handwritten digits\n28 × 28 × 1\nCIFAR-10/100\n32 × 32 × 3\nImageNet\n256 × 256 × 3",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 39,
          "text": "Page 39 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nwhen downsampling images to different resolutions. The researchers found that composing an ensemble of models trained with high and low-resolution images performed \nbetter than any one model individually. This ensemble prediction is found by averaging the softmax predictions. The models trained on 256 × 256 images and 512 × 512 \nimages achieve 7.96% and 7.42% top-5 error rates, respectively. When aggregated they \nachieved a lower top-5 error rate of 6.97%. Therefore, different downsampled images can \nbe viewed as another Data Augmentation scheme (Fig. 33).\nWith the advance of Super-Resolution Convolutional Neural Networks presented by \nChong et  al. [128] or SRGANs, Super-Resolution Generative Adversarial Networks, \npresented by Ledig et al. [129], it is interesting to consider if upsampling images to an \neven higher resolution would result in better models. Quality upsampling on CIFAR-10 \nimages from even 32 × 32 × 3 to 64 × 64 × 3 could lead to better and more robust image \nclassifiers.\nResolution is also a very important topic with GANs. Producing high resolution outputs from GANs is very difficult due to issues with training stability and mode collapse. \nMany of the newer GAN architectures such as StackGAN [130] and Progressively-Growing GANs [34] are designed to produce higher resolution images. In addition to these \narchitectures, the use of super-resolution networks such as SRGAN could be an effective \ntechnique for improving the quality of outputs from a DCGAN [91] model. Once it is \npractical to produce high resolution outputs from GAN samples, these outputs will be \nvery useful for Data Augmentation.\nFinal dataset size\nA necessary component of Data Augmentation is the determination of the final dataset size. For example, if all images are horizontally flipped and added to the dataset, \nthe resulting dataset size changes from N to 2N. One of the main considerations with \nrespect to final dataset size is the additional memory and compute constraints associated with augmenting data. Practitioners have the choice between using generators \nwhich transform data on the fly during training or transforming the data beforehand and \nFig. 33 Classifications of the Image to the right by different resolution models trained by Wu et al. [127]",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 40,
          "text": "Page 40 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nstoring it in memory. Transforming data on the fly can save memory, but will result in \nslower training. Storing datasets in memory can be extremely problematic depending on \nhow heavily the dataset size has been inflated. Storing augmented datasets in memory is \nespecially problematic when augmenting big data. This decision is generally categorized \nas online or offline data augmentation, (with online augmentation referring to on the \nfly augmentations and offline augmentation referring to editing and storing data on the \ndisk).\nIn the design of a massively distributed training system, Chilimbi et al. [131] augment \nimages before training to speed up image serving. By augmenting images in advance, the \ndistributed system is able to request and pre-cache training batches. Augmentations can \nalso be built into the computational graph used to construct Deep Learning models and \nfacilitate fast differentiation. These augmentations process images immediately after the \ninput image tensor.\nAdditionally, it is also interesting to explore a subset of the inflated data that will result \nin higher or similar performance to the entire training set. This is a similar concept to \ncurriculum learning, since the central idea is to find an optimal ordering of training data. \nThis idea is also very related to final dataset size and the considerations of transformation compute and available memory for storing augmented images.\nAlleviating class imbalance with Data Augmentation\nClass imbalance is a common problem in which a dataset is primarily composed of \nexamples from one class. This could manifest itself in a binary classification problem \nsuch that there is a clear majority-minority class distinction, or in multi-class classification in which there is one or multiple majority classes and one or multiple minority \nclasses. Imbalanced datasets are harmful because they bias models towards majority \nclass predictions. Imbalanced datasets also render accuracy as a deceitful performance \nmetric. Buda et al. [132] provide a systematic study specifically investigating the impact \nof imbalanced data in CNNs processing image data. Leevy et al. [27] cover many Datalevel and Algorithm-level solutions to class imbalance in big data in general. Data \nAugmentation falls under a Data-level solution to class imbalance and there are many \ndifferent strategies for implementation.\nA naive solution to oversampling with Data Augmentation would be a simple random \noversampling with small geometric transformations such as a 30° rotation. Other simple image manipulations such as color augmentations, mixing images, kernel filters, and \nrandom erasing can also be extended to oversample data in the same manner as geometric augmentations. This can be useful for ease of implementation and quick experimentation with different class ratios. One problem of oversampling with basic image \ntransformations is that it could cause overfitting on the minority class which is being \noversampled. The biases present in the minority class are more prevalent post-sampling \nwith these techniques.\nOversampling methods based on Deep Learning such as adversarial training, Neural Style Transfer, GANs, and meta-learning schemes can also be used as a more intelligent oversampling strategy. Neural Style Transfer is an interesting way to create new \nimages. These new images can be created either through extrapolating style with a foreign style or by interpolating styles amongst instances within the dataset. Using GANs",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 41,
          "text": "Page 41 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nto oversample data could be another effective way to increase the minority class size \nwhile preserving the extrinsic distribution. Oversampling with GANs can be done using \nthe entire minority class as “real” examples, or by using subsets of the minority class as \ninputs to GANs. The use of evolutionary sampling [133] to find these subsets to input to \nGANs for class sampling is a promising area for future work.\nDiscussion\nThe interesting ways to augment image data fall into two general categories: data warping and oversampling. Many of these augmentations elucidate how an image classifier \ncan be improved, while others do not. It is easy to explain the benefit of horizontal flipping or random cropping. However, it is not clear why mixing pixels or entire images \ntogether such as in PatchShuffle regularization or SamplePairing is so effective. Additionally, it is difficult to interpret the representations learned by neural networks for \nGAN-based augmentation, variational auto-encoders, and meta-learning. CNN visualization has been led by Yosinski et al. [134] with their deep visualization method. Having a human-level understanding of convolutional networks features could greatly help \nguide the augmentation process.\nManipulating the representation power of neural networks is being used in many \ninteresting ways to further the advancement of augmentation techniques. Traditional \nhand-crafted augmentation techniques such as cropping, flipping, and altering the color \nspace are being extended with the use of GANs, Neural Style Transfer, and meta-learning search algorithms.\nImage-to-image translation has many potential uses in Data Augmentation. Neural \nStyle Transfer uses neural layers to translate images into new styles. This technique not \nonly utilizes neural representations to separate ‘style’ and ‘content’ from images, but also \nuses neural transformations to transfer the style of one image into another. Neural Style \nTransfer is a much more powerful augmentation technique than traditional color space \naugmentations, but even these methods can be combined together.\nAn interesting characteristic of these augmentation methods is their ability to be combined together. For example, the random erasing technique can be stacked on top of any \nof these augmentation methods. The GAN framework possesses an intrinsic property \nof recursion which is very interesting. Samples taken from GANs can be augmented \nwith traditional augmentations such as lighting filters, or even used in neural network \naugmentation strategies such as Smart Augmentation or Neural Augmentation to create even more samples. These samples can be fed into further GANs and dramatically \nincrease the size of the original dataset. The extensibility of the GAN framework is \namongst many reasons they are so interesting to Deep Learning researchers.\nTest-time augmentation is analogous to ensemble learning in the data space. Instead \nof aggregating the predictions of different learning algorithms, we aggregate predictions \nacross augmented images. We can even extend the solution algorithm to parameterize \nprediction weights from different augmentations. This seems like a good solution for systems concerned with achieving very high performance scores, more so than prediction",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 42,
          "text": "Page 42 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nspeed. Determining the effectiveness of test-time augmentation by primarily exploring \ntest-time geometric transformations and Neural Style Transfer, is an area of future work.\nAn interesting question for practical Data Augmentation is how to determine postaugmented dataset size. There is no consensus as to which ratio of original to final \ndataset size will result in the best performing model. However, imagine using color augmentations exclusively. If the initial training dataset consists of 50 dogs and 50 cats, and \neach image is augmented with 100 color filters to produce 5000 dogs and 5000 cats, this \ndataset will be heavily biased towards the spatial characteristics of the original 50 dogs \nand 50 cats. This over-extensive color-augmented data will cause a deep model to overfit \neven worse than the original. From this anecdote, we can conceptualize the existence of \nan optimal size for post-augmented data.\nAdditionally, there is no consensus about the best strategy for combining data warping \nand oversampling techniques. One important consideration is the intrinsic bias in the \ninitial, limited dataset. There are no existing augmentation techniques that can correct a \ndataset that has very poor diversity with respect to the testing data. All these augmentation algorithms perform best under the assumption that the training data and testing \ndata are both drawn from the same distribution. If this is not true, it is very unlikely that \nthese methods will be useful.\nFuture work\nFuture work in Data Augmentation will be focused on many different areas such as \nestablishing a taxonomy of augmentation techniques, improving the quality of GAN \nsamples, learning new ways to combine meta-learning and Data Augmentation, discovering relationships between Data Augmentation and classifier architecture, and extending these principles to other data types. We are interested in seeing how the time-series \ncomponent in video data impacts the use of static image augmentation techniques. Data \nAugmentation is not limited to the image domain and can be useful for text, bioinformatics, tabular records, and many more.\nOur future work intends to explore performance benchmarks across geometric and \ncolor space augmentations across several datasets from different image recognition \ntasks. These datasets will be constrained in size to test the effectiveness with respect to \nlimited data problems. Zhang et al. [135] test their novel GAN augmentation technique \non the SVHN dataset across 50, 80, 100, 200, and 500 training instances. Similar to this \nwork, we will look to further establish benchmarks for different levels of limited data.\nImproving the quality of GAN samples and testing their effectiveness on a wide range \nof datasets is another very important area for future work. We would like to further \nexplore the combinatorics of GAN samples with other augmentation techniques such as \napplying a range of style transfers to GAN-generated samples.\nSuper-resolution networks through the use of SRCNNs, Super-Resolution Convolutional Neural Networks, and SRGANs are also very interesting areas for future work \nin Data Augmentation. We want to explore the performance differences across architectures with upsampled images such as expanding CIFAR-10 images from 32 × 32 to \n64 × 64 to 128 × 128 and so on. One of the primary difficulties with GAN samples is trying to achieve high-resolution outputs. Therefore, it will be interesting to see how we \ncan use super-resolution networks to achieve high-resolution such as DCGAN samples",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 43,
          "text": "Page 43 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \ninputted into an SRCNN or SRGAN. The result of this strategy will be compared with \nthe performance of the Progressively Growing GAN architecture.\nTest-time augmentation has the potential to make a massive difference in Computer \nVision performance and has not been heavily explored. We want to establish benchmarks for different ensembles of test-time augmentations and investigate the solution \nalgorithms used. Currently, majority voting seems to be the dominant solution algorithm for test-time augmentation. It seems highly likely that test-time augmentation \ncan be further improved if the weight of each augmented images prediction is further \nparameterized and learned. Additionally, we will explore the effectiveness of test-time \naugmentation on object detection, comparing color space augmentations and the Neural \nStyle Transfer algorithm.\nMeta-learning GAN architectures is another exciting area of interest. Using Reinforcement Learning algorithms such as NAS on the generator and discriminator architectures \nseem very promising. Another interesting area of further research is to use an evolutionary approach to speed up the training of GANs through parallelization and cluster \ncomputing.\nAnother important area of future work for practical integration of Data Augmentation \ninto Deep Learning workflows is the development of software tools. Similar to how the \nTensorflow [136] system automates the back-end processes of gradient-descent learning, Data Augmentation libraries will automate preprocessing functions. The Keras [137] \nlibrary provides an ImageDataGenerator class that greatly facilitates the implementation of geometric augmentations. Buslaev et  al. presented another augmentation tool \nthey called Albumentations [138]. The development of Neural Style Transfer, adversarial training, GANs, and meta-learning APIs will help engineers utilize the performance \npower of advanced Data Augmentation techniques much faster and more easily.\nConclusion\nThis survey presents a series of Data Augmentation solutions to the problem of overfitting in Deep Learning models due to limited data. Deep Learning models rely on big \ndata to avoid overfitting. Artificially inflating datasets using the methods discussed in \nthis survey achieves the benefit of big data in the limited data domain. Data Augmentation is a very useful technique for constructing better datasets. Many augmentations \nhave been proposed which can generally be classified as either a data warping or oversampling technique.\nThe future of Data Augmentation is very bright. The use of search algorithms combining data warping and oversampling methods has enormous potential. The layered \narchitecture of deep neural networks presents many opportunities for Data Augmentation. Most of the augmentations surveyed operate in the input layer. However, some \nare derived from hidden layer representations, and one method, DisturbLabel [28], is \neven manifested in the output layer. The space of intermediate representations and the \nlabel space are under-explored areas of Data Augmentation with interesting results. This \nsurvey focuses on applications for image data, although many of these techniques and \nconcepts can be expanded to other data domains.\nData Augmentation cannot overcome all biases present in a small dataset. For example, in a dog breed classification task, if there are only bulldogs and no instances of",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 44,
          "text": "Page 44 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \ngolden retrievers, no augmentation method discussed, from SamplePairing to AutoAugment to GANs, will create a golden retriever. However, several forms of biases such as \nlighting, occlusion, scale, background, and many more are preventable or at least dramatically lessened with Data Augmentation. Overfitting is generally not as much of an \nissue with access to big data. Data Augmentation prevents overfitting by modifying limited datasets to possess the characteristics of big data.\nAbbreviations\nGAN: generative adversarial network; CNN: convolutional neural network; DCGAN: deep convolutional generative \nadversarial network; NAS: neural architecture search; SRCNN: super-resolution convolutional neural network; SRGAN: \nsuper-resolution generative adversarial network; CT: computerized tomography; MRI: magnetic resonance imaging; \nPET: positron emission tomography; ROS: random oversampling; SMOTE: synthetic minority oversampling technique; \nRGB: red-green-blue; PCA: principal components analysis; UCI: University of California Irvine; MNIST: Modified National \nInstitute of Standards and Technology; CIFAR: Canadian Institute for Advanced Research; t-SNE: t-distributed stochastic \nneighbor embedding.\nAcknowledgements\nWe would like to thank the reviewers in the Data Mining and Machine Learning Laboratory at Florida Atlantic University. \nAdditionally, we acknowledge partial support by the NSF (CNS-1427536). Opinions, findings, conclusions, or recommendations in this paper are solely of the authors’ and do not reflect the views of the NSF.\nAuthors’ contributions\nCS performed the primary literature review and analysis for this work, and also drafted the manuscript. TMK, JLL, RAB, \nRZ, KW, NS, and RK worked with CS to develop the article’s framework and focus. TMK introduced this topic to CS, and \nhelped to complete and finalize this work. All authors read and approved the final manuscript.\nFunding\nNot applicable.\nAvailability of data and materials\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nConsent for publication\nNot applicable.\nEthics approval and consent to participate\nNot applicable.\nReceived: 9 January 2019 Accepted: 22 April 2019\nReferences\n \n1. Krizhevsky A, Sutskever I, Hinton GE. ImageNet classification with deep convolutional neural networks. Adv Neural \nInf Process Syst. 2012;25:1106-14.\n \n2. Karen S, Andrew Z. Very deep convolutional networks for large-scale image recognition. arXiv e-prints. 2014.\n \n3. Kaiming H, Xiangyu Z, Shaoqing R, Jian S. Deep residual learning for image recognition. In: CVPR, 2016.\n \n4. Christian S, Vincent V, Sergey I, Jon S, Zbigniew W. Rethinking the inception architecture for computer vision. arXiv \ne-prints, 2015.\n \n5. Gao H, Zhuang L, Laurens M, Kilian QW. Densely connected convolutional networks. arXiv preprint. 2016.\n \n6. Jan K, Vladimir G, Daniel C. Regularization for deep learning: a taxonomy. arXiv preprint. 2017.\n \n7. Nitish S, Geoffrey H, Alex K, Ilya S, Ruslan S. Dropout: a simple way to prevent neural networks from overfitting. J \nMach Learn Res. 2014;15(1):1929-58.\n \n8. Jonathan T, Ross G, Arjun J, Yann L, Christoph B. Efficient object localization using convolutional networks. In: \nCVPR’15. 2015.\n \n9. Sergey I, Christan S. Batch normalization: accelerating deep network training by reducing internal covariate shift. \nIn: ICML; 2015.\n 10. Karl W, Taghi MK, DingDing W. A survey of transfer learning. J Big Data. 2016;3:9.\n 11. Shao L. Transfer learning for visual categorization: a survey. IEEE Trans Neural Netw Learn Syst. 2015;26(5):1019-34.\n 12. Jia D, Wei D, Richard S, Li-Jia L, Kai L, Li F-F. ImageNet: a large-scale hierarchical image database. In: CVPR09, 2009.\n 13. Amir Z, Alexander S, William S, Leonidas G, Jitendra M, Silvio S. Taskonomy: disentangling task transfer learning. In: \nCVPR ‘18. 2018.\n 14. Yosinski J, Clune J, Bengio Y, Lipson H. How transferable are features in deep neural networks? Adv Neural Inf \nProcess Syst. 2014;27:3320-8.",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 45,
          "text": "Page 45 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \n 15. Erhan D, Bengio Y, Courville A, Manzagol PA, Vincent P. Why does unsupervised pre-training help deep learning? J \nMach Learn Res. 2010;11:625-60.\n 16. Mark P, Dean P, Geoffrey H, Tom MM. Zero-shot learning with semantic output codes. In: NIPS; 2009.\n 17. Yongqin X, Christoph HL, Bernt S, Zeynep A. Zero-shot learning-a comprehensive evaluation of the good, the \nbad and the ugly. arXiv preprint. 2018.\n 18. Yaniv T, Ming Y, Marc’ AR, Lior W. DeepFace: closing the gap to human-level performance in face verification. In: \nCVPR ’14; 2014.\n 19. Gregory K, Richard Z, Ruslan S. Siamese neural networks for one-shot image recognition. In: ICML Deep Learning \nworkshop; 2015.\n 20. Adam S, Sergey B, Matthew B, Dean W, Timothy L. One-shot learning with memory-augmented neural networks. \narXiv preprint. 2016.\n 21. Tomas M, Ilya S, Kai C, Greg C, Jeffrey D. Distributed representations of words and phrases and their compositionality. Accepted to NIPS 2013.\n 22. Jeffrey P, Richard S, Christopher DM. GloVe: global vectors for word representation. In: Proceedings of the empirical \nmethods in natural language processing (EMNLP 2014) 12. 2014.\n 23. Halevy A, Norvig P, Pereira F. The unreasonable effectiveness of data. IEEE Intell Syst. 2009;24:8-12.\n 24. Chen S, Abhinav S, Saurabh S, Abhinav G. Revisting unreasonable effectivness of data in deep learning era. In: \nICCV; 2017. p. 843-52.\n 25. Esteva A, Kuprel B, Novoa RA, Ko J, Swetter SM, Blau HM, Thrun S. Dermatologist-level classification of skin cancer \nwith deep neural networks. Nature. 2017;542:115-8.\n 26. Geert L, Thijs K, Babak EB, Arnaud AAS, Francesco C, Mohsen G, Jeroen AWM, van Bram G, Clara IS. A survey on \ndeep learning in medical image analysis. Med Image Anal. 2017;42:60-88.\n 27. Joffrey LL, Taghi MK, Richard AB, Naeem S. A survey on addressing high-class imbalance in big data. Springer J Big \nData. 2018;5:42.\n 28. LeCun Y, Bottou L, Bengio Y, Haffner P. Gradient-based learning applied to document recognition. Proc IEEE. \n1998;86(11):2278-324.\n 29. Nitesh VC, Kevin WB, Lawrence OH, Kegelmeyer W. SMOTE: synthetic minority over-sampling technique. J Artif \nIntellig Res. 2002;16:321-57.\n 30. Hui H, Wen-Yuan W, Bing-Huan M. Borderline-SMOTE: a new over-sampling method in imbalanced data sets learning. In: Proceedings of ICIC, vol. 3644, Lecture Notes in Computer Science, New York. 2005, p. 878-87.\n 31. Ian JG, Jean PA, Mehdi M, Bing X, David WF, Sherjil O, Aaron C, Yoshua B. Generative adversarial nets. NIPS. 2014.\n 32. Leon AG, Alexander SE, Matthias B. A neural algorithm of artistic style. ArXiv. 2015.\n 33. Barret Z, Quoc VL. Neural architecture search with reinforcement learning. In: International conference on learning \nrepresentatoins, 2017.\n 34. Tero K, Timo A, Samuli L, Jaakko L. Progressive growing of GANs for improved quality, stability, and variation. CoRR, \nabs/1710.10196, 2017.\n 35. Justin J, Alexandre A, Li FF. Perceptual losses for real-time style transfer and super-resolution. ECCV. \n2016;2016:694-711.\n 36. Luis P, Jason W. The effectiveness of data augmentation in image classification using deep learning. In: Stanford \nUniversity research report, 2017.\n 37. Lemley J, Barzrafkan S, Corcoran P. Smart augmentation learning an optimal data augmentation strategy. In: IEEE \nAccess. 2017.\n 38. Ekin DC, Barret Z, Dandelion M, Vijay V, Quoc VL. AutoAugment: learning augmentation policies from data. ArXiv \npreprint. 2018.\n 39. Xin Y, Paul SB, Ekta W. Generative adversarial network in medical imaging: a review. arXiv preprint. 2018.\n 40. Jelmer MW, Tim L, Max AV, Ivana I. Generative adversarial networks for noise reduction in low-dose CT. In: IEEE \nTransactions on Medical Imaging. 2017.\n 41. Ohad S, Tammy RR. Accelerated magnetic resonance imaging by adversarial neural network. In: DLMIA/ML-CDS@\nMICCAI, 2017.\n 42. Wang Y, Biting Y, Wang L, Chen Z, Lalush DS, Lin W, Xi W, Zhou J, Shen D, Zhou L. 3D conditional generative adversarial networks for high-quality PET image estimation at low dose. NeuroImage. 2018;174:550-62.\n 43. Dwarikanath M, Behzad B. Retinal vasculature segmentation using local saliency maps and generative adversarial \nnetworks for image super resolution. arXiv preprint. 2017.\n 44. Francesco C, Aldo M, Claudio S, Giorgio T. Biomedical data augmentation using generative adversarial neural \nnetworks. In: International conference on artificial neural networks. Berlin: Springer; 2017. P. 626-34.\n 45. Camilo B, Andrew JP, Larry TD, Allen TN, Susan MR, Bennett AL. Learning implicit brain MRI manifolds with deep \nlearning. Int Soc Opt Photonics. 2018;10574:105741.\n 46. Maria JMC, Sarfaraz H, Jeremy B, Ulas B. How to fool radiologists with generative adversarial networks? A visual \nturing test for lung cancer diagnosis. arXiv preprint. 2017.\n 47. Baur C, Albarqouni S, Navab N. MelanoGANs: high resolution skin lesion synthesis with GANs. arXiv preprint. 2018.\n 48. Madani A, Moradi M, Karargyris A, Syeda-Mahmood T. Chest x-ray generation and data augmentation for cardiovascular abnormality classification. In: Medical imaging 2018. Image Processing 2018;10574:105741.\n 49. Maayan F-A, Eyal K, Jacob G, Hayit G. GAN-based data augmentation for improved liver lesion classification. arXiv \npreprint. 2018.\n 50. Joseph R, Santosh D, Ross G, Ali F. You only look once: unified, real-time object detection. In: CVPR‘16. 2016.\n 51. Ross G, Jeff D, Trevor D, Jitendra M. Rich feature hierarchies for accurate object detection and semantic segmentation. In: CVPR ‘14. 2014.\n 52. Ross G. Fast R-CNN. CoRR, abs/1504.08083. 2015.\n 53. Shaoqing R, Kaiming H, Ross G, Jian S. Faster R-CNN: towards real-time object detection with region proposal \nnetworks. In: NIPS, 2015.\n 54. Jonathan L, Evan S, Trevor D. Fully convolutional networks for semantic segmentation. CoRR, abs/1411.4038. 2014.",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 46,
          "text": "Page 46 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \n 55. Olaf R, Philipp F, Thomas B. U-Net: convolutional networks for biomedical image segmentation. In: MICCAI. \nSpringer; 2015, p. 234-41.\n 56. Hessam B, Maxwell H, Mohammad R, Ali F. Label refinery: improving imagenet classification through label progression. arXiv preprint. 2018.\n 57. Francisco JM-B, Fiammetta S, Jose MJ, Daniel U, Leonardo F. Forward noise adjustment scheme for data augmentation. arXiv preprints. 2018.\n 58. Dua D, Karra TE. UCI machine learning repository [ ve.ics.uci.edu/ml]. Irvine, CA: University of California, \nSchool of Information and Computer Science; 2017.\n 59. Ken C, Karen S, Andrea V, Andrew Z. Return of the devil in the details: delving deep into convolutional nets. In: \nProceedings of BMVC. 2014.\n 60. Mark E, Luc VG, Christopher KIW, John W, Andrew Z. The pascal visual object classes (VOC) challenge. \npasca l-netwo rk.org/chall enges /VOC/voc20 08/works hop/. 2008.\n 61. Aranzazu J, Miguel P, Mikel G, Carlos L-M, Daniel P. A comparison study of different color spaces in clustering based \nimage segmentation. IPMU; 2010.\n 62. Quanzeng Y, Jiebo L, Hailin J, Jianchao Y. Robust image sentiment analysis using progressively trained and domain \ntransferred deep networks. In: AAAI. 2015, p. 381-8.\n 63. Luke T, Geoff N. Improving deep learning using generic data augmentation. arXiv preprint. 2017.\n 64. Guoliang K, Xuanyi D, Liang Z, Yi Y. PatchShuffle regularization. arXiv preprint. 2017.\n 65. Hiroshi I. Data augmentation by pairing samples for images classification. ArXiv e-prints. 2018.\n 66. Cecilia S, Michael JD. Improved mixed-example data augmentation. ArXiv preprint. 2018.\n 67. Daojun L, Feng Y, Tian Z, Peter Y. Understanding mixup training methods. In: IEEE access. 2018. p. 1.\n 68. Ryo T, Takashi M. Data augmentation using random image cropping and patches for deep CNNs. arXiv preprints. \n2018.\n 69. Yoshua B, Jerome L, Ronan C, Jason W. Curriculum learning. In: Proceedings of the 26th annual international \nconference on machine learning, ACM. 2009, p. 41-8.\n 70. Zhun Z, Liang Z, Guoliang K, Shaozi L, Yi Y. Random erasing data augmentation. ArXiv e-prints. 2017.\n 71. Terrance V, Graham WT. Improved regularization of convolutional neural networks with cutout. arXiv preprint. \n2017.\n 72. Agnieszka M, Michal G. Data augmentation for improving deep learning in image classification problem. In: IEEE \n2018 international interdisciplinary Ph.D. Workshop, 2018.\n 73. Jonathan K, Michael S, Jia D, Li F-F. 3D object representations for fine-grained categorization. In: 4th IEEE Workshop on 3D Representation and Recognition, at ICCV 2013 (3dRR-13). Sydney, Australia. Dec. 8, 2013.\n 74. Tomohiko K, Michiaki I. Icing on the cake: an easy and quick post-learning method you can try after deep learning. \narXiv preprints. 2018.\n 75. Terrance V, Graham WT. Dataset augmentation in feature space. In: Proceedings of the international conference on \nmachine learning (ICML), workshop track, 2017.\n 76. Sebastien CW, Adam G, Victor S, Mark DM. Understanding data augmentation for classification: when to warp? \nCoRR, abs/1609.08764, 2016.\n 77. Seyed-Mohsen MD, Alhussein F, Pascal F. DeepFool: a simple and accurate method to fool deep neural networks. \narXiv preprint. 2016.\n 78. Jiawei S, Danilo VV, Sakurai K. One pixel attack for fooling deep neural networoks. arXiv preprints. 2018.\n 79. Michal Z, Konrad Z, Negar R, Pedro OP. Adversarial framing for image and video classification. arXiv preprints. 2018.\n 80. Logan E, Brandon T, Dimitris T, Ludwig S, Aleksander M. A rotation and a translation suffice: fooling CNNs with \nsimple transformations. ArXiv preprint. 2018.\n 81. Goodfellow I, Shlens J, Szegedy C. Explaining and Harnessing Adversarial Examples. International Conference on \nLearning Representations, 2015.\n 82. Ian JG, David W-F, Mehdi M, Aaron C, Yoshua B. Maxout networks. arXiv preprint. 2013.\n 83. Shuangtao L, Yuanke C, Yanlin P, Lin B. Learning more robust features with adversarial training. ArXiv preprints. \n2018.\n 84. Lingxi X, Jingdong W, Zhen W, Meng W, Qi T. DisturbLabel: regularizing CNN on the loss layer. arXiv preprint. 2016.\n 85. Christopher B, Liang C, Ricardo GPB, Roger G, Alexander H, David AD, Maria VH, Joanna W, Daniel R. GAN augmentation: augmenting training data using generative adversarial networks. arXiv preprint. 2018.\n 86. Doersch C. Tutorial on Variational Autoencoders. ArXiv e-prints. 2016.\n 87. Laurens M, Geoffrey H. Visualizing data using t-SNE. J Mach Learn Res. 2008;9:2431-56.\n 88. Jeff D, Philipp K, Trevor D. Adversarial feature learning. In: CVPR’16. 2016.\n 89. Lin Z, Shi Y, Xue Z. IDSGAN: Generative Adversarial Networks for Attack Generation against Intrusion Detection. \narXiv preprint; 2018.\n 90. William F, Mihaela R, Balaji L, Andrew MD, Shakir M, Ian G. Many paths to equilibrium: GANs do not need to \ndecrease a divergence at every step. In: International conference on learning representations (ICLR); 2017.\n 91. Alec R, Luke M, Soumith C. Unsupervised representation learning with deep convolutional generative adversarial \nnetworks. ICLR, 2016.\n 92. Jun-Yan Z, Taesung P, Phillip I, Alexei AE. Unpaired image-to-image translation using cycle-consistent adversarial \nnetworks. In: International conference on cmoputer vision (ICCV), 2017.\n 93. Xinyue Z, Yifan L, Zengchang Q, Jiahong L. Emotion classification with data augmentation using generative adversarial networks. CoRR, vol. abs/1711.00648. 2017.\n 94. Goodfellow IJ, Erhan D, Carrier PL, Courville A, Mirza M, Hamner B, Cukierski W, Tang Y, Thaler D, Lee DH, et al. Challenges in representation learning: A report on three machine learning contests. In: NIPS. Berlin: Springer; 2013. \np.117-24.\n 95. Mehdi M, Simon O. Conditional generative adversarial nets. arXiv preprint. 2014.\n 96. Mario L, Karol K, Marcin M, Olivier B, Sylvain G. Are GANs created equal? A large-scale study. arXiv preprint. 2018.",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 47,
          "text": "Page 47 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \n 97. Swee KL, Yi L, Ngoc-Trung T, Ngai-Man C, Gemma R, Yuval E. DOPING: generative data augmentation for unsupervised anomaly detection with GAN. arXiv preprint. 2018.\n 98. Alireza M, Jonathon S, Navdeep J, Ian G, Brendan F. Adversarial autoencoders. arXiv preprint. 2015.\n 99. Tim S, Ian G, Wojciech Z, Vicki C, Alec R, Xi C. Improved techniques for training GANs. arXiv preprint. 2016.\n 100. Yanghao L, Naiyan W, Jiaying L, Xiaodi H. Demistifying neural style transfer. arXiv preprint. 2017.\n 101. Khizar H. Super-resolution via deep learning. arXiv preprint. 2017.\n 102. Dmitry U, Andrea V, Victor L. Instance normalization: the missing ingredient for fast stylization. arXiv preprint. 2016.\n 103. Philip TJ, Amir AA, Stephen B, Toby B, Boguslaw O. Style augmentation: data augmentation via style randomization. arXiv e-prints. 2018.\n 104. Josh T, Rachel F, Alex R, Jonas S, Wojciech Z, Pieter A. Domain randomization for transferring deep neural networks \nfrom simulation to the real world. arXiv preprint. 2017.\n 105. Ashish S, Tomas P, Oncel T, Josh S, Wenda W, Russ W. Learning from simulated and unsupervised images through \nadversarial training. In: Conference on computer vision and pattern recognition, 2017.\n 106. Stephan RR, Vibhav V, Stefan R, Vladlen K. Playing for data: ground truth from computer games. In: European \nconference on computer vision (ECCV); 2016.\n 107. Brostow Gabriel J, Fauqueur Julien, Cipolla Roberto. Semantic object classes in video: a high-definition ground \ntruth database. Pattern Recogn Lett. 2008;30(2):88-97.\n 108. Marius C, Mohamed O, Sebastian R, Timo R, Markus E, Rodrigo B, Uwe F, Stefan R, Bernt S. The cityscape dataset for \nsemantic urban scene understanding. In: CVPR; 2016.\n 109. Esteban R, Sherry M, Andrew S, Saurabh S, Yutaka LS, Jie T, Quoc VL, Alexey K. Large-scale evolution of image classifiers. In: Proceedings of the 34th international conference on machine learning (ICML ‘17). 2017.\n 110. Esteban R, Alok A, Yanping H, Quoc VL. Regularized evolution for image classifier architecture search. arXiv preprint. 2018.\n 111. Tim S, Jonathan H, Xi C, Szymon S, Ilya S. Evolution strategies as a scalable alternative to reinforcement learning. \narXiv e-prints. 2017.\n 112. Horia M, Aurelia G, Benjamin R. Simple random search provides a competitive approach to reinforcement learning. \nIn: Advances in neural information processing systems (NIPS); 2018.\n 113. David GL. Distinctive image features from scale-invariant keypoints. Int J Comput Vis. 2004;2004:91-110.\n 114. Navneet D, Bill T. Histograms of oriented gradients for human detection. In: CVPR, 2005.\n 115. Sutton RS, Reinforcement AG. Learning: an introduction. New York: MIT Press; 1998.\n 116. Mingyang G, Kele X, Bo D, Huaimin W, Lei Z. Learning data augmentation policies using augmented random \nsearch. arXiv preprint. 2018.\n 117. Tran NM, Mathieu S, Hoang TL, Martin W. Automated image data preprocessing with deep reinforcement learning. \narXiv preprints. 2018.\n 118. Hochreiter S. The vanishing gradient problem during learning recurrent neural nets and problem solutions. Int J \nUncertain Fuzzin Know Based Syst. 1998;6(02):107-16.\n 119. Jia S, Wang P, Jia P, Hu S. Research on data augmentation for image classification based on convolutional neural \nnetworks. In: 2017 Chinese automation congress (CAC), 2017. p. 4165-70.\n 120. Ilija R, Piotr D, Ross G, Georgia G, Kaiming H. Data distillation: towards omni-supervised learning. In: CVPR ’18; 2018.\n 121. Guotai W, Michael A, Sebastien O, Wenqi L, Jan D, Tom V. Test-time augmentation with uncertainty estimation for \ndeep learning-based medical image segmentation. OpenReview.net. 2018.\n 122. Fabio P, Christina V, Sandra A, Eduardo V. Data augmentation for skin lesion analysis. In: ISIC skin image analysis \nworkshop and challenge @ MICCAI 2018. 2018.\n 123. Karzuhisa M, Akira H, Akane M, Hiroshi K. Image classification of melanoma, nevus and seborrheic keratosis by \ndeep neural network ensemble. In: International skin imaging collaboration (ISIC) 2017 challenge at the international symposium on biomedical imaging (ISBI). 2017.\n 124. Max J, Karen S, Andrea V, Andrew Z. Synthetic data and artificial neural networks for natural scene text recognition. \narXiv preprint. 2014.\n 125. Florian S, Dmitry K, James P. FaceNet: a unified embedding for face recognition and clustering. In: CVPR ‘15. 2015.\n 126. Xudong M, Qing L, Haoran X, Raymond YKL, Zhen W, Stephen PS. Least squares generative adversarial networks. \nIn: International conference on computer vision (ICCV), 2017.\n 127. Ren W, Shengen Y, Yi S, Qingqing D, Gang S. Deep image: scaling up image recognition. CoRR, abs/1501.02876, \n2015.\n 128. Chao D, Chen CL, Kaiming H, Ziaoou T. Learning a deep convolutional network for image super-resolution. In: \nECCV. Berlin: Springer; 2014. , p. 184-99.\n 129. Christian L, Lucas T, Ferenc H, Jose C, Andrew C, Alejandro A, Andrew A, Alykhan T, Johannes T, Zehan W, Wenzhe \nS. Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint. 2016.\n 130. Han Z, Tao X, Hongsheng L, Shaoting Z, Xiaogang W, Xiaolei H, Dimitris M. StackGAN: text to photo-realistic image \nsynthesis with stacked generative adversarial networks. In: ICCV, 2017.\n 131. Trishul C, Yutaka S, Johnson A, Karthik K. Project adam: building an efficient and scalable deep learning training \nsystem. In: Proceedings of OSDI. 2014. P. 571-82.\n 132. Buda Mateusz, Maki Atsuto, Mazurowski Maciej A. A systematic study of the class imbalance problem in convolutional neural networks. Neural Networks. 2018;106:249-59.\n 133. Drown DJ, Khoshgoftaar TM, Seliya N. Evolutionary sampling and software quality modeling of high-assurance \nsystems. IEEE Trans Syst. 2009;39(5):1097-107.\n 134. Jason Y, Jeff C, Anh N, Thomas F, Hod L. Understanding neural networks through deep visualization. In: European \nconference on computer vision (ECCV). Berlin: Springer; 2015. p. 818-33.\n 135. Xiaofeng Z, Zhangyang W, Dong L, Qing L. DADA: deep adversarial data augmentation for extremely low data \nregime classification. arXiv preprint. 2018.\n 136. Martin A, Paul B, Jianmin C, Zhifeng C, Andy D, Jeffrey D, Matthieu D, Sanjay G, Geoffrey I, Michael I, Manjunath \nK, Josh L, Rajat M, Sherry M, Derek GM, Benoit S, Paul T, Vijay V, Pete W, Matrin W, Yuan Y, Xiaoqiang Z. TensorFlow:",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 48,
          "text": "Page 48 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \na system for large-scale machine learning. In: Proceedings of the 12th USENIX symposium on operating system \ndesign and implementation (OSDI ‘16), 2016.\n 137. Keras  ://keras .io/. 2015.\n 138. Alexander B, Alex P, Eugene K, Vladimir II, Alexandr AK. Albumentations: fast and flexible image augmentations. \nArXiv preprints. 2018.\n 139. Maayan F-A, Idit D, Eyal K, Michal A, Jacob G, Hayit G. GAN-based synthetic medical image augmentation for \nincreased CNN performance in liver lesion classification. arXiv preprint. 2018.\n 140. Changhee H, Hideaki H, Leonardo R, Ryosuke A, Wataru S, Shinichi M, Yujiro F, Giancarlo M, Hideki N. GAN-based \nsynthetic brain mr image generation. In: 2018 IEEE 15th International Symposium on biomedical imaging (ISBI \n2018). IEEE, 2011. P. 734-8.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
          "method": "pymupdf",
          "page_width": 595.2760009765625,
          "page_height": 790.8660278320312,
          "has_images": false,
          "image_count": 0
        }
      ],
      "sections": {
        "title": "Not applicable.\nReceived: 9 January 2019 Accepted: 22 April 2019",
        "abstract": "A survey on Image Data Augmentation \nfor Deep Learning\nConnor Shorten* and Taghi M. Khoshgoftaar",
        "introduction": "Deep Learning models have made incredible progress in discriminative tasks. This has \nbeen fueled by the advancement of deep network architectures, powerful computation, \nand access to big data. Deep neural networks have been successfully applied to Computer Vision tasks such as image classification, object detection, and image segmentation thanks to the development of convolutional neural networks (CNNs). These neural \nnetworks utilize parameterized, sparsely connected kernels which preserve the spatial \ncharacteristics of images. Convolutional layers sequentially downsample the spatial \nresolution of images while expanding the depth of their feature maps. This series of \nconvolutional transformations can create much lower-dimensional and more useful representations of images than what could possibly be hand-crafted. The success of CNNs \nhas spiked interest and optimism in applying Deep Learning to Computer Vision tasks.",
        "literature_review": "",
        "methodology": "",
        "results": "",
        "discussion": "The interesting ways to augment image data fall into two general categories: data warping and oversampling. Many of these augmentations elucidate how an image classifier \ncan be improved, while others do not. It is easy to explain the benefit of horizontal flipping or random cropping. However, it is not clear why mixing pixels or entire images \ntogether such as in PatchShuffle regularization or SamplePairing is so effective. Additionally, it is difficult to interpret the representations learned by neural networks for \nGAN-based augmentation, variational auto-encoders, and meta-learning. CNN visualization has been led by Yosinski et al. [134] with their deep visualization method. Having a human-level understanding of convolutional networks features could greatly help \nguide the augmentation process.\nManipulating the representation power of neural networks is being used in many \ninteresting ways to further the advancement of augmentation techniques. Traditional \nhand-crafted augmentation techniques such as cropping, flipping, and altering the color \nspace are being extended with the use of GANs, Neural Style Transfer, and meta-learning search algorithms.\nImage-to-image translation has many potential uses in Data Augmentation. Neural \nStyle Transfer uses neural layers to translate images into new styles. This technique not \nonly utilizes neural representations to separate ‘style’ and ‘content’ from images, but also \nuses neural transformations to transfer the style of one image into another. Neural Style \nTransfer is a much more powerful augmentation technique than traditional color space \naugmentations, but even these methods can be combined together.\nAn interesting characteristic of these augmentation methods is their ability to be combined together. For example, the random erasing technique can be stacked on top of any \nof these augmentation methods. The GAN framework possesses an intrinsic property \nof recursion which is very interesting. Samples taken from GANs can be augmented \nwith traditional augmentations such as lighting filters, or even used in neural network \naugmentation strategies such as Smart Augmentation or Neural Augmentation to create even more samples. These samples can be fed into further GANs and dramatically \nincrease the size of the original dataset. The extensibility of the GAN framework is \namongst many reasons they are so interesting to Deep Learning researchers.\nTest-time augmentation is analogous to ensemble learning in the data space. Instead \nof aggregating the predictions of different learning algorithms, we aggregate predictions \nacross augmented images. We can even extend the solution algorithm to parameterize \nprediction weights from different augmentations. This seems like a good solution for systems concerned with achieving very high performance scores, more so than prediction\n\nPage 42 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \nspeed. Determining the effectiveness of test-time augmentation by primarily exploring \ntest-time geometric transformations and Neural Style Transfer, is an area of future work.\nAn interesting question for practical Data Augmentation is how to determine postaugmented dataset size. There is no consensus as to which ratio of original to final \ndataset size will result in the best performing model. However, imagine using color augmentations exclusively. If the initial training dataset consists of 50 dogs and 50 cats, and \neach image is augmented with 100 color filters to produce 5000 dogs and 5000 cats, this \ndataset will be heavily biased towards the spatial characteristics of the original 50 dogs \nand 50 cats. This over-extensive color-augmented data will cause a deep model to overfit \neven worse than the original. From this anecdote, we can conceptualize the existence of \nan optimal size for post-augmented data.\nAdditionally, there is no consensus about the best strategy for combining data warping \nand oversampling techniques. One important consideration is the intrinsic bias in the \ninitial, limited dataset. There are no existing augmentation techniques that can correct a \ndataset that has very poor diversity with respect to the testing data. All these augmentation algorithms perform best under the assumption that the training data and testing \ndata are both drawn from the same distribution. If this is not true, it is very unlikely that \nthese methods will be useful.\nFuture work\nFuture work in Data Augmentation will be focused on many different areas such as \nestablishing a taxonomy of augmentation techniques, improving the quality of GAN \nsamples, learning new ways to combine meta-learning and Data Augmentation, discovering relationships between Data Augmentation and classifier architecture, and extending these principles to other data types. We are interested in seeing how the time-series \ncomponent in video data impacts the use of static image augmentation techniques. Data \nAugmentation is not limited to the image domain and can be useful for text, bioinformatics, tabular records, and many more.\nOur future work intends to explore performance benchmarks across geometric and \ncolor space augmentations across several datasets from different image recognition \ntasks. These datasets will be constrained in size to test the effectiveness with respect to \nlimited data problems. Zhang et al. [135] test their novel GAN augmentation technique \non the SVHN dataset across 50, 80, 100, 200, and 500 training instances. Similar to this \nwork, we will look to further establish benchmarks for different levels of limited data.\nImproving the quality of GAN samples and testing their effectiveness on a wide range \nof datasets is another very important area for future work. We would like to further \nexplore the combinatorics of GAN samples with other augmentation techniques such as \napplying a range of style transfers to GAN-generated samples.\nSuper-resolution networks through the use of SRCNNs, Super-Resolution Convolutional Neural Networks, and SRGANs are also very interesting areas for future work \nin Data Augmentation. We want to explore the performance differences across architectures with upsampled images such as expanding CIFAR-10 images from 32 × 32 to \n64 × 64 to 128 × 128 and so on. One of the primary difficulties with GAN samples is trying to achieve high-resolution outputs. Therefore, it will be interesting to see how we \ncan use super-resolution networks to achieve high-resolution such as DCGAN samples\n\nPage 43 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \ninputted into an SRCNN or SRGAN. The result of this strategy will be compared with \nthe performance of the Progressively Growing GAN architecture.\nTest-time augmentation has the potential to make a massive difference in Computer \nVision performance and has not been heavily explored. We want to establish benchmarks for different ensembles of test-time augmentations and investigate the solution \nalgorithms used. Currently, majority voting seems to be the dominant solution algorithm for test-time augmentation. It seems highly likely that test-time augmentation \ncan be further improved if the weight of each augmented images prediction is further \nparameterized and learned. Additionally, we will explore the effectiveness of test-time \naugmentation on object detection, comparing color space augmentations and the Neural \nStyle Transfer algorithm.\nMeta-learning GAN architectures is another exciting area of interest. Using Reinforcement Learning algorithms such as NAS on the generator and discriminator architectures \nseem very promising. Another interesting area of further research is to use an evolutionary approach to speed up the training of GANs through parallelization and cluster \ncomputing.\nAnother important area of future work for practical integration of Data Augmentation \ninto Deep Learning workflows is the development of software tools. Similar to how the \nTensorflow [136] system automates the back-end processes of gradient-descent learning, Data Augmentation libraries will automate preprocessing functions. The Keras [137] \nlibrary provides an ImageDataGenerator class that greatly facilitates the implementation of geometric augmentations. Buslaev et  al. presented another augmentation tool \nthey called Albumentations [138]. The development of Neural Style Transfer, adversarial training, GANs, and meta-learning APIs will help engineers utilize the performance \npower of advanced Data Augmentation techniques much faster and more easily.",
        "conclusion": "This survey presents a series of Data Augmentation solutions to the problem of overfitting in Deep Learning models due to limited data. Deep Learning models rely on big \ndata to avoid overfitting. Artificially inflating datasets using the methods discussed in \nthis survey achieves the benefit of big data in the limited data domain. Data Augmentation is a very useful technique for constructing better datasets. Many augmentations \nhave been proposed which can generally be classified as either a data warping or oversampling technique.\nThe future of Data Augmentation is very bright. The use of search algorithms combining data warping and oversampling methods has enormous potential. The layered \narchitecture of deep neural networks presents many opportunities for Data Augmentation. Most of the augmentations surveyed operate in the input layer. However, some \nare derived from hidden layer representations, and one method, DisturbLabel [28], is \neven manifested in the output layer. The space of intermediate representations and the \nlabel space are under-explored areas of Data Augmentation with interesting results. This \nsurvey focuses on applications for image data, although many of these techniques and \nconcepts can be expanded to other data domains.\nData Augmentation cannot overcome all biases present in a small dataset. For example, in a dog breed classification task, if there are only bulldogs and no instances of\n\nPage 44 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \ngolden retrievers, no augmentation method discussed, from SamplePairing to AutoAugment to GANs, will create a golden retriever. However, several forms of biases such as \nlighting, occlusion, scale, background, and many more are preventable or at least dramatically lessened with Data Augmentation. Overfitting is generally not as much of an \nissue with access to big data. Data Augmentation prevents overfitting by modifying limited datasets to possess the characteristics of big data.\nAbbreviations\nGAN: generative adversarial network; CNN: convolutional neural network; DCGAN: deep convolutional generative \nadversarial network; NAS: neural architecture search; SRCNN: super-resolution convolutional neural network; SRGAN: \nsuper-resolution generative adversarial network; CT: computerized tomography; MRI: magnetic resonance imaging; \nPET: positron emission tomography; ROS: random oversampling; SMOTE: synthetic minority oversampling technique; \nRGB: red-green-blue; PCA: principal components analysis; UCI: University of California Irvine; MNIST: Modified National \nInstitute of Standards and Technology; CIFAR: Canadian Institute for Advanced Research; t-SNE: t-distributed stochastic \nneighbor embedding.\nAcknowledgements\nWe would like to thank the reviewers in the Data Mining and Machine Learning Laboratory at Florida Atlantic University. \nAdditionally, we acknowledge partial support by the NSF (CNS-1427536). Opinions, findings, conclusions, or recommendations in this paper are solely of the authors’ and do not reflect the views of the NSF.\nAuthors’ contributions\nCS performed the primary literature review and analysis for this work, and also drafted the manuscript. TMK, JLL, RAB, \nRZ, KW, NS, and RK worked with CS to develop the article’s framework and focus. TMK introduced this topic to CS, and \nhelped to complete and finalize this work. All authors read and approved the final manuscript.\nFunding\nNot applicable.\nAvailability of data and materials\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nConsent for publication\nNot applicable.",
        "references": "1. Krizhevsky A, Sutskever I, Hinton GE. ImageNet classification with deep convolutional neural networks. Adv Neural \nInf Process Syst. 2012;25:1106-14.\n \n2. Karen S, Andrew Z. Very deep convolutional networks for large-scale image recognition. arXiv e-prints. 2014.\n \n3. Kaiming H, Xiangyu Z, Shaoqing R, Jian S. Deep residual learning for image recognition. In: CVPR, 2016.\n \n4. Christian S, Vincent V, Sergey I, Jon S, Zbigniew W. Rethinking the inception architecture for computer vision. arXiv \ne-prints, 2015.\n \n5. Gao H, Zhuang L, Laurens M, Kilian QW. Densely connected convolutional networks. arXiv preprint. 2016.\n \n6. Jan K, Vladimir G, Daniel C. Regularization for deep learning: a taxonomy. arXiv preprint. 2017.\n \n7. Nitish S, Geoffrey H, Alex K, Ilya S, Ruslan S. Dropout: a simple way to prevent neural networks from overfitting. J \nMach Learn Res. 2014;15(1):1929-58.\n \n8. Jonathan T, Ross G, Arjun J, Yann L, Christoph B. Efficient object localization using convolutional networks. In: \nCVPR’15. 2015.\n \n9. Sergey I, Christan S. Batch normalization: accelerating deep network training by reducing internal covariate shift. \nIn: ICML; 2015.\n 10. Karl W, Taghi MK, DingDing W. A survey of transfer learning. J Big Data. 2016;3:9.\n 11. Shao L. Transfer learning for visual categorization: a survey. IEEE Trans Neural Netw Learn Syst. 2015;26(5):1019-34.\n 12. Jia D, Wei D, Richard S, Li-Jia L, Kai L, Li F-F. ImageNet: a large-scale hierarchical image database. In: CVPR09, 2009.\n 13. Amir Z, Alexander S, William S, Leonidas G, Jitendra M, Silvio S. Taskonomy: disentangling task transfer learning. In: \nCVPR ‘18. 2018.\n 14. Yosinski J, Clune J, Bengio Y, Lipson H. How transferable are features in deep neural networks? Adv Neural Inf \nProcess Syst. 2014;27:3320-8.\n\nPage 45 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \n 15. Erhan D, Bengio Y, Courville A, Manzagol PA, Vincent P. Why does unsupervised pre-training help deep learning? J \nMach Learn Res. 2010;11:625-60.\n 16. Mark P, Dean P, Geoffrey H, Tom MM. Zero-shot learning with semantic output codes. In: NIPS; 2009.\n 17. Yongqin X, Christoph HL, Bernt S, Zeynep A. Zero-shot learning-a comprehensive evaluation of the good, the \nbad and the ugly. arXiv preprint. 2018.\n 18. Yaniv T, Ming Y, Marc’ AR, Lior W. DeepFace: closing the gap to human-level performance in face verification. In: \nCVPR ’14; 2014.\n 19. Gregory K, Richard Z, Ruslan S. Siamese neural networks for one-shot image recognition. In: ICML Deep Learning \nworkshop; 2015.\n 20. Adam S, Sergey B, Matthew B, Dean W, Timothy L. One-shot learning with memory-augmented neural networks. \narXiv preprint. 2016.\n 21. Tomas M, Ilya S, Kai C, Greg C, Jeffrey D. Distributed representations of words and phrases and their compositionality. Accepted to NIPS 2013.\n 22. Jeffrey P, Richard S, Christopher DM. GloVe: global vectors for word representation. In: Proceedings of the empirical \nmethods in natural language processing (EMNLP 2014) 12. 2014.\n 23. Halevy A, Norvig P, Pereira F. The unreasonable effectiveness of data. IEEE Intell Syst. 2009;24:8-12.\n 24. Chen S, Abhinav S, Saurabh S, Abhinav G. Revisting unreasonable effectivness of data in deep learning era. In: \nICCV; 2017. p. 843-52.\n 25. Esteva A, Kuprel B, Novoa RA, Ko J, Swetter SM, Blau HM, Thrun S. Dermatologist-level classification of skin cancer \nwith deep neural networks. Nature. 2017;542:115-8.\n 26. Geert L, Thijs K, Babak EB, Arnaud AAS, Francesco C, Mohsen G, Jeroen AWM, van Bram G, Clara IS. A survey on \ndeep learning in medical image analysis. Med Image Anal. 2017;42:60-88.\n 27. Joffrey LL, Taghi MK, Richard AB, Naeem S. A survey on addressing high-class imbalance in big data. Springer J Big \nData. 2018;5:42.\n 28. LeCun Y, Bottou L, Bengio Y, Haffner P. Gradient-based learning applied to document recognition. Proc IEEE. \n1998;86(11):2278-324.\n 29. Nitesh VC, Kevin WB, Lawrence OH, Kegelmeyer W. SMOTE: synthetic minority over-sampling technique. J Artif \nIntellig Res. 2002;16:321-57.\n 30. Hui H, Wen-Yuan W, Bing-Huan M. Borderline-SMOTE: a new over-sampling method in imbalanced data sets learning. In: Proceedings of ICIC, vol. 3644, Lecture Notes in Computer Science, New York. 2005, p. 878-87.\n 31. Ian JG, Jean PA, Mehdi M, Bing X, David WF, Sherjil O, Aaron C, Yoshua B. Generative adversarial nets. NIPS. 2014.\n 32. Leon AG, Alexander SE, Matthias B. A neural algorithm of artistic style. ArXiv. 2015.\n 33. Barret Z, Quoc VL. Neural architecture search with reinforcement learning. In: International conference on learning \nrepresentatoins, 2017.\n 34. Tero K, Timo A, Samuli L, Jaakko L. Progressive growing of GANs for improved quality, stability, and variation. CoRR, \nabs/1710.10196, 2017.\n 35. Justin J, Alexandre A, Li FF. Perceptual losses for real-time style transfer and super-resolution. ECCV. \n2016;2016:694-711.\n 36. Luis P, Jason W. The effectiveness of data augmentation in image classification using deep learning. In: Stanford \nUniversity research report, 2017.\n 37. Lemley J, Barzrafkan S, Corcoran P. Smart augmentation learning an optimal data augmentation strategy. In: IEEE \nAccess. 2017.\n 38. Ekin DC, Barret Z, Dandelion M, Vijay V, Quoc VL. AutoAugment: learning augmentation policies from data. ArXiv \npreprint. 2018.\n 39. Xin Y, Paul SB, Ekta W. Generative adversarial network in medical imaging: a review. arXiv preprint. 2018.\n 40. Jelmer MW, Tim L, Max AV, Ivana I. Generative adversarial networks for noise reduction in low-dose CT. In: IEEE \nTransactions on Medical Imaging. 2017.\n 41. Ohad S, Tammy RR. Accelerated magnetic resonance imaging by adversarial neural network. In: DLMIA/ML-CDS@\nMICCAI, 2017.\n 42. Wang Y, Biting Y, Wang L, Chen Z, Lalush DS, Lin W, Xi W, Zhou J, Shen D, Zhou L. 3D conditional generative adversarial networks for high-quality PET image estimation at low dose. NeuroImage. 2018;174:550-62.\n 43. Dwarikanath M, Behzad B. Retinal vasculature segmentation using local saliency maps and generative adversarial \nnetworks for image super resolution. arXiv preprint. 2017.\n 44. Francesco C, Aldo M, Claudio S, Giorgio T. Biomedical data augmentation using generative adversarial neural \nnetworks. In: International conference on artificial neural networks. Berlin: Springer; 2017. P. 626-34.\n 45. Camilo B, Andrew JP, Larry TD, Allen TN, Susan MR, Bennett AL. Learning implicit brain MRI manifolds with deep \nlearning. Int Soc Opt Photonics. 2018;10574:105741.\n 46. Maria JMC, Sarfaraz H, Jeremy B, Ulas B. How to fool radiologists with generative adversarial networks? A visual \nturing test for lung cancer diagnosis. arXiv preprint. 2017.\n 47. Baur C, Albarqouni S, Navab N. MelanoGANs: high resolution skin lesion synthesis with GANs. arXiv preprint. 2018.\n 48. Madani A, Moradi M, Karargyris A, Syeda-Mahmood T. Chest x-ray generation and data augmentation for cardiovascular abnormality classification. In: Medical imaging 2018. Image Processing 2018;10574:105741.\n 49. Maayan F-A, Eyal K, Jacob G, Hayit G. GAN-based data augmentation for improved liver lesion classification. arXiv \npreprint. 2018.\n 50. Joseph R, Santosh D, Ross G, Ali F. You only look once: unified, real-time object detection. In: CVPR‘16. 2016.\n 51. Ross G, Jeff D, Trevor D, Jitendra M. Rich feature hierarchies for accurate object detection and semantic segmentation. In: CVPR ‘14. 2014.\n 52. Ross G. Fast R-CNN. CoRR, abs/1504.08083. 2015.\n 53. Shaoqing R, Kaiming H, Ross G, Jian S. Faster R-CNN: towards real-time object detection with region proposal \nnetworks. In: NIPS, 2015.\n 54. Jonathan L, Evan S, Trevor D. Fully convolutional networks for semantic segmentation. CoRR, abs/1411.4038. 2014.\n\nPage 46 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \n 55. Olaf R, Philipp F, Thomas B. U-Net: convolutional networks for biomedical image segmentation. In: MICCAI. \nSpringer; 2015, p. 234-41.\n 56. Hessam B, Maxwell H, Mohammad R, Ali F. Label refinery: improving imagenet classification through label progression. arXiv preprint. 2018.\n 57. Francisco JM-B, Fiammetta S, Jose MJ, Daniel U, Leonardo F. Forward noise adjustment scheme for data augmentation. arXiv preprints. 2018.\n 58. Dua D, Karra TE. UCI machine learning repository [ ve.ics.uci.edu/ml]. Irvine, CA: University of California, \nSchool of Information and Computer Science; 2017.\n 59. Ken C, Karen S, Andrea V, Andrew Z. Return of the devil in the details: delving deep into convolutional nets. In: \nProceedings of BMVC. 2014.\n 60. Mark E, Luc VG, Christopher KIW, John W, Andrew Z. The pascal visual object classes (VOC) challenge. \npasca l-netwo rk.org/chall enges /VOC/voc20 08/works hop/. 2008.\n 61. Aranzazu J, Miguel P, Mikel G, Carlos L-M, Daniel P. A comparison study of different color spaces in clustering based \nimage segmentation. IPMU; 2010.\n 62. Quanzeng Y, Jiebo L, Hailin J, Jianchao Y. Robust image sentiment analysis using progressively trained and domain \ntransferred deep networks. In: AAAI. 2015, p. 381-8.\n 63. Luke T, Geoff N. Improving deep learning using generic data augmentation. arXiv preprint. 2017.\n 64. Guoliang K, Xuanyi D, Liang Z, Yi Y. PatchShuffle regularization. arXiv preprint. 2017.\n 65. Hiroshi I. Data augmentation by pairing samples for images classification. ArXiv e-prints. 2018.\n 66. Cecilia S, Michael JD. Improved mixed-example data augmentation. ArXiv preprint. 2018.\n 67. Daojun L, Feng Y, Tian Z, Peter Y. Understanding mixup training methods. In: IEEE access. 2018. p. 1.\n 68. Ryo T, Takashi M. Data augmentation using random image cropping and patches for deep CNNs. arXiv preprints. \n2018.\n 69. Yoshua B, Jerome L, Ronan C, Jason W. Curriculum learning. In: Proceedings of the 26th annual international \nconference on machine learning, ACM. 2009, p. 41-8.\n 70. Zhun Z, Liang Z, Guoliang K, Shaozi L, Yi Y. Random erasing data augmentation. ArXiv e-prints. 2017.\n 71. Terrance V, Graham WT. Improved regularization of convolutional neural networks with cutout. arXiv preprint. \n2017.\n 72. Agnieszka M, Michal G. Data augmentation for improving deep learning in image classification problem. In: IEEE \n2018 international interdisciplinary Ph.D. Workshop, 2018.\n 73. Jonathan K, Michael S, Jia D, Li F-F. 3D object representations for fine-grained categorization. In: 4th IEEE Workshop on 3D Representation and Recognition, at ICCV 2013 (3dRR-13). Sydney, Australia. Dec. 8, 2013.\n 74. Tomohiko K, Michiaki I. Icing on the cake: an easy and quick post-learning method you can try after deep learning. \narXiv preprints. 2018.\n 75. Terrance V, Graham WT. Dataset augmentation in feature space. In: Proceedings of the international conference on \nmachine learning (ICML), workshop track, 2017.\n 76. Sebastien CW, Adam G, Victor S, Mark DM. Understanding data augmentation for classification: when to warp? \nCoRR, abs/1609.08764, 2016.\n 77. Seyed-Mohsen MD, Alhussein F, Pascal F. DeepFool: a simple and accurate method to fool deep neural networks. \narXiv preprint. 2016.\n 78. Jiawei S, Danilo VV, Sakurai K. One pixel attack for fooling deep neural networoks. arXiv preprints. 2018.\n 79. Michal Z, Konrad Z, Negar R, Pedro OP. Adversarial framing for image and video classification. arXiv preprints. 2018.\n 80. Logan E, Brandon T, Dimitris T, Ludwig S, Aleksander M. A rotation and a translation suffice: fooling CNNs with \nsimple transformations. ArXiv preprint. 2018.\n 81. Goodfellow I, Shlens J, Szegedy C. Explaining and Harnessing Adversarial Examples. International Conference on \nLearning Representations, 2015.\n 82. Ian JG, David W-F, Mehdi M, Aaron C, Yoshua B. Maxout networks. arXiv preprint. 2013.\n 83. Shuangtao L, Yuanke C, Yanlin P, Lin B. Learning more robust features with adversarial training. ArXiv preprints. \n2018.\n 84. Lingxi X, Jingdong W, Zhen W, Meng W, Qi T. DisturbLabel: regularizing CNN on the loss layer. arXiv preprint. 2016.\n 85. Christopher B, Liang C, Ricardo GPB, Roger G, Alexander H, David AD, Maria VH, Joanna W, Daniel R. GAN augmentation: augmenting training data using generative adversarial networks. arXiv preprint. 2018.\n 86. Doersch C. Tutorial on Variational Autoencoders. ArXiv e-prints. 2016.\n 87. Laurens M, Geoffrey H. Visualizing data using t-SNE. J Mach Learn Res. 2008;9:2431-56.\n 88. Jeff D, Philipp K, Trevor D. Adversarial feature learning. In: CVPR’16. 2016.\n 89. Lin Z, Shi Y, Xue Z. IDSGAN: Generative Adversarial Networks for Attack Generation against Intrusion Detection. \narXiv preprint; 2018.\n 90. William F, Mihaela R, Balaji L, Andrew MD, Shakir M, Ian G. Many paths to equilibrium: GANs do not need to \ndecrease a divergence at every step. In: International conference on learning representations (ICLR); 2017.\n 91. Alec R, Luke M, Soumith C. Unsupervised representation learning with deep convolutional generative adversarial \nnetworks. ICLR, 2016.\n 92. Jun-Yan Z, Taesung P, Phillip I, Alexei AE. Unpaired image-to-image translation using cycle-consistent adversarial \nnetworks. In: International conference on cmoputer vision (ICCV), 2017.\n 93. Xinyue Z, Yifan L, Zengchang Q, Jiahong L. Emotion classification with data augmentation using generative adversarial networks. CoRR, vol. abs/1711.00648. 2017.\n 94. Goodfellow IJ, Erhan D, Carrier PL, Courville A, Mirza M, Hamner B, Cukierski W, Tang Y, Thaler D, Lee DH, et al. Challenges in representation learning: A report on three machine learning contests. In: NIPS. Berlin: Springer; 2013. \np.117-24.\n 95. Mehdi M, Simon O. Conditional generative adversarial nets. arXiv preprint. 2014.\n 96. Mario L, Karol K, Marcin M, Olivier B, Sylvain G. Are GANs created equal? A large-scale study. arXiv preprint. 2018.\n\nPage 47 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \n 97. Swee KL, Yi L, Ngoc-Trung T, Ngai-Man C, Gemma R, Yuval E. DOPING: generative data augmentation for unsupervised anomaly detection with GAN. arXiv preprint. 2018.\n 98. Alireza M, Jonathon S, Navdeep J, Ian G, Brendan F. Adversarial autoencoders. arXiv preprint. 2015.\n 99. Tim S, Ian G, Wojciech Z, Vicki C, Alec R, Xi C. Improved techniques for training GANs. arXiv preprint. 2016.\n 100. Yanghao L, Naiyan W, Jiaying L, Xiaodi H. Demistifying neural style transfer. arXiv preprint. 2017.\n 101. Khizar H. Super-resolution via deep learning. arXiv preprint. 2017.\n 102. Dmitry U, Andrea V, Victor L. Instance normalization: the missing ingredient for fast stylization. arXiv preprint. 2016.\n 103. Philip TJ, Amir AA, Stephen B, Toby B, Boguslaw O. Style augmentation: data augmentation via style randomization. arXiv e-prints. 2018.\n 104. Josh T, Rachel F, Alex R, Jonas S, Wojciech Z, Pieter A. Domain randomization for transferring deep neural networks \nfrom simulation to the real world. arXiv preprint. 2017.\n 105. Ashish S, Tomas P, Oncel T, Josh S, Wenda W, Russ W. Learning from simulated and unsupervised images through \nadversarial training. In: Conference on computer vision and pattern recognition, 2017.\n 106. Stephan RR, Vibhav V, Stefan R, Vladlen K. Playing for data: ground truth from computer games. In: European \nconference on computer vision (ECCV); 2016.\n 107. Brostow Gabriel J, Fauqueur Julien, Cipolla Roberto. Semantic object classes in video: a high-definition ground \ntruth database. Pattern Recogn Lett. 2008;30(2):88-97.\n 108. Marius C, Mohamed O, Sebastian R, Timo R, Markus E, Rodrigo B, Uwe F, Stefan R, Bernt S. The cityscape dataset for \nsemantic urban scene understanding. In: CVPR; 2016.\n 109. Esteban R, Sherry M, Andrew S, Saurabh S, Yutaka LS, Jie T, Quoc VL, Alexey K. Large-scale evolution of image classifiers. In: Proceedings of the 34th international conference on machine learning (ICML ‘17). 2017.\n 110. Esteban R, Alok A, Yanping H, Quoc VL. Regularized evolution for image classifier architecture search. arXiv preprint. 2018.\n 111. Tim S, Jonathan H, Xi C, Szymon S, Ilya S. Evolution strategies as a scalable alternative to reinforcement learning. \narXiv e-prints. 2017.\n 112. Horia M, Aurelia G, Benjamin R. Simple random search provides a competitive approach to reinforcement learning. \nIn: Advances in neural information processing systems (NIPS); 2018.\n 113. David GL. Distinctive image features from scale-invariant keypoints. Int J Comput Vis. 2004;2004:91-110.\n 114. Navneet D, Bill T. Histograms of oriented gradients for human detection. In: CVPR, 2005.\n 115. Sutton RS, Reinforcement AG. Learning: an introduction. New York: MIT Press; 1998.\n 116. Mingyang G, Kele X, Bo D, Huaimin W, Lei Z. Learning data augmentation policies using augmented random \nsearch. arXiv preprint. 2018.\n 117. Tran NM, Mathieu S, Hoang TL, Martin W. Automated image data preprocessing with deep reinforcement learning. \narXiv preprints. 2018.\n 118. Hochreiter S. The vanishing gradient problem during learning recurrent neural nets and problem solutions. Int J \nUncertain Fuzzin Know Based Syst. 1998;6(02):107-16.\n 119. Jia S, Wang P, Jia P, Hu S. Research on data augmentation for image classification based on convolutional neural \nnetworks. In: 2017 Chinese automation congress (CAC), 2017. p. 4165-70.\n 120. Ilija R, Piotr D, Ross G, Georgia G, Kaiming H. Data distillation: towards omni-supervised learning. In: CVPR ’18; 2018.\n 121. Guotai W, Michael A, Sebastien O, Wenqi L, Jan D, Tom V. Test-time augmentation with uncertainty estimation for \ndeep learning-based medical image segmentation. OpenReview.net. 2018.\n 122. Fabio P, Christina V, Sandra A, Eduardo V. Data augmentation for skin lesion analysis. In: ISIC skin image analysis \nworkshop and challenge @ MICCAI 2018. 2018.\n 123. Karzuhisa M, Akira H, Akane M, Hiroshi K. Image classification of melanoma, nevus and seborrheic keratosis by \ndeep neural network ensemble. In: International skin imaging collaboration (ISIC) 2017 challenge at the international symposium on biomedical imaging (ISBI). 2017.\n 124. Max J, Karen S, Andrea V, Andrew Z. Synthetic data and artificial neural networks for natural scene text recognition. \narXiv preprint. 2014.\n 125. Florian S, Dmitry K, James P. FaceNet: a unified embedding for face recognition and clustering. In: CVPR ‘15. 2015.\n 126. Xudong M, Qing L, Haoran X, Raymond YKL, Zhen W, Stephen PS. Least squares generative adversarial networks. \nIn: International conference on computer vision (ICCV), 2017.\n 127. Ren W, Shengen Y, Yi S, Qingqing D, Gang S. Deep image: scaling up image recognition. CoRR, abs/1501.02876, \n2015.\n 128. Chao D, Chen CL, Kaiming H, Ziaoou T. Learning a deep convolutional network for image super-resolution. In: \nECCV. Berlin: Springer; 2014. , p. 184-99.\n 129. Christian L, Lucas T, Ferenc H, Jose C, Andrew C, Alejandro A, Andrew A, Alykhan T, Johannes T, Zehan W, Wenzhe \nS. Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint. 2016.\n 130. Han Z, Tao X, Hongsheng L, Shaoting Z, Xiaogang W, Xiaolei H, Dimitris M. StackGAN: text to photo-realistic image \nsynthesis with stacked generative adversarial networks. In: ICCV, 2017.\n 131. Trishul C, Yutaka S, Johnson A, Karthik K. Project adam: building an efficient and scalable deep learning training \nsystem. In: Proceedings of OSDI. 2014. P. 571-82.\n 132. Buda Mateusz, Maki Atsuto, Mazurowski Maciej A. A systematic study of the class imbalance problem in convolutional neural networks. Neural Networks. 2018;106:249-59.\n 133. Drown DJ, Khoshgoftaar TM, Seliya N. Evolutionary sampling and software quality modeling of high-assurance \nsystems. IEEE Trans Syst. 2009;39(5):1097-107.\n 134. Jason Y, Jeff C, Anh N, Thomas F, Hod L. Understanding neural networks through deep visualization. In: European \nconference on computer vision (ECCV). Berlin: Springer; 2015. p. 818-33.\n 135. Xiaofeng Z, Zhangyang W, Dong L, Qing L. DADA: deep adversarial data augmentation for extremely low data \nregime classification. arXiv preprint. 2018.\n 136. Martin A, Paul B, Jianmin C, Zhifeng C, Andy D, Jeffrey D, Matthieu D, Sanjay G, Geoffrey I, Michael I, Manjunath \nK, Josh L, Rajat M, Sherry M, Derek GM, Benoit S, Paul T, Vijay V, Pete W, Matrin W, Yuan Y, Xiaoqiang Z. TensorFlow:\n\nPage 48 of 48\nShorten and Khoshgoftaar J Big Data (2019) 6:60 \na system for large-scale machine learning. In: Proceedings of the 12th USENIX symposium on operating system \ndesign and implementation (OSDI ‘16), 2016.\n 137. Keras  ://keras .io/. 2015.\n 138. Alexander B, Alex P, Eugene K, Vladimir II, Alexandr AK. Albumentations: fast and flexible image augmentations. \nArXiv preprints. 2018.\n 139. Maayan F-A, Idit D, Eyal K, Michal A, Jacob G, Hayit G. GAN-based synthetic medical image augmentation for \nincreased CNN performance in liver lesion classification. arXiv preprint. 2018.\n 140. Changhee H, Hideaki H, Leonardo R, Ryosuke A, Wataru S, Shinichi M, Yujiro F, Giancarlo M, Hideki N. GAN-based \nsynthetic brain mr image generation. In: 2018 IEEE 15th International Symposium on biomedical imaging (ISBI \n2018). IEEE, 2011. P. 734-8.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
        "other": ""
      },
      "saved_path": "C:\\Users\\ASUS\\Documents\\GitHub\\AI-System-to-automatically-Review-and-Summarize-Research-Papers\\data\\extracted_text\\A_survey_on_Image_Data_Augmentation_for_Deep_Learn_extracted.json"
    },
    {
      "file_name": "PointNet_Deep_Learning_on_Point_Sets_for_3D_Classi_20260129.pdf",
      "file_path": "C:\\Users\\ASUS\\Documents\\GitHub\\AI-System-to-automatically-Review-and-Summarize-Research-Papers\\data\\papers\\PointNet_Deep_Learning_on_Point_Sets_for_3D_Classi_20260129.pdf",
      "file_size": 9083131,
      "extraction_date": "2026-01-29T22:01:09.545951",
      "pdf_type": "text_based",
      "total_pages": 19,
      "total_chars": 67708,
      "total_words": 11324,
      "extraction_method": "pymupdf",
      "pages": [
        {
          "page_number": 1,
          "text": "PointNet: Deep Learning on Point Sets for 3D Classiﬁcation and Segmentation\nCharles R. Qi*\nHao Su*\nKaichun Mo\nLeonidas J. Guibas\nStanford University\nAbstract\nPoint cloud is an important type of geometric data\nstructure.\nDue to its irregular format, most researchers\ntransform such data to regular 3D voxel grids or collections\nof images.\nThis, however, renders data unnecessarily\nvoluminous and causes issues. In this paper, we design a\nnovel type of neural network that directly consumes point\nclouds, which well respects the permutation invariance of\npoints in the input.\nOur network, named PointNet, provides a uniﬁed architecture for applications ranging from\nobject classiﬁcation, part segmentation, to scene semantic\nparsing. Though simple, PointNet is highly efﬁcient and\neffective.\nEmpirically, it shows strong performance on\npar or even better than state of the art.\nTheoretically,\nwe provide analysis towards understanding of what the\nnetwork has learnt and why the network is robust with\nrespect to input perturbation and corruption.\n1. Introduction\nIn this paper we explore deep learning architectures\ncapable of reasoning about 3D geometric data such as\npoint clouds or meshes. Typical convolutional architectures\nrequire highly regular input data formats, like those of\nimage grids or 3D voxels, in order to perform weight\nsharing and other kernel optimizations. Since point clouds\nor meshes are not in a regular format, most researchers\ntypically transform such data to regular 3D voxel grids or\ncollections of images (e.g, views) before feeding them to\na deep net architecture. This data representation transformation, however, renders the resulting data unnecessarily\nvoluminous - while also introducing quantization artifacts\nthat can obscure natural invariances of the data.\nFor this reason we focus on a different input representation for 3D geometry using simply point clouds\n- and name our resulting deep nets PointNets.\nPoint\nclouds are simple and uniﬁed structures that avoid the\ncombinatorial irregularities and complexities of meshes,\nand thus are easier to learn from. The PointNet, however,\n* indicates equal contributions.\nmug?\ntable?\ncar?\nClassification\nPart Segmentation\nPointNet\nSemantic Segmentation\nInput Point Cloud (point set representation)\nFigure 1. Applications of PointNet. We propose a novel deep net\narchitecture that consumes raw point cloud (set of points) without\nvoxelization or rendering. It is a uniﬁed architecture that learns\nboth global and local point features, providing a simple, efﬁcient\nand effective approach for a number of 3D recognition tasks.\nstill has to respect the fact that a point cloud is just a\nset of points and therefore invariant to permutations of its\nmembers, necessitating certain symmetrizations in the net\ncomputation. Further invariances to rigid motions also need\nto be considered.\nOur PointNet is a uniﬁed architecture that directly\ntakes point clouds as input and outputs either class labels\nfor the entire input or per point segment/part labels for\neach point of the input.\nThe basic architecture of our\nnetwork is surprisingly simple as in the initial stages each\npoint is processed identically and independently.\nIn the\nbasic setting each point is represented by just its three\ncoordinates (x, y, z). Additional dimensions may be added\nby computing normals and other local or global features.\nKey to our approach is the use of a single symmetric\nfunction, max pooling.\nEffectively the network learns a\nset of optimization functions/criteria that select interesting\nor informative points of the point cloud and encode the\nreason for their selection. The ﬁnal fully connected layers\nof the network aggregate these learnt optimal values into the\nglobal descriptor for the entire shape as mentioned above\n(shape classiﬁcation) or are used to predict per point labels\n(shape segmentation).\nOur input format is easy to apply rigid or afﬁne transformations to, as each point transforms independently. Thus\nwe can add a data-dependent spatial transformer network\nthat attempts to canonicalize the data before the PointNet\nprocesses them, so as to further improve the results.\narXiv:1612.00593v2 [cs.CV] 10 Apr 2017",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 7
        },
        {
          "page_number": 2,
          "text": "We provide both a theoretical analysis and an experimental evaluation of our approach.\nWe show that\nour network can approximate any set function that is\ncontinuous. More interestingly, it turns out that our network\nlearns to summarize an input point cloud by a sparse set of\nkey points, which roughly corresponds to the skeleton of\nobjects according to visualization. The theoretical analysis\nprovides an understanding why our PointNet is highly\nrobust to small perturbation of input points as well as\nto corruption through point insertion (outliers) or deletion\n(missing data).\nOn a number of benchmark datasets ranging from shape\nclassiﬁcation, part segmentation to scene segmentation,\nwe experimentally compare our PointNet with state-ofthe-art approaches based upon multi-view and volumetric\nrepresentations. Under a uniﬁed architecture, not only is\nour PointNet much faster in speed, but it also exhibits strong\nperformance on par or even better than state of the art.\nThe key contributions of our work are as follows:\n• We design a novel deep net architecture suitable for\nconsuming unordered point sets in 3D;\n• We show how such a net can be trained to perform\n3D shape classiﬁcation, shape part segmentation and\nscene semantic parsing tasks;\n• We provide thorough empirical and theoretical analysis on the stability and efﬁciency of our method;\n• We illustrate the 3D features computed by the selected\nneurons in the net and develop intuitive explanations\nfor its performance.\nThe problem of processing unordered sets by neural nets\nis a very general and fundamental problem - we expect that\nour ideas can be transferred to other domains as well.\n2. Related Work\nPoint Cloud Features\nMost existing features for point\ncloud are handcrafted towards speciﬁc tasks. Point features\noften encode certain statistical properties of points and are\ndesigned to be invariant to certain transformations, which\nare typically classiﬁed as intrinsic [2, 24, 3] or extrinsic\n[20, 19, 14, 10, 5]. They can also be categorized as local\nfeatures and global features. For a speciﬁc task, it is not\ntrivial to ﬁnd the optimal feature combination.\nDeep Learning on 3D Data\n3D data has multiple popular\nrepresentations, leading to various approaches for learning.\nVolumetric CNNs: [28, 17, 18] are the pioneers applying\n3D convolutional neural networks on voxelized shapes.\nHowever, volumetric representation is constrained by its\nresolution due to data sparsity and computation cost of\n3D convolution.\nFPNN [13] and Vote3D [26] proposed\nspecial methods to deal with the sparsity problem; however,\ntheir operations are still on sparse volumes, it’s challenging\nfor them to process very large point clouds.\nMultiview\nCNNs: [23, 18] have tried to render 3D point cloud or\nshapes into 2D images and then apply 2D conv nets to\nclassify them.\nWith well engineered image CNNs, this\nline of methods have achieved dominating performance on\nshape classiﬁcation and retrieval tasks [21]. However, it’s\nnontrivial to extend them to scene understanding or other\n3D tasks such as point classiﬁcation and shape completion.\nSpectral CNNs: Some latest works [4, 16] use spectral\nCNNs on meshes. However, these methods are currently\nconstrained on manifold meshes such as organic objects\nand it’s not obvious how to extend them to non-isometric\nshapes such as furniture.\nFeature-based DNNs: [6, 8]\nﬁrstly convert the 3D data into a vector, by extracting\ntraditional shape features and then use a fully connected net\nto classify the shape. We think they are constrained by the\nrepresentation power of the features extracted.\nDeep Learning on Unordered Sets\nFrom a data structure\npoint of view, a point cloud is an unordered set of vectors.\nWhile most works in deep learning focus on regular input\nrepresentations like sequences (in speech and language\nprocessing), images and volumes (video or 3D data), not\nmuch work has been done in deep learning on point sets.\nOne recent work from Oriol Vinyals et al [25] looks\ninto this problem. They use a read-process-write network\nwith attention mechanism to consume unordered input sets\nand show that their network has the ability to sort numbers.\nHowever, since their work focuses on generic sets and NLP\napplications, there lacks the role of geometry in the sets.\n3. Problem Statement\nWe design a deep learning framework that directly\nconsumes unordered point sets as inputs. A point cloud is\nrepresented as a set of 3D points {Pi| i = 1, ..., n}, where\neach point Pi is a vector of its (x, y, z) coordinate plus extra\nfeature channels such as color, normal etc. For simplicity\nand clarity, unless otherwise noted, we only use the (x, y, z)\ncoordinate as our point’s channels.\nFor the object classiﬁcation task, the input point cloud is\neither directly sampled from a shape or pre-segmented from\na scene point cloud. Our proposed deep network outputs\nk scores for all the k candidate classes.\nFor semantic\nsegmentation, the input can be a single object for part region\nsegmentation, or a sub-volume from a 3D scene for object\nregion segmentation. Our model will output n × m scores\nfor each of the n points and each of the m semantic subcategories.",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 3,
          "text": "input points\npoint features\noutput scores\nmax\npool\nshared \nshared \nshared \nnx3\nnx3\nnx64\nnx64\nnx1024\nn x 1088\nnx128\nmlp (64,64)\nmlp (64,128,1024)\ninput\ntransform\nfeature\ntransform\nmlp\n(512,256,k)\nglobal feature\nmlp (512,256,128)\nT-Net\nmatrix\nmultiply\n3x3\ntransform\nT-Net\nmatrix\nmultiply\n64x64\ntransform\nshared \nmlp (128,m)\noutput scores\nnxm\nk\nClassification Network\nSegmentation Network\nFigure 2. PointNet Architecture. The classiﬁcation network takes n points as input, applies input and feature transformations, and then\naggregates point features by max pooling. The output is classiﬁcation scores for k classes. The segmentation network is an extension to the\nclassiﬁcation net. It concatenates global and local features and outputs per point scores. “mlp” stands for multi-layer perceptron, numbers\nin bracket are layer sizes. Batchnorm is used for all layers with ReLU. Dropout layers are used for the last mlp in classiﬁcation net.\n4. Deep Learning on Point Sets\nThe architecture of our network (Sec 4.2) is inspired by\nthe properties of point sets in Rn (Sec 4.1).\n4.1. Properties of Point Sets in Rn\nOur input is a subset of points from an Euclidean space.\nIt has three main properties:\n• Unordered.\nUnlike pixel arrays in images or voxel\narrays in volumetric grids, point cloud is a set of points\nwithout speciﬁc order. In other words, a network that\nconsumes N 3D point sets needs to be invariant to N!\npermutations of the input set in data feeding order.\n• Interaction among points. The points are from a space\nwith a distance metric. It means that points are not\nisolated, and neighboring points form a meaningful\nsubset.\nTherefore, the model needs to be able to\ncapture local structures from nearby points, and the\ncombinatorial interactions among local structures.\n• Invariance under transformations.\nAs a geometric\nobject, the learned representation of the point set\nshould be invariant to certain transformations.\nFor\nexample, rotating and translating points all together\nshould not modify the global point cloud category nor\nthe segmentation of the points.\n4.2. PointNet Architecture\nOur full network architecture is visualized in Fig 2,\nwhere the classiﬁcation network and the segmentation\nnetwork share a great portion of structures. Please read the\ncaption of Fig 2 for the pipeline.\nOur network has three key modules: the max pooling\nlayer as a symmetric function to aggregate information from\nall the points, a local and global information combination\nstructure, and two joint alignment networks that align both\ninput points and point features.\nWe will discuss our reason behind these design choices\nin separate paragraphs below.\nSymmetry Function for Unordered Input\nIn order\nto make a model invariant to input permutation, three\nstrategies exist: 1) sort input into a canonical order; 2) treat\nthe input as a sequence to train an RNN, but augment the\ntraining data by all kinds of permutations; 3) use a simple\nsymmetric function to aggregate the information from each\npoint. Here, a symmetric function takes n vectors as input\nand outputs a new vector that is invariant to the input\norder. For example, + and ∗ operators are symmetric binary\nfunctions.\nWhile sorting sounds like a simple solution, in high\ndimensional space there in fact does not exist an ordering\nthat is stable w.r.t.\npoint perturbations in the general\nsense.\nThis can be easily shown by contradiction.\nIf\nsuch an ordering strategy exists, it deﬁnes a bijection map\nbetween a high-dimensional space and a 1d real line. It\nis not hard to see, to require an ordering to be stable w.r.t\npoint perturbations is equivalent to requiring that this map\npreserves spatial proximity as the dimension reduces, a task\nthat cannot be achieved in the general case.\nTherefore,\nsorting does not fully resolve the ordering issue, and it’s\nhard for a network to learn a consistent mapping from\ninput to output as the ordering issue persists. As shown in\nexperiments (Fig 5), we ﬁnd that applying a MLP directly\non the sorted point set performs poorly, though slightly\nbetter than directly processing an unsorted input.\nThe idea to use RNN considers the point set as a\nsequential signal and hopes that by training the RNN",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 4,
          "text": "with randomly permuted sequences, the RNN will become\ninvariant to input order. However in “OrderMatters” [25]\nthe authors have shown that order does matter and cannot be\ntotally omitted. While RNN has relatively good robustness\nto input ordering for sequences with small length (dozens),\nit’s hard to scale to thousands of input elements, which is\nthe common size for point sets. Empirically, we have also\nshown that model based on RNN does not perform as well\nas our proposed method (Fig 5).\nOur idea is to approximate a general function deﬁned on\na point set by applying a symmetric function on transformed\nelements in the set:\nf({x1, . . . , xn}) ≈ g(h(x1), . . . , h(xn)),\n(1)\nwhere f\n:\n2RN\n→\nR, h\n:\n→\nRK and g\n:\nRK × · · · × RK\n\n\n\nn\n→ R is a symmetric function.\nEmpirically, our basic module is very simple:\nwe\napproximate h by a multi-layer perceptron network and\ng by a composition of a single variable function and a\nmax pooling function.\nThis is found to work well by\nexperiments. Through a collection of h, we can learn a\nnumber of f’s to capture different properties of the set.\nWhile our key module seems simple, it has interesting\nproperties (see Sec 5.3) and can achieve strong performace\n(see Sec 5.1) in a few different applications. Due to the\nsimplicity of our module, we are also able to provide\ntheoretical analysis as in Sec 4.3.\nLocal and Global Information Aggregation\nThe output\nfrom the above section forms a vector [f1, . . . , fK], which\nis a global signature of the input set.\nWe can easily\ntrain a SVM or multi-layer perceptron classiﬁer on the\nshape global features for classiﬁcation.\nHowever, point\nsegmentation requires a combination of local and global\nknowledge. We can achieve this by a simple yet highly\neffective manner.\nOur solution can be seen in Fig 2 (Segmentation Network). After computing the global point cloud feature vector, we feed it back to per point features by concatenating\nthe global feature with each of the point features. Then we\nextract new per point features based on the combined point\nfeatures - this time the per point feature is aware of both the\nlocal and global information.\nWith this modiﬁcation our network is able to predict\nper point quantities that rely on both local geometry and\nglobal semantics. For example we can accurately predict\nper-point normals (ﬁg in supplementary), validating that the\nnetwork is able to summarize information from the point’s\nlocal neighborhood. In experiment session, we also show\nthat our model can achieve state-of-the-art performance on\nshape part segmentation and scene segmentation.\nJoint Alignment Network\nThe semantic labeling of a\npoint cloud has to be invariant if the point cloud undergoes\ncertain geometric transformations, such as rigid transformation. We therefore expect that the learnt representation by\nour point set is invariant to these transformations.\nA natural solution is to align all input set to a canonical\nspace before feature extraction.\nJaderberg et al. [9]\nintroduces the idea of spatial transformer to align 2D\nimages through sampling and interpolation, achieved by a\nspeciﬁcally tailored layer implemented on GPU.\nOur input form of point clouds allows us to achieve this\ngoal in a much simpler way compared with [9]. We do not\nneed to invent any new layers and no alias is introduced as in\nthe image case. We predict an afﬁne transformation matrix\nby a mini-network (T-net in Fig 2) and directly apply this\ntransformation to the coordinates of input points. The mininetwork itself resembles the big network and is composed\nby basic modules of point independent feature extraction,\nmax pooling and fully connected layers. More details about\nthe T-net are in the supplementary.\nThis idea can be further extended to the alignment of\nfeature space, as well. We can insert another alignment network on point features and predict a feature transformation\nmatrix to align features from different input point clouds.\nHowever, transformation matrix in the feature space has\nmuch higher dimension than the spatial transform matrix,\nwhich greatly increases the difﬁculty of optimization. We\ntherefore add a regularization term to our softmax training\nloss. We constrain the feature transformation matrix to be\nclose to orthogonal matrix:\nLreg = ∥I − AAT ∥2\nF ,\n(2)\nwhere A is the feature alignment matrix predicted by a\nmini-network. An orthogonal transformation will not lose\ninformation in the input, thus is desired. We ﬁnd that by\nadding the regularization term, the optimization becomes\nmore stable and our model achieves better performance.\n4.3. Theoretical Analysis\nUniversal approximation\nWe ﬁrst show the universal\napproximation ability of our neural network to continuous\nset functions. By the continuity of set functions, intuitively,\na small perturbation to the input point set should not\ngreatly change the function values, such as classiﬁcation or\nsegmentation scores.\nFormally, let X = {S : S ⊆ [0, 1]m and |S| = n}, f :\nX → R is a continuous set function on X w.r.t to Hausdorff\ndistance dH(·, ·), i.e., ∀ϵ > 0, ∃δ > 0, for any S, S′ ∈ X,\nif dH(S, S′) < δ, then |f(S) − f(S′)| < ϵ. Our theorem\nsays that f can be arbitrarily approximated by our network\ngiven enough neurons at the max pooling layer, i.e., K in\n(1) is sufﬁciently large.",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 5,
          "text": "Partial Inputs\nComplete Inputs\nairplane\ncar\nchair\nlamp\nguitar\nmotorbike\nmug\ntable\nbag\nrocket\nearphone\nlaptop\ncap\nknife\npistol\nskateboard\nFigure 3. Qualitative results for part segmentation.\nWe\nvisualize the CAD part segmentation results across all 16 object\ncategories. We show both results for partial simulated Kinect scans\n(left block) and complete ShapeNet CAD models (right block).\nTheorem 1. Suppose f\n:\nX\n→\nR is a continuous\nset function w.r.t Hausdorff distance dH(·, ·).\n∀ϵ\n>\n0, ∃ a continuous function h and a symmetric function\ng(x1, . . . , xn) = γ ◦ MAX, such that for any S ∈ X,\nf(S) − γ\n\nxi∈S {h(xi)}\n < ϵ\nwhere x1, . . . , xn is the full list of elements in S ordered\narbitrarily, γ is a continuous function, and MAX is a vector\nmax operator that takes n vectors as input and returns a\nnew vector of the element-wise maximum.\nThe proof to this theorem can be found in our supplementary material. The key idea is that in the worst case the\nnetwork can learn to convert a point cloud into a volumetric\nrepresentation, by partitioning the space into equal-sized\nvoxels. In practice, however, the network learns a much\nsmarter strategy to probe the space, as we shall see in point\nfunction visualizations.\nBottleneck dimension and stability\nTheoretically and\nexperimentally we ﬁnd that the expressiveness of our\nnetwork is strongly affected by the dimension of the max\npooling layer, i.e., K in (1). Here we provide an analysis,\nwhich also reveals properties related to the stability of our\nmodel.\nWe deﬁne u = MAX\nxi∈S {h(xi)} to be the sub-network of f\nwhich maps a point set in [0, 1]m to a K-dimensional vector.\nThe following theorem tells us that small corruptions or\nextra noise points in the input set are not likely to change\nthe output of our network:\nTheorem 2. Suppose u : X\n→ RK such that u =\nxi∈S{h(xi)} and f = γ ◦ u. Then,\n(a) ∀S, ∃ CS, NS ⊆ X, f(T) = f(S) if CS ⊆ T ⊆ NS;\n(b) |CS| ≤ K\ninput\n#views\naccuracy\naccuracy\navg. class\noverall\nSPH [11]\nmesh\n-\n68.2\n-\n3DShapeNets [28]\nvolume\n77.3\n84.7\nVoxNet [17]\nvolume\n83.0\n85.9\nSubvolume [18]\nvolume\n86.0\n89.2\nLFD [28]\nimage\n75.5\n-\nMVCNN [23]\nimage\n90.1\n-\nOurs baseline\npoint\n-\n72.6\n77.4\nOurs PointNet\npoint\n86.2\n89.2\nTable 1. Classiﬁcation results on ModelNet40. Our net achieves\nstate-of-the-art among deep nets on 3D input.\nWe explain the implications of the theorem. (a) says that\nf(S) is unchanged up to the input corruption if all points\nin CS are preserved; it is also unchanged with extra noise\npoints up to NS. (b) says that CS only contains a bounded\nnumber of points, determined by K in (1). In other words,\nf(S) is in fact totally determined by a ﬁnite subset CS ⊆ S\nof less or equal to K elements. We therefore call CS the\ncritical point set of S and K the bottleneck dimension of f.\nCombined with the continuity of h, this explains the\nrobustness of our model w.r.t point perturbation, corruption\nand extra noise points. The robustness is gained in analogy\nto the sparsity principle in machine learning models.\nIntuitively, our network learns to summarize a shape by\na sparse set of key points. In experiment section we see\nthat the key points form the skeleton of an object.\n5. Experiment\nExperiments are divided into four parts. First, we show\nPointNets can be applied to multiple 3D recognition tasks\n(Sec 5.1).\nSecond, we provide detailed experiments to\nvalidate our network design (Sec 5.2). At last we visualize\nwhat the network learns (Sec 5.3) and analyze time and\nspace complexity (Sec 5.4).\n5.1. Applications\nIn this section we show how our network can be\ntrained to perform 3D object classiﬁcation, object part\nsegmentation and semantic scene segmentation 1.\nEven\nthough we are working on a brand new data representation\n(point sets), we are able to achieve comparable or even\nbetter performance on benchmarks for several tasks.\n3D Object Classiﬁcation\nOur network learns global\npoint cloud feature that can be used for object classiﬁcation.\nWe evaluate our model on the ModelNet40 [28] shape\nclassiﬁcation benchmark. There are 12,311 CAD models\nfrom 40 man-made object categories, split into 9,843 for\n1More application examples such as correspondence and point cloud\nbased CAD model retrieval are included in supplementary material.",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 16
        },
        {
          "page_number": 6,
          "text": "mean\naero\nbag\ncap\ncar\nchair\near\nguitar knife\nlamp\nlaptop motor\nmug pistol rocket skate\ntable\nphone\nboard\n# shapes\n76\n898\n69\n392\n451\n184 283\n152\nWu [27]\n-\n63.2\n-\n-\n-\n73.5\n-\n-\n-\n74.4\n-\n-\n-\n-\n-\n-\n74.8\nYi [29]\n81.4\n81.0\n78.4\n77.7\n75.7\n87.6\n61.9\n92.0\n85.4\n82.5\n95.7\n70.6\n91.9 85.9\n53.1\n69.8\n75.3\n3DCNN\n79.4\n75.1\n72.8\n73.3\n70.0\n87.2\n63.5\n88.4\n79.6\n74.4\n93.9\n58.7\n91.8 76.4\n51.2\n65.3\n77.1\nOurs\n83.7\n83.4\n78.7\n82.5\n74.9\n89.6\n73.0\n91.5\n85.9\n80.8\n95.3\n65.2\n93.0 81.2\n57.9\n72.8\n80.6\nTable 2. Segmentation results on ShapeNet part dataset. Metric is mIoU(%) on points. We compare with two traditional methods [27]\nand [29] and a 3D fully convolutional network baseline proposed by us. Our PointNet method achieved the state-of-the-art in mIoU.\ntraining and 2,468 for testing.\nWhile previous methods\nfocus on volumetric and mult-view image representations,\nwe are the ﬁrst to directly work on raw point cloud.\nWe uniformly sample 1024 points on mesh faces according to face area and normalize them into a unit sphere.\nDuring training we augment the point cloud on-the-ﬂy by\nrandomly rotating the object along the up-axis and jitter the\nposition of each points by a Gaussian noise with zero mean\nand 0.02 standard deviation.\nIn Table 1, we compare our model with previous works\nas well as our baseline using MLP on traditional features\nextracted from point cloud (point density, D2, shape contour\netc.).\nOur model achieved state-of-the-art performance\namong methods based on 3D input (volumetric and point\ncloud). With only fully connected layers and max pooling,\nour net gains a strong lead in inference speed and can be\neasily parallelized in CPU as well. There is still a small\ngap between our method and multi-view based method\n(MVCNN [23]), which we think is due to the loss of ﬁne\ngeometry details that can be captured by rendered images.\n3D Object Part Segmentation\nPart segmentation is a\nchallenging ﬁne-grained 3D recognition task. Given a 3D\nscan or a mesh model, the task is to assign part category\nlabel (e.g. chair leg, cup handle) to each point or face.\nWe evaluate on ShapeNet part data set from [29], which\ncontains 16,881 shapes from 16 categories, annotated with\n50 parts in total. Most object categories are labeled with\ntwo to ﬁve parts. Ground truth annotations are labeled on\nsampled points on the shapes.\nWe formulate part segmentation as a per-point classiﬁcation problem. Evaluation metric is mIoU on points. For\neach shape S of category C, to calculate the shape’s mIoU:\nFor each part type in category C, compute IoU between\ngroundtruth and prediction. If the union of groundtruth and\nprediction points is empty, then count part IoU as 1. Then\nwe average IoUs for all part types in category C to get mIoU\nfor that shape. To calculate mIoU for the category, we take\naverage of mIoUs for all shapes in that category.\nIn this section, we compare our segmentation version\nPointNet (a modiﬁed version of Fig 2, Segmentation\nNetwork) with two traditional methods [27] and [29] that\nboth take advantage of point-wise geometry features and\ncorrespondences between shapes, as well as our own\n3D CNN baseline.\nSee supplementary for the detailed\nmodiﬁcations and network architecture for the 3D CNN.\nIn Table 2, we report per-category and mean IoU(%)\nscores. We observe a 2.3% mean IoU improvement and our\nnet beats the baseline methods in most categories.\nWe also perform experiments on simulated Kinect scans\nto test the robustness of these methods. For every CAD\nmodel in the ShapeNet part data set, we use Blensor Kinect\nSimulator [7] to generate incomplete point clouds from six\nrandom viewpoints. We train our PointNet on the complete\nshapes and partial scans with the same network architecture\nand training setting. Results show that we lose only 5.3%\nmean IoU. In Fig 3, we present qualitative results on both\ncomplete and partial data. One can see that though partial\ndata is fairly challenging, our predictions are reasonable.\nSemantic Segmentation in Scenes\nOur network on part\nsegmentation can be easily extended to semantic scene\nsegmentation, where point labels become semantic object\nclasses instead of object part labels.\nWe experiment on the Stanford 3D semantic parsing data\nset [1].\nThe dataset contains 3D scans from Matterport\nscanners in 6 areas including 271 rooms. Each point in the\nscan is annotated with one of the semantic labels from 13\ncategories (chair, table, ﬂoor, wall etc. plus clutter).\nTo prepare training data, we ﬁrstly split points by room,\nand then sample rooms into blocks with area 1m by 1m.\nWe train our segmentation version of PointNet to predict\nmean IoU\noverall accuracy\nOurs baseline\n20.12\n53.19\nOurs PointNet\n47.71\n78.62\nTable 3. Results on semantic segmentation in scenes. Metric is\naverage IoU over 13 classes (structural and furniture elements plus\nclutter) and classiﬁcation accuracy calculated on points.\ntable\nchair\nsofa\nboard\nmean\n# instance\n1363\n137\nArmeni et al. [1]\n46.02\n16.15\n6.78\n3.91\n18.22\nOurs\n46.67\n33.80\n4.76\n11.72\n24.24\nTable 4. Results on 3D object detection in scenes. Metric is\naverage precision with threshold IoU 0.5 computed in 3D volumes.",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 7,
          "text": "Input\nOutput\nFigure 4. Qualitative results for semantic segmentation. Top\nrow is input point cloud with color. Bottom row is output semantic\nsegmentation result (on points) displayed in the same camera\nviewpoint as input.\nper point class in each block. Each point is represented by\na 9-dim vector of XYZ, RGB and normalized location as\nto the room (from 0 to 1). At training time, we randomly\nsample 4096 points in each block on-the-ﬂy. At test time,\nwe test on all the points. We follow the same protocol as [1]\nto use k-fold strategy for train and test.\nWe compare our method with a baseline using handcrafted point features. The baseline extracts the same 9dim local features and three additional ones: local point\ndensity, local curvature and normal. We use standard MLP\nas the classiﬁer.\nResults are shown in Table 3, where\nour PointNet method signiﬁcantly outperforms the baseline\nmethod. In Fig 4, we show qualitative segmentation results.\nOur network is able to output smooth predictions and is\nrobust to missing points and occlusions.\nBased on the semantic segmentation output from our\nnetwork, we further build a 3D object detection system\nusing connected component for object proposal (see supplementary for details). We compare with previous stateof-the-art method in Table 4. The previous method is based\non a sliding shape method (with CRF post processing) with\nSVMs trained on local geometric features and global room\ncontext feature in voxel grids. Our method outperforms it\nby a large margin on the furniture categories reported.\n5.2. Architecture Design Analysis\nIn this section we validate our design choices by control\nexperiments. We also show the effects of our network’s\nhyperparameters.\nComparison with Alternative Order-invariant Methods\nAs mentioned in Sec 4.2, there are at least three options for\nconsuming unordered set inputs. We use the ModelNet40\nshape classiﬁcation problem as a test bed for comparisons\nof those options, the following two control experiment will\nalso use this task.\nThe baselines (illustrated in Fig 5) we compared with\ninclude multi-layer perceptron on unsorted and sorted\n(1,2,3)\n(2,3,4)\n(1,3,1)\nrnn \ncell\nrnn \ncell\nrnn \ncell\n...\n(1,2,3)\n(2,3,4)\n(1,3,1)\n...\n...\n(1,2,3)\n(1,3,1)\n(2,3,4)\n...\nsorting\nsequential model\nsymmetry function\nsorted\nFigure 5. Three approaches to achieve order invariance. Multilayer perceptron (MLP) applied on points consists of 5 hidden\nlayers with neuron sizes 64,64,64,128,1024, all points share a\nsingle copy of MLP. The MLP close to the output consists of two\nlayers with sizes 512,256.\npoints as n×3 arrays, RNN model that considers input point\nas a sequence, and a model based on symmetry functions.\nThe symmetry operation we experimented include max\npooling, average pooling and an attention based weighted\nsum. The attention method is similar to that in [25], where\na scalar score is predicted from each point feature, then the\nscore is normalized across points by computing a softmax.\nThe weighted sum is then computed on the normalized\nscores and the point features. As shown in Fig 5, maxpooling operation achieves the best performance by a large\nwinning margin, which validates our choice.\nEffectiveness of Input and Feature Transformations\nIn\nTable 5 we demonstrate the positive effects of our input\nand feature transformations (for alignment). It’s interesting\nto see that the most basic architecture already achieves\nquite reasonable results. Using input transformation gives\na 0.8% performance boost.\nThe regularization loss is\nnecessary for the higher dimension transform to work.\nBy combining both transformations and the regularization\nterm, we achieve the best performance.\nRobustness Test\nWe show our PointNet, while simple\nand effective, is robust to various kinds of input corruptions.\nWe use the same architecture as in Fig 5’s max pooling\nnetwork. Input points are normalized into a unit sphere.\nResults are in Fig 6.\nAs to missing points, when there are 50% points missing,\nthe accuracy only drops by 2.4% and 3.8% w.r.t. furthest\nand random input sampling. Our net is also robust to outlier\nTransform\naccuracy\nnone\n87.1\ninput (3x3)\n87.9\nfeature (64x64)\n86.9\nfeature (64x64) + reg.\n87.4\nboth\n89.2\nTable 5. Effects of input feature transforms. Metric is overall\nclassiﬁcation accuracy on ModelNet40 test set.",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 7
        },
        {
          "page_number": 8,
          "text": "30 \n50 \n70 \n90 \n0.05 \n0.1 \nAccuracy (%) \nPerturbation noise std \n40 \n60 \n80 \n100 \n0.2 \n0.4 \n0.6 \n0.8 \nAccuracy (%) \nMissing data ratio \nFurthest \nRandom \n30 \n50 \n70 \n90 \n0.1 \n0.2 \n0.3 \n0.4 \n0.5 \nAccuracy (%) \nOutlier ratio \nXYZ+density \nFigure 6. PointNet robustness test.\nThe metric is overall\nclassiﬁcation accuracy on ModelNet40 test set.\nLeft: Delete\npoints. Furthest means the original 1024 points are sampled with\nfurthest sampling. Middle: Insertion. Outliers uniformly scattered\nin the unit sphere. Right: Perturbation. Add Gaussian noise to\neach point independently.\npoints, if it has seen those during training. We evaluate two\nmodels: one trained on points with (x, y, z) coordinates; the\nother on (x, y, z) plus point density. The net has more than\n80% accuracy even when 20% of the points are outliers.\nFig 6 right shows the net is robust to point perturbations.\n5.3. Visualizing PointNet\nIn Fig 7, we visualize critical point sets CS and upperbound shapes NS (as discussed in Thm 2) for some sample\nshapes S. The point sets between the two shapes will give\nexactly the same global shape feature f(S).\nWe can see clearly from Fig 7 that the critical point\nsets CS, those contributed to the max pooled feature,\nsummarizes the skeleton of the shape. The upper-bound\nshapes NS illustrates the largest possible point cloud that\ngive the same global shape feature f(S) as the input point\ncloud S. CS and NS reﬂect the robustness of PointNet,\nmeaning that losing some non-critical points does not\nchange the global shape signature f(S) at all.\nThe NS is constructed by forwarding all the points in a\nedge-length-2 cube through the network and select points p\nwhose point function values (h1(p), h2(p), · · · , hK(p)) are\nno larger than the global shape descriptor.\nOriginal Shape\nCritical Point Sets\nUpper-bound Shapes\nFigure 7. Critical points and upper bound shape. While critical\npoints jointly determine the global shape feature for a given shape,\nany point cloud that falls between the critical points set and the\nupper bound shape gives exactly the same feature. We color-code\nall ﬁgures to show the depth information.\n5.4. Time and Space Complexity Analysis\nTable 6 summarizes space (number of parameters in\nthe network) and time (ﬂoating-point operations/sample)\ncomplexity of our classiﬁcation PointNet. We also compare\nPointNet to a representative set of volumetric and multiview based architectures in previous works.\nWhile MVCNN [23] and Subvolume (3D CNN) [18]\nachieve high performance, PointNet is orders more efﬁcient\nin computational cost (measured in FLOPs/sample: 141x\nand 8x more efﬁcient, respectively).\nBesides, PointNet\nis much more space efﬁcient than MVCNN in terms of\n#param in the network (17x less parameters). Moreover,\nPointNet is much more scalable - it’s space and time\ncomplexity is O(N) - linear in the number of input points.\nHowever, since convolution dominates computing time,\nmulti-view method’s time complexity grows squarely on\nimage resolution and volumetric convolution based method\ngrows cubically with the volume size.\nEmpirically, PointNet is able to process more than\none million points per second for point cloud classiﬁcation (around 1K objects/second) or semantic segmentation\n(around 2 rooms/second) with a 1080X GPU on TensorFlow, showing great potential for real-time applications.\n#params\nFLOPs/sample\nPointNet (vanilla)\n0.8M\n148M\nPointNet\n3.5M\n440M\nSubvolume [18]\n16.6M\n3633M\nMVCNN [23]\n60.0M\n62057M\nTable 6. Time and space complexity of deep architectures for\n3D data classiﬁcation.\nPointNet (vanilla) is the classiﬁcation\nPointNet without input and feature transformations.\nstands for ﬂoating-point operation. The “M” stands for million.\nSubvolume and MVCNN used pooling on input data from multiple\nrotations or views, without which they have much inferior\nperformance.\n6. Conclusion\nIn this work, we propose a novel deep neural network\nPointNet that directly consumes point cloud. Our network\nprovides a uniﬁed approach to a number of 3D recognition\ntasks including object classiﬁcation, part segmentation and\nsemantic segmentation, while obtaining on par or better\nresults than state of the arts on standard benchmarks. We\nalso provide theoretical analysis and visualizations towards\nunderstanding of our network.\nAcknowledgement.\nThe authors gratefully acknowledge\nthe support of a Samsung GRO grant, ONR MURI N0001413-1-0341 grant, NSF grant IIS-1528025, a Google Focused Research Award, a gift from the Adobe corporation\nand hardware donations by NVIDIA.",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 17
        },
        {
          "page_number": 9,
          "text": "References\n[1] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis,\nM. Fischer, and S. Savarese.\n3d semantic parsing of\nlarge-scale indoor spaces.\nIn Proceedings of the IEEE\nInternational Conference on Computer Vision and Pattern\nRecognition, 2016. 6, 7\n[2] M. Aubry, U. Schlickewei, and D. Cremers.\nThe wave\nkernel signature: A quantum mechanical approach to shape\nanalysis. In Computer Vision Workshops (ICCV Workshops),\n2011 IEEE International Conference on, pages 1626-1633.\nIEEE, 2011. 2\n[3] M. M. Bronstein and I. Kokkinos.\nScale-invariant heat\nkernel signatures for non-rigid shape recognition.\nIn\nComputer Vision and Pattern Recognition (CVPR), 2010\nIEEE Conference on, pages 1704-1711. IEEE, 2010. 2\n[4] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral\nnetworks and locally connected networks on graphs. arXiv\npreprint arXiv:1312.6203, 2013. 2\n[5] D.-Y. Chen, X.-P. Tian, Y.-T. Shen, and M. Ouhyoung. On\nvisual similarity based 3d model retrieval.\nIn Computer\ngraphics forum, volume 22, pages 223-232. Wiley Online\nLibrary, 2003. 2\n[6] Y. Fang, J. Xie, G. Dai, M. Wang, F. Zhu, T. Xu, and\nE. Wong.\n3d deep shape descriptor.\nIn Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 2319-2328, 2015. 2\n[7] M. Gschwandtner, R. Kwitt, A. Uhl, and W. Pree. BlenSor:\nBlender Sensor Simulation Toolbox Advances in Visual\nComputing.\nvolume 6939 of Lecture Notes in Computer\nScience, chapter 20, pages 199-208. Springer Berlin /\nHeidelberg, Berlin, Heidelberg, 2011. 6\n[8] K. Guo, D. Zou, and X. Chen.\n3d mesh labeling via\ndeep convolutional neural networks. ACM Transactions on\nGraphics (TOG), 35(1):3, 2015. 2\n[9] M. Jaderberg, K. Simonyan, A. Zisserman, et al.\nSpatial\ntransformer networks. In NIPS 2015. 4\n[10] A. E. Johnson and M. Hebert. Using spin images for efﬁcient\nobject recognition in cluttered 3d scenes. IEEE Transactions\non pattern analysis and machine intelligence, 21(5):433-\n449, 1999. 2\n[11] M. Kazhdan, T. Funkhouser, and S. Rusinkiewicz. Rotation\ninvariant spherical harmonic representation of 3 d shape descriptors. In Symposium on geometry processing, volume 6,\npages 156-164, 2003. 5\n[12] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. 13\n[13] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas.\nFpnn:\nField probing neural networks for 3d data. arXiv preprint\narXiv:1605.06240, 2016. 2\n[14] H. Ling and D. W. Jacobs. Shape classiﬁcation using the\ninner-distance. IEEE transactions on pattern analysis and\nmachine intelligence, 29(2):286-299, 2007. 2\n[15] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne.\nJournal of Machine Learning Research, 9(Nov):2579-2605,\n2008. 15\n[16] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst.\nGeodesic convolutional neural networks on riemannian manifolds. In Proceedings of the IEEE International Conference\non Computer Vision Workshops, pages 37-45, 2015. 2\n[17] D. Maturana and S. Scherer. Voxnet: A 3d convolutional\nneural network for real-time object recognition. In IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems,\nSeptember 2015. 2, 5, 10, 11\n[18] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. Guibas.\nVolumetric and multi-view cnns for object classiﬁcation on\n3d data. In Proc. Computer Vision and Pattern Recognition\n(CVPR), IEEE, 2016. 2, 5, 8\n[19] R. B. Rusu, N. Blodow, and M. Beetz. Fast point feature\nhistograms (fpfh) for 3d registration.\nIn Robotics and\nAutomation, 2009. ICRA’09. IEEE International Conference\non, pages 3212-3217. IEEE, 2009. 2\n[20] R. B. Rusu, N. Blodow, Z. C. Marton, and M. Beetz. Aligning point cloud views using persistent feature histograms.\nIn 2008 IEEE/RSJ International Conference on Intelligent\nRobots and Systems, pages 3384-3391. IEEE, 2008. 2\n[21] M. Savva, F. Yu, H. Su, M. Aono, B. Chen, D. Cohen-Or,\nW. Deng, H. Su, S. Bai, X. Bai, et al. Shrec16 track largescale 3d shape retrieval from shapenet core55. 2\n[22] P. Y. Simard, D. Steinkraus, and J. C. Platt. Best practices for\nconvolutional neural networks applied to visual document\nanalysis. In ICDAR, volume 3, pages 958-962, 2003. 13\n[23] H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller.\nMulti-view convolutional neural networks for 3d shape\nrecognition. In Proc. ICCV, to appear, 2015. 2, 5, 6, 8\n[24] J. Sun, M. Ovsjanikov, and L. Guibas.\nA concise and\nprovably informative multi-scale signature based on heat\ndiffusion. In Computer graphics forum, volume 28, pages\n1383-1392. Wiley Online Library, 2009. 2\n[25] O. Vinyals, S. Bengio, and M. Kudlur.\nOrder matters:\nSequence to sequence for sets.\narXiv preprint\narXiv:1511.06391, 2015. 2, 4, 7\n[26] D. Z. Wang and I. Posner. Voting for voting in online point\ncloud object detection. Proceedings of the Robotics: Science\nand Systems, Rome, Italy, 1317, 2015. 2\n[27] Z. Wu, R. Shou, Y. Wang, and X. Liu. Interactive shape cosegmentation via label propagation. Computers & Graphics,\n38:248-254, 2014. 6, 10\n[28] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and\nJ. Xiao. 3d shapenets: A deep representation for volumetric\nshapes. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 1912-1920, 2015. 2,\n5, 11\n[29] L. Yi, V. G. Kim, D. Ceylan, I.-C. Shen, M. Yan, H. Su,\nC. Lu, Q. Huang, A. Sheffer, and L. Guibas. A scalable active\nframework for region annotation in 3d shape collections.\nSIGGRAPH Asia, 2016. 6, 10, 18",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 10,
          "text": "Supplementary\nA. Overview\nThis document provides additional quantitative results,\ntechnical details and more qualitative test examples to the\nmain paper.\nIn Sec B we extend the robustness test to compare\nPointNet with VoxNet on incomplete input.\nIn Sec C\nwe provide more details on neural network architectures,\ntraining parameters and in Sec D we describe our detection\npipeline in scenes. Then Sec E illustrates more applications\nof PointNet, while Sec F shows more analysis experiments.\nSec G provides a proof for our theory on PointNet. At last,\nwe show more visualization results in Sec H.\nB. Comparison between PointNet and VoxNet\n(Sec 5.2)\nWe extend the experiments in Sec 5.2 Robustness Test\nto compare PointNet and VoxNet [17] (a representative\narchitecture for volumetric representation) on robustness to\nmissing data in the input point cloud. Both networks are\ntrained on the same train test split with 1024 number of\npoints as input. For VoxNet we voxelize the point cloud\nto 32 × 32 × 32 occupancy grids and augment the training\ndata by random rotation around up-axis and jittering.\nAt test time, input points are randomly dropped out\nby a certain ratio.\nAs VoxNet is sensitive to rotations,\nits prediction uses average scores from 12 viewpoints of\na point cloud.\nAs shown in Fig 8, we see that our\nPointNet is much more robust to missing points. VoxNet’s\naccuracy dramatically drops when half of the input points\nare missing, from 86.3% to 46.0% with a 40.3% difference,\nwhile our PointNet only has a 3.7% performance drop. This\ncan be explained by the theoretical analysis and explanation\nof our PointNet - it is learning to use a collection of critical\npoints to summarize the shape, thus it is very robust to\nmissing data.\nC. Network Architecture and Training Details\n(Sec 5.1)\nPointNet Classiﬁcation Network\nAs the basic architecture is already illustrated in the main paper, here we\nprovides more details on the joint alignment/transformation\nnetwork and training parameters.\nThe ﬁrst transformation network is a mini-PointNet that\ntakes raw point cloud as input and regresses to a 3 × 3\nmatrix.\nIt’s composed of a shared MLP(64, 128, 1024)\nnetwork (with layer output sizes 64, 128, 1024) on each\npoint, a max pooling across points and two fully connected\nlayers with output sizes 512, 256.\nThe output matrix is\ninitialized as an identity matrix. All layers, except the last\none, include ReLU and batch normalization. The second\nPointNet\nVoxNet\n87.1\n86.3\n0.5\n83.3\n0.75\n18.5\n0.875\n59.2\n13.3\n0.9375\n33.2\n10.2\n20 \n60 \n100 \n0.2 \n0.4 \n0.6 \n0.8 \nAccuracy (%) \nMissing Data Ratio \nPointNet \nVoxNet \nFigure 8. PointNet v.s. VoxNet [17] on incomplete input data.\nMetric is overall classiﬁcation accurcacy on ModelNet40 test set.\nNote that VoxNet is using 12 viewpoints averaging while PointNet\nis using only one view of the point cloud. Evidently PointNet\npresents much stronger robustness to missing points.\ntransformation network has the same architecture as the ﬁrst\none except that the output is a 64 × 64 matrix. The matrix\nis also initialized as an identity. A regularization loss (with\nweight 0.001) is added to the softmax classiﬁcation loss to\nmake the matrix close to orthogonal.\nWe use dropout with keep ratio 0.7 on the last fully\nconnected layer, whose output dimension 256, before class\nscore prediction. The decay rate for batch normalization\nstarts with 0.5 and is gradually increased to 0.99. We use\nadam optimizer with initial learning rate 0.001, momentum\n0.9 and batch size 32. The learning rate is divided by 2\nevery 20 epochs. Training on ModelNet takes 3-6 hours to\nconverge with TensorFlow and a GTX1080 GPU.\nPointNet Segmentation Network\nThe segmentation network is an extension to the classiﬁcation PointNet. Local\npoint features (the output after the second transformation\nnetwork) and global feature (output of the max pooling)\nare concatenated for each point. No dropout is used for\nsegmentation network. Training parameters are the same\nas the classiﬁcation network.\nAs to the task of shape part segmentation, we made\na few modiﬁcations to the basic segmentation network\narchitecture (Fig 2 in main paper) in order to achieve best\nperformance, as illustrated in Fig 9.\nWe add a one-hot\nvector indicating the class of the input and concatenate it\nwith the max pooling layer’s output.\nWe also increase\nneurons in some layers and add skip links to collect local\npoint features in different layers and concatenate them to\nform point feature input to the segmentation network.\nWhile [27] and [29] deal with each object category\nindependently, due to the lack of training data for some\ncategories (the total number of shapes for all the categories\nin the data set are shown in the ﬁrst line), we train our\nPointNet across categories (but with one-hot vector input to\nindicate category). To allow fair comparison, when testing",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 5
        },
        {
          "page_number": 11,
          "text": "input points\npoint features\noutput scores\nmax\npool\nshared \nshared \nshared \nnx3\nnx3\nnx64\nnx64\nnx1024\nn x 1088\nnx128\nmlp (64,128,128)\nmlp (64,128,1024)\ninput\ntransform\nfeature\ntransform\nmlp\n(512,256,k)\nglobal feature\nmlp (512,256,128)\nT-Net\nmatrix\nmultiply\n3x3\ntransform\nT-Net\nmatrix\nmultiply\n64x64\ntransform\nshared \nmlp (128,m)\noutput scores\nnxm\nk\nClassification Network\nSegmentation Network\nnx3\nnx3\nT1\nnx64\nnx128\nnx128\nnx128\nT2\nnx512\nnx2048\nnx64\nnx128\nnx128\nnx128\nnx512\nn x 3024\n(256,256,128)\nnx50\none-hot\ninput points\npart scores\nFigure 9. Network architecture for part segmentation. T1 and\nT2 are alignment/transformation networks for input points and\nfeatures. FC is fully connected layer operating on each point. MLP\nis multi-layer perceptron on each point. One-hot is a vector of size\n16 indicating category of the input shape.\n32 filters \nof stride 1\n32\n32\n32\n5\n32 filters \nof stride 1\n32 filters \nof stride 1\n32 filters \nof stride 1\n32 filters \nof stride 1\n1\n64 filters \nof stride 1\n1\n64 filters \nof stride 1\n32\n1\n50 filters \nof stride 1\nin-category\nprediction\nFigure 10. Baseline 3D CNN segmentation network.\nThe\nnetwork is fully convolutional and predicts part scores for each\nvoxel.\nthese two models, we only predict part labels for the given\nspeciﬁc object category.\nAs to semantic segmentation task, we used the architecture as in Fig 2 in the main paper.\nIt takes around six to twelve hours to train the model on\nShapeNet part dataset and around half a day to train on the\nStanford semantic parsing dataset.\nBaseline 3D CNN Segmentation Network\nIn ShapeNet\npart segmentation experiment, we compare our proposed\nsegmentation version PointNet to two traditional methods\nas well as a 3D volumetric CNN network baseline.\nIn\nFig 10, we show the baseline 3D volumetric CNN network\nwe use. We generalize the well-known 3D CNN architectures, such as VoxNet [17] and 3DShapeNets [28] to a fully\nconvolutional 3D CNN segmentation network.\nFor a given point cloud, we ﬁrst convert it to the volumetric representation as a occupancy grid with resolution\n32 × 32 × 32. Then, ﬁve 3D convolution operations each\nwith 32 output channels and stride of 1 are sequentially\napplied to extract features. The receptive ﬁeld is 19 for each\nvoxel. Finally, a sequence of 3D convolutional layers with\nkernel size 1 × 1 × 1 is appended to the computed feature\nmap to predict segmentation label for each voxel. ReLU and\nbatch normalization are used for all layers except the last\none. The network is trained across categories, however, in\norder to compare with other baseline methods where object\ncategory is given, we only consider output scores in the\ngiven object category.\nD. Details on Detection Pipeline (Sec 5.1)\nWe build a simple 3D object detection system based on\nthe semantic segmentation results and our object classiﬁcation PointNet.\nWe use connected component with segmentation scores\nto get object proposals in scenes. Starting from a random\npoint in the scene, we ﬁnd its predicted label and use\nBFS to search nearby points with the same label, with\na search radius of 0.2 meter.\nIf the resulted cluster has\nmore than 200 points (assuming a 4096 point sample in\na 1m by 1m area), the cluster’s bounding box is marked\nas one object proposal.\nFor each proposed object, it’s\ndetection score is computed as the average point score for\nthat category. Before evaluation, proposals with extremely\nsmall areas/volumes are pruned.\nFor tables, chairs and\nsofas, the bounding boxes are extended to the ﬂoor in case\nthe legs are separated with the seat/surface.\nWe observe that in some rooms such as auditoriums\nlots of objects (e.g. chairs) are close to each other, where\nconnected component would fail to correctly segment out\nindividual ones. Therefore we leverage our classiﬁcation\nnetwork and uses sliding shape method to alleviate the\nproblem for the chair class. We train a binary classiﬁcation\nnetwork for each category and use the classiﬁer for sliding\nwindow detection.\nThe resulted boxes are pruned by\nnon-maximum suppression.\nThe proposed boxes from\nconnected component and sliding shapes are combined for\nﬁnal evaluation.\nIn Fig 11, we show the precision-recall curves for object\ndetection. We trained six models, where each one of them\nis trained on ﬁve areas and tested on the left area. At test\nphase, each model is tested on the area it has never seen.\nThe test results for all six areas are aggregated for the PR\ncurve generation.\nE. More Applications (Sec 5.1)\nModel Retrieval from Point Cloud\nOur PointNet learns\na global shape signature for every given input point cloud.\nWe expect geometrically similar shapes have similar global\nsignature.\nIn this section, we test our conjecture on the\nshape retrieval application. To be more speciﬁc, for every\ngiven query shape from ModelNet test split, we compute\nits global signature (output of the layer before the score\nprediction layer) given by our classiﬁcation PointNet and\nretrieve similar shapes in the train split by nearest neighbor\nsearch. Results are shown in Fig 12.",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 2
        },
        {
          "page_number": 12,
          "text": "Figure 11. Precision-recall curves for object detection in 3D\npoint cloud. We evaluated on all six areas for four categories:\ntable, chair, sofa and board. IoU threshold is 0.5 in volume.\nQuery\nPoint Cloud\nTop-5 Retrieval CAD Models\nFigure 12. Model retrieval from point cloud.\nFor every\ngiven point cloud, we retrieve the top-5 similar shapes from the\nModelNet test split. From top to bottom rows, we show examples\nof chair, plant, nightstand and bathtub queries. Retrieved results\nthat are in wrong category are marked by red boxes.\nShape Correspondence\nIn this section, we show that\npoint features learnt by PointNet can be potentially used\nto compute shape correspondences. Given two shapes, we\ncompute the correspondence between their critical point\nsets CS’s by matching the pairs of points that activate\nthe same dimensions in the global features.\nFig 13 and\nFig 14 show the detected shape correspondence between\ntwo similar chairs and tables.\nF. More Architecture Analysis (Sec 5.2)\nEffects of Bottleneck Dimension and Number of Input\nPoints\nHere we show our model’s performance change\nwith regard to the size of the ﬁrst max layer output as\nwell as the number of input points. In Fig 15 we see that\nperformance grows as we increase the number of points\nhowever it saturates at around 1K points. The max layer\nsize plays an important role, increasing the layer size from\nFigure 13. Shape correspondence between two chairs. For the\nclarity of the visualization, we only show 20 randomly picked\ncorrespondence pairs.\nFigure 14. Shape correspondence between two tables. For the\nclarity of the visualization, we only show 20 randomly picked\ncorrespondence pairs.\n64 to 1024 results in a 2−4% performance gain. It indicates\nthat we need enough point feature functions to cover the 3D\nspace in order to discriminate different shapes.\nIt’s worth notice that even with 64 points as input\n(obtained from furthest point sampling on meshes), our\nnetwork can achieve decent performance.\n82 \n84 \n86 \n88 \n200 \n600 \n1000 \nAccuracy (%) \nBottleneck size \n128 \n1024 \n#points\nFigure 15. Effects of bottleneck size and number of input\npoints. The metric is overall classiﬁcation accuracy on ModelNet40 test set.\nMNIST Digit Classiﬁcation\nWhile we focus on 3D point\ncloud learning, a sanity check experiment is to apply our\nnetwork on a 2D point clouds - pixel sets.\nTo convert an MNIST image into a 2D point set we\nthreshold pixel values and add the pixel (represented as a",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 32
        },
        {
          "page_number": 13,
          "text": "point with (x, y) coordinate in the image) with values larger\nthan 128 to the set. We use a set size of 256. If there are\nmore than 256 pixels int he set, we randomly sub-sample it;\nif there are less, we pad the set with the one of the pixels in\nthe set (due to our max operation, which point to use for the\npadding will not affect outcome).\nAs seen in Table 7, we compare with a few baselines\nincluding multi-layer perceptron that considers input image\nas an ordered vector, a RNN that consider input as sequence\nfrom pixel (0,0) to pixel (27,27), and a vanilla version CNN.\nWhile the best performing model on MNIST is still well\nengineered CNNs (achieving less than 0.3% error rate),\nit’s interesting to see that our PointNet model can achieve\nreasonable performance by considering image as a 2D point\nset.\ninput\nerror (%)\nMulti-layer perceptron [22]\nvector\n1.60\nLeNet5 [12]\nimage\n0.80\nOurs PointNet\npoint set\n0.78\nTable 7. MNIST classiﬁcation results. We compare with vanilla\nversions of other deep architectures to show that our network based\non point sets input is achieving reasonable performance on this\ntraditional task.\nNormal Estimation\nIn segmentation version of PointNet,\nlocal point features and global feature are concatenated\nin order to provide context to local points.\nHowever,\nit’s unclear whether the context is learnt through this\nconcatenation. In this experiment, we validate our design\nby showing that our segmentation network can be trained\nto predict point normals, a local geometric property that is\ndetermined by a point’s neighborhood.\nWe train a modiﬁed version of our segmentation PointNet in a supervised manner to regress to the groundtruth point normals. We just change the last layer of our\nsegmentation PointNet to predict normal vector for each\npoint. We use absolute value of cosine distance as loss.\nFig. 16 compares our PointNet normal prediction results\n(the left columns) to the ground-truth normals computed\nfrom the mesh (the right columns).\nWe observe a\nreasonable normal reconstruction.\nOur predictions are\nmore smooth and continuous than the ground-truth which\nincludes ﬂipped normal directions in some region.\nSegmentation Robustness\nAs discussed in Sec 5.2 and\nSec B, our PointNet is less sensitive to data corruption and\nmissing points for classiﬁcation tasks since the global shape\nfeature is extracted from a collection of critical points from\nthe given input point cloud. In this section, we show that the\nrobustness holds for segmentation tasks too. The per-point\npart labels are predicted based on the combination of perpoint features and the learnt global shape feature. In Fig 17,\nGround-truth\nPrediction\nFigure 16. PointNet normal reconstrution results. In this ﬁgure,\nwe show the reconstructed normals for all the points in some\nsample point clouds and the ground-truth normals computed on\nthe mesh.\nwe illustrate the segmentation results for the given input\npoint clouds S (the left-most column), the critical point sets\nCS (the middle column) and the upper-bound shapes NS.\nNetwork Generalizability to Unseen Shape Categories\nIn Fig 18, we visualize the critical point sets and the upperbound shapes for new shapes from unseen categories (face,\nhouse, rabbit, teapot) that are not present in ModelNet or\nShapeNet. It shows that the learnt per-point functions are\ngeneralizable.\nHowever, since we train mostly on manmade objects with lots of planar structures, the reconstructed upper-bound shape in novel categories also contain\nmore planar surfaces.\nG. Proof of Theorem (Sec 4.3)\nLet X = {S : S ⊆ [0, 1] and |S| = n}.\nf : X → R is a continuous function on X w.r.t to\nHausdorff distance dH(·, ·) if the following condition is\nsatisﬁed:\n∀ϵ > 0, ∃δ > 0, for any S, S′ ∈ X, if dH(S, S′) < δ,\nthen |f(S) − f(S′)| < ϵ.\nWe show that f can be approximated arbitrarily by\ncomposing a symmetric function and a continuous function.",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 6
        },
        {
          "page_number": 14,
          "text": "Input Point Cloud\nCritical Point Sets\nUpper-bound Shapes\nFigure 17. The consistency of segmentation results.\nWe\nillustrate the segmentation results for some sample given point\nclouds S, their critical point sets CS and upper-bound shapes NS.\nWe observe that the shape family between the CS and NS share a\nconsistent segmentation results.\nOriginal Shape\nCritical Point Sets\nUpper-bound Shapes\nFigure 18. The critical point sets and the upper-bound shapes\nfor unseen objects. We visualize the critical point sets and the\nupper-bound shapes for teapot, bunny, hand and human body,\nwhich are not in the ModelNet or ShapeNet shape repository to\ntest the generalizability of the learnt per-point functions of our\nPointNet on other unseen objects. The images are color-coded\nto reﬂect the depth information.\nTheorem 1. Suppose f\n:\nX\n→\nR is a continuous\nset function w.r.t Hausdorff distance dH(·, ·).\n∀ϵ\n>\n0, ∃ a continuous function h and a symmetric function\ng(x1, . . . , xn) = γ◦MAX, where γ is a continuous function,\nMAX is a vector max operator that takes n vectors as input\nand returns a new vector of the element-wise maximum,\nsuch that for any S ∈ X,\n|f(S) − γ(MAX(h(x1), . . . , h(xn)))| < ϵ\nwhere x1, . . . , xn are the elements of S extracted in certain\norder,\nProof. By the continuity of f, we take δϵ so that |f(S) −\nf(S′)| < ϵ for any S, S′ ∈ X if dH(S, S′) < δϵ.\nDeﬁne K = ⌈1/δϵ⌉, which split [0, 1] into K intervals\nevenly and deﬁne an auxiliary function that maps a point to\nthe left end of the interval it lies in:\nσ(x) = ⌊Kx⌋\nK\nLet ˜S = {σ(x) : x ∈ S}, then\n|f(S) − f( ˜S)| < ϵ\nbecause dH(S, ˜S) < 1/K ≤ δϵ.\nLet hk(x) = e−d(x,[ k−1\nK , k\nK ]) be a soft indicator function\nwhere d(x, I) is the point to set (interval) distance. Let\nh(x) = [h1(x); . . . ; hK(x)], then h : R → RK.\nLet vj(x1, . . . , xn) = max{˜hj(x1), . . . , ˜hj(xn)}, indicating the occupancy of the j-th interval by points in S.\nLet v = [v1; . . . ; vK], then v : R × . . . × R\n\n\n\nn\n→ {0, 1}K\nis a symmetric function, indicating the occupancy of each\ninterval by points in S.\nDeﬁne τ : {0, 1}K → X as τ(v) = { k−1\nK\n: vk ≥ 1},\nwhich maps the occupancy vector to a set which contains\nthe left end of each occupied interval. It is easy to show:\nτ(v(x1, . . . , xn)) ≡ ˜S\nwhere x1, . . . , xn are the elements of S extracted in certain\norder.\nLet γ : RK → R be a continuous function such that\nγ(v) = f(τ(v)) for v ∈ {0, 1}K. Then,\n|γ(v(x1, . . . , xn)) − f(S)|\n=|f(τ(v(x1, . . . , xn))) − f(S)| < ϵ\nNote that γ(v(x1, . . . , xn)) can be rewritten as follows:\nγ(v(x1, . . . , xn)) =γ(MAX(h(x1), . . . , h(xn)))\n=(γ ◦ MAX)(h(x1), . . . , h(xn))\nObviously γ ◦ MAX is a symmetric function.\nNext we give the proof of Theorem 2.\nWe deﬁne\nu = MAX\nxi∈S {h(xi)} to be the sub-network of f which\nmaps a point set in [0, 1]m to a K-dimensional vector. The\nfollowing theorem tells us that small corruptions or extra\nnoise points in the input set is not likely to change the output\nof our network:\nTheorem 2. Suppose u : X\n→ RK such that u =\nxi∈S{h(xi)} and f = γ ◦ u. Then,",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 15
        },
        {
          "page_number": 15,
          "text": "(a) ∀S, ∃ CS, NS ⊆ X, f(T) = f(S) if CS ⊆ T ⊆ NS;\n(b) |CS| ≤ K\nProof. Obviously, ∀S ∈ X, f(S) is determined by u(S).\nSo we only need to prove that ∀S, ∃ CS, NS ⊆ X, f(T) =\nf(S) if CS ⊆ T ⊆ NS.\nFor the jth dimension as the output of u, there exists at\nleast one xj ∈ X such that hj(xj) = uj, where hj is the\njth dimension of the output vector from h. Take CS as the\nunion of all xj for j = 1, . . . , K. Then, CS satisﬁes the\nabove condition.\nAdding any additional points x such that h(x) ≤ u(S) at\nall dimensions to CS does not change u, hence f. Therefore,\nTS can be obtained adding the union of all such points to\nNS.\nFigure 19. Point function visualization.\nFor each per-point\nfunction h, we calculate the values h(p) for all the points p in a\ncube of diameter two located at the origin, which spatially covers\nthe unit sphere to which our input shapes are normalized when\ntraining our PointNet. In this ﬁgure, we visualize all the points\np that give h(p) > 0.5 with function values color-coded by the\nbrightness of the voxel. We randomly pick 15 point functions and\nvisualize the activation regions for them.\nH. More Visualizations\nClassiﬁcation Visualization\nWe use t-SNE[15] to embed\npoint cloud global signature (1024-dim) from our classiﬁcation PointNet into a 2D space. Fig 20 shows the embedding\nspace of ModelNet 40 test split shapes. Similar shapes are\nclustered together according to their semantic categories.\nSegmentation Visualization\nWe present more segmentation results on both complete CAD models and simulated\nKinect partial scans. We also visualize failure cases with\nerror analysis. Fig 21 and Fig 22 show more segmentation\nresults generated on complete CAD models and their\nsimulated Kinect scans.\nFig 23 illustrates some failure\ncases. Please read the caption for the error analysis.\nScene Semantic Parsing Visualization\nWe give a visualization of semantic parsing in Fig 24 where we show input\npoint cloud, prediction and ground truth for both semantic\nsegmentation and object detection for two ofﬁce rooms and\none conference room. The area and the rooms are unseen in\nthe training set.\nPoint Function Visualization\nOur classiﬁcation PointNet computes K (we take K = 1024 in this visualization)\ndimension point features for each point and aggregates\nall the per-point local features via a max pooling layer\ninto a single K-dim vector, which forms the global shape\ndescriptor.\nTo gain more insights on what the learnt per-point\nfunctions h’s detect, we visualize the points pi’s that\ngive high per-point function value f(pi) in Fig 19. This\nvisualization clearly shows that different point functions\nlearn to detect for points in different regions with various\nshapes scattered in the whole space.",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 15
        },
        {
          "page_number": 16,
          "text": "Figure 20. 2D embedding of learnt shape global features. We use t-SNE technique to visualize the learnt global shape features for the\nshapes in ModelNet40 test split.",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 17,
          "text": "airplane\nbag\ncap\ncar\nearphone chair\nguitar\nknife\nrocket\npistol\ntable\nskate\nboard\nmotor\nbike\nmug\nlaptop\nlamp\nFigure 21. PointNet segmentation results on complete CAD models.\nairplane\nbag\ncap\ncar\nearphone chair\nguitar\nknife\nrocket\npistol\ntable\nskate\nboard\nmotor\nbike\nmug\nlaptop\nlamp\nFigure 22. PointNet segmentation results on simulated Kinect scans.",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 96
        },
        {
          "page_number": 18,
          "text": "(d)\n(b)\n(c)\n(a)\n(e)\n(f)\nFigure 23. PointNet segmentation failure cases. In this ﬁgure, we summarize six types of common errors in our segmentation application.\nThe prediction and the ground-truth segmentations are given in the ﬁrst and second columns, while the difference maps are computed and\nshown in the third columns. The red dots correspond to the wrongly labeled points in the given point clouds. (a) illustrates the most\ncommon failure cases: the points on the boundary are wrongly labeled. In the examples, the label predictions for the points near the\nintersections between the table/chair legs and the tops are not accurate. However, most segmentation algorithms suffer from this error. (b)\nshows the errors on exotic shapes. For examples, the chandelier and the airplane shown in the ﬁgure are very rare in the data set. (c) shows\nthat small parts can be overwritten by nearby large parts. For example, the jet engines for airplanes (yellow in the ﬁgure) are mistakenly\nclassiﬁed as body (green) or the plane wing (purple). (d) shows the error caused by the inherent ambiguity of shape parts. For example,\nthe two bottoms of the two tables in the ﬁgure are classiﬁed as table legs and table bases (category other in [29]), while ground-truth\nsegmentation is the opposite. (e) illustrates the error introduced by the incompleteness of the partial scans. For the two caps in the ﬁgure,\nalmost half of the point clouds are missing. (f) shows the failure cases when some object categories have too less training data to cover\nenough variety. There are only 54 bags and 39 caps in the whole dataset for the two categories shown here.",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 36
        },
        {
          "page_number": 19,
          "text": "Figure 24. Examples of semantic segmentation and object detection. First row is input point cloud, where walls and ceiling are hided\nfor clarity. Second and third rows are prediction and ground-truth of semantic segmentation on points, where points belonging to different\nsemantic regions are colored differently (chairs in red, tables in purple, sofa in orange, board in gray, bookcase in green, ﬂoors in blue,\nwindows in violet, beam in yellow, column in magenta, doors in khaki and clutters in black). The last two rows are object detection with\nbounding boxes, where predicted boxes are from connected components based on semantic segmentation prediction.",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 1
        }
      ],
      "sections": {
        "title": "Figure 22. PointNet segmentation results on simulated Kinect scans.\n\n(d)\n(b)\n(c)\n(a)\n(e)\n(f)\nFigure 23. PointNet segmentation failure cases. In this ﬁgure, we summarize six types of common errors in our segmentation application.\nThe prediction and the ground-truth segmentations are given in the ﬁrst and second columns, while the difference maps are computed and\nshown in the third columns. The red dots correspond to the wrongly labeled points in the given point clouds. (a) illustrates the most\ncommon failure cases: the points on the boundary are wrongly labeled. In the examples, the label predictions for the points near the\nintersections between the table/chair legs and the tops are not accurate. However, most segmentation algorithms suffer from this error. (b)\nshows the errors on exotic shapes. For examples, the chandelier and the airplane shown in the ﬁgure are very rare in the data set. (c) shows\nthat small parts can be overwritten by nearby large parts. For example, the jet engines for airplanes (yellow in the ﬁgure) are mistakenly\nclassiﬁed as body (green) or the plane wing (purple). (d) shows the error caused by the inherent ambiguity of shape parts. For example,\nthe two bottoms of the two tables in the ﬁgure are classiﬁed as table legs and table bases (category other in [29]), while ground-truth\nsegmentation is the opposite. (e) illustrates the error introduced by the incompleteness of the partial scans. For the two caps in the ﬁgure,\nalmost half of the point clouds are missing. (f) shows the failure cases when some object categories have too less training data to cover\nenough variety. There are only 54 bags and 39 caps in the whole dataset for the two categories shown here.\n\nFigure 24. Examples of semantic segmentation and object detection. First row is input point cloud, where walls and ceiling are hided\nfor clarity. Second and third rows are prediction and ground-truth of semantic segmentation on points, where points belonging to different\nsemantic regions are colored differently (chairs in red, tables in purple, sofa in orange, board in gray, bookcase in green, ﬂoors in blue,\nwindows in violet, beam in yellow, column in magenta, doors in khaki and clutters in black). The last two rows are object detection with\nbounding boxes, where predicted boxes are from connected components based on semantic segmentation prediction.",
        "abstract": "PointNet: Deep Learning on Point Sets for 3D Classiﬁcation and Segmentation\nCharles R. Qi*\nHao Su*\nKaichun Mo\nLeonidas J. Guibas\nStanford University",
        "introduction": "In this paper we explore deep learning architectures\ncapable of reasoning about 3D geometric data such as\npoint clouds or meshes. Typical convolutional architectures\nrequire highly regular input data formats, like those of\nimage grids or 3D voxels, in order to perform weight\nsharing and other kernel optimizations. Since point clouds\nor meshes are not in a regular format, most researchers\ntypically transform such data to regular 3D voxel grids or\ncollections of images (e.g, views) before feeding them to\na deep net architecture. This data representation transformation, however, renders the resulting data unnecessarily\nvoluminous - while also introducing quantization artifacts\nthat can obscure natural invariances of the data.\nFor this reason we focus on a different input representation for 3D geometry using simply point clouds\n- and name our resulting deep nets PointNets.\nPoint\nclouds are simple and uniﬁed structures that avoid the\ncombinatorial irregularities and complexities of meshes,\nand thus are easier to learn from. The PointNet, however,\n* indicates equal contributions.\nmug?\ntable?\ncar?\nClassification\nPart Segmentation\nPointNet\nSemantic Segmentation\nInput Point Cloud (point set representation)\nFigure 1. Applications of PointNet. We propose a novel deep net\narchitecture that consumes raw point cloud (set of points) without\nvoxelization or rendering. It is a uniﬁed architecture that learns\nboth global and local point features, providing a simple, efﬁcient\nand effective approach for a number of 3D recognition tasks.\nstill has to respect the fact that a point cloud is just a\nset of points and therefore invariant to permutations of its\nmembers, necessitating certain symmetrizations in the net\ncomputation. Further invariances to rigid motions also need\nto be considered.\nOur PointNet is a uniﬁed architecture that directly\ntakes point clouds as input and outputs either class labels\nfor the entire input or per point segment/part labels for\neach point of the input.\nThe basic architecture of our\nnetwork is surprisingly simple as in the initial stages each\npoint is processed identically and independently.\nIn the\nbasic setting each point is represented by just its three\ncoordinates (x, y, z). Additional dimensions may be added\nby computing normals and other local or global features.\nKey to our approach is the use of a single symmetric\nfunction, max pooling.\nEffectively the network learns a\nset of optimization functions/criteria that select interesting\nor informative points of the point cloud and encode the\nreason for their selection. The ﬁnal fully connected layers\nof the network aggregate these learnt optimal values into the\nglobal descriptor for the entire shape as mentioned above\n(shape classiﬁcation) or are used to predict per point labels\n(shape segmentation).\nOur input format is easy to apply rigid or afﬁne transformations to, as each point transforms independently. Thus\nwe can add a data-dependent spatial transformer network\nthat attempts to canonicalize the data before the PointNet\nprocesses them, so as to further improve the results.\narXiv:1612.00593v2 [cs.CV] 10 Apr 2017\n\nWe provide both a theoretical analysis and an experimental evaluation of our approach.\nWe show that\nour network can approximate any set function that is\ncontinuous. More interestingly, it turns out that our network\nlearns to summarize an input point cloud by a sparse set of\nkey points, which roughly corresponds to the skeleton of\nobjects according to visualization. The theoretical analysis\nprovides an understanding why our PointNet is highly\nrobust to small perturbation of input points as well as\nto corruption through point insertion (outliers) or deletion\n(missing data).\nOn a number of benchmark datasets ranging from shape\nclassiﬁcation, part segmentation to scene segmentation,\nwe experimentally compare our PointNet with state-ofthe-art approaches based upon multi-view and volumetric\nrepresentations. Under a uniﬁed architecture, not only is\nour PointNet much faster in speed, but it also exhibits strong\nperformance on par or even better than state of the art.\nThe key contributions of our work are as follows:\n• We design a novel deep net architecture suitable for\nconsuming unordered point sets in 3D;\n• We show how such a net can be trained to perform\n3D shape classiﬁcation, shape part segmentation and\nscene semantic parsing tasks;\n• We provide thorough empirical and theoretical analysis on the stability and efﬁciency of our method;\n• We illustrate the 3D features computed by the selected\nneurons in the net and develop intuitive explanations\nfor its performance.\nThe problem of processing unordered sets by neural nets\nis a very general and fundamental problem - we expect that\nour ideas can be transferred to other domains as well.",
        "literature_review": "",
        "methodology": "",
        "results": "",
        "discussion": "",
        "conclusion": "In this work, we propose a novel deep neural network\nPointNet that directly consumes point cloud. Our network\nprovides a uniﬁed approach to a number of 3D recognition\ntasks including object classiﬁcation, part segmentation and\nsemantic segmentation, while obtaining on par or better\nresults than state of the arts on standard benchmarks. We\nalso provide theoretical analysis and visualizations towards\nunderstanding of our network.\nAcknowledgement.\nThe authors gratefully acknowledge\nthe support of a Samsung GRO grant, ONR MURI N0001413-1-0341 grant, NSF grant IIS-1528025, a Google Focused Research Award, a gift from the Adobe corporation\nand hardware donations by NVIDIA.",
        "references": "[1] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis,\nM. Fischer, and S. Savarese.\n3d semantic parsing of\nlarge-scale indoor spaces.\nIn Proceedings of the IEEE\nInternational Conference on Computer Vision and Pattern\nRecognition, 2016. 6, 7\n[2] M. Aubry, U. Schlickewei, and D. Cremers.\nThe wave\nkernel signature: A quantum mechanical approach to shape\nanalysis. In Computer Vision Workshops (ICCV Workshops),\n2011 IEEE International Conference on, pages 1626-1633.\nIEEE, 2011. 2\n[3] M. M. Bronstein and I. Kokkinos.\nScale-invariant heat\nkernel signatures for non-rigid shape recognition.\nIn\nComputer Vision and Pattern Recognition (CVPR), 2010\nIEEE Conference on, pages 1704-1711. IEEE, 2010. 2\n[4] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral\nnetworks and locally connected networks on graphs. arXiv\npreprint arXiv:1312.6203, 2013. 2\n[5] D.-Y. Chen, X.-P. Tian, Y.-T. Shen, and M. Ouhyoung. On\nvisual similarity based 3d model retrieval.\nIn Computer\ngraphics forum, volume 22, pages 223-232. Wiley Online\nLibrary, 2003. 2\n[6] Y. Fang, J. Xie, G. Dai, M. Wang, F. Zhu, T. Xu, and\nE. Wong.\n3d deep shape descriptor.\nIn Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 2319-2328, 2015. 2\n[7] M. Gschwandtner, R. Kwitt, A. Uhl, and W. Pree. BlenSor:\nBlender Sensor Simulation Toolbox Advances in Visual\nComputing.\nvolume 6939 of Lecture Notes in Computer\nScience, chapter 20, pages 199-208. Springer Berlin /\nHeidelberg, Berlin, Heidelberg, 2011. 6\n[8] K. Guo, D. Zou, and X. Chen.\n3d mesh labeling via\ndeep convolutional neural networks. ACM Transactions on\nGraphics (TOG), 35(1):3, 2015. 2\n[9] M. Jaderberg, K. Simonyan, A. Zisserman, et al.\nSpatial\ntransformer networks. In NIPS 2015. 4\n[10] A. E. Johnson and M. Hebert. Using spin images for efﬁcient\nobject recognition in cluttered 3d scenes. IEEE Transactions\non pattern analysis and machine intelligence, 21(5):433-\n449, 1999. 2\n[11] M. Kazhdan, T. Funkhouser, and S. Rusinkiewicz. Rotation\ninvariant spherical harmonic representation of 3 d shape descriptors. In Symposium on geometry processing, volume 6,\npages 156-164, 2003. 5\n[12] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. 13\n[13] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas.\nFpnn:\nField probing neural networks for 3d data. arXiv preprint\narXiv:1605.06240, 2016. 2\n[14] H. Ling and D. W. Jacobs. Shape classiﬁcation using the\ninner-distance. IEEE transactions on pattern analysis and\nmachine intelligence, 29(2):286-299, 2007. 2\n[15] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne.\nJournal of Machine Learning Research, 9(Nov):2579-2605,\n2008. 15\n[16] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst.\nGeodesic convolutional neural networks on riemannian manifolds. In Proceedings of the IEEE International Conference\non Computer Vision Workshops, pages 37-45, 2015. 2\n[17] D. Maturana and S. Scherer. Voxnet: A 3d convolutional\nneural network for real-time object recognition. In IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems,\nSeptember 2015. 2, 5, 10, 11\n[18] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. Guibas.\nVolumetric and multi-view cnns for object classiﬁcation on\n3d data. In Proc. Computer Vision and Pattern Recognition\n(CVPR), IEEE, 2016. 2, 5, 8\n[19] R. B. Rusu, N. Blodow, and M. Beetz. Fast point feature\nhistograms (fpfh) for 3d registration.\nIn Robotics and\nAutomation, 2009. ICRA’09. IEEE International Conference\non, pages 3212-3217. IEEE, 2009. 2\n[20] R. B. Rusu, N. Blodow, Z. C. Marton, and M. Beetz. Aligning point cloud views using persistent feature histograms.\nIn 2008 IEEE/RSJ International Conference on Intelligent\nRobots and Systems, pages 3384-3391. IEEE, 2008. 2\n[21] M. Savva, F. Yu, H. Su, M. Aono, B. Chen, D. Cohen-Or,\nW. Deng, H. Su, S. Bai, X. Bai, et al. Shrec16 track largescale 3d shape retrieval from shapenet core55. 2\n[22] P. Y. Simard, D. Steinkraus, and J. C. Platt. Best practices for\nconvolutional neural networks applied to visual document\nanalysis. In ICDAR, volume 3, pages 958-962, 2003. 13\n[23] H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller.\nMulti-view convolutional neural networks for 3d shape\nrecognition. In Proc. ICCV, to appear, 2015. 2, 5, 6, 8\n[24] J. Sun, M. Ovsjanikov, and L. Guibas.\nA concise and\nprovably informative multi-scale signature based on heat\ndiffusion. In Computer graphics forum, volume 28, pages\n1383-1392. Wiley Online Library, 2009. 2\n[25] O. Vinyals, S. Bengio, and M. Kudlur.\nOrder matters:\nSequence to sequence for sets.\narXiv preprint\narXiv:1511.06391, 2015. 2, 4, 7\n[26] D. Z. Wang and I. Posner. Voting for voting in online point\ncloud object detection. Proceedings of the Robotics: Science\nand Systems, Rome, Italy, 1317, 2015. 2\n[27] Z. Wu, R. Shou, Y. Wang, and X. Liu. Interactive shape cosegmentation via label propagation. Computers & Graphics,\n38:248-254, 2014. 6, 10\n[28] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and\nJ. Xiao. 3d shapenets: A deep representation for volumetric\nshapes. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 1912-1920, 2015. 2,\n5, 11\n[29] L. Yi, V. G. Kim, D. Ceylan, I.-C. Shen, M. Yan, H. Su,\nC. Lu, Q. Huang, A. Sheffer, and L. Guibas. A scalable active\nframework for region annotation in 3d shape collections.\nSIGGRAPH Asia, 2016. 6, 10, 18\n\nSupplementary\nA. Overview\nThis document provides additional quantitative results,\ntechnical details and more qualitative test examples to the\nmain paper.\nIn Sec B we extend the robustness test to compare\nPointNet with VoxNet on incomplete input.\nIn Sec C\nwe provide more details on neural network architectures,\ntraining parameters and in Sec D we describe our detection\npipeline in scenes. Then Sec E illustrates more applications\nof PointNet, while Sec F shows more analysis experiments.\nSec G provides a proof for our theory on PointNet. At last,\nwe show more visualization results in Sec H.\nB. Comparison between PointNet and VoxNet\n(Sec 5.2)\nWe extend the experiments in Sec 5.2 Robustness Test\nto compare PointNet and VoxNet [17] (a representative\narchitecture for volumetric representation) on robustness to\nmissing data in the input point cloud. Both networks are\ntrained on the same train test split with 1024 number of\npoints as input. For VoxNet we voxelize the point cloud\nto 32 × 32 × 32 occupancy grids and augment the training\ndata by random rotation around up-axis and jittering.\nAt test time, input points are randomly dropped out\nby a certain ratio.\nAs VoxNet is sensitive to rotations,\nits prediction uses average scores from 12 viewpoints of\na point cloud.\nAs shown in Fig 8, we see that our\nPointNet is much more robust to missing points. VoxNet’s\naccuracy dramatically drops when half of the input points\nare missing, from 86.3% to 46.0% with a 40.3% difference,\nwhile our PointNet only has a 3.7% performance drop. This\ncan be explained by the theoretical analysis and explanation\nof our PointNet - it is learning to use a collection of critical\npoints to summarize the shape, thus it is very robust to\nmissing data.\nC. Network Architecture and Training Details\n(Sec 5.1)\nPointNet Classiﬁcation Network\nAs the basic architecture is already illustrated in the main paper, here we\nprovides more details on the joint alignment/transformation\nnetwork and training parameters.\nThe ﬁrst transformation network is a mini-PointNet that\ntakes raw point cloud as input and regresses to a 3 × 3\nmatrix.\nIt’s composed of a shared MLP(64, 128, 1024)\nnetwork (with layer output sizes 64, 128, 1024) on each\npoint, a max pooling across points and two fully connected\nlayers with output sizes 512, 256.\nThe output matrix is\ninitialized as an identity matrix. All layers, except the last\none, include ReLU and batch normalization. The second\nPointNet\nVoxNet\n87.1\n86.3\n0.5\n83.3\n0.75\n18.5\n0.875\n59.2\n13.3\n0.9375\n33.2\n10.2\n20 \n60 \n100 \n0.2 \n0.4 \n0.6 \n0.8 \nAccuracy (%) \nMissing Data Ratio \nPointNet \nVoxNet \nFigure 8. PointNet v.s. VoxNet [17] on incomplete input data.\nMetric is overall classiﬁcation accurcacy on ModelNet40 test set.\nNote that VoxNet is using 12 viewpoints averaging while PointNet\nis using only one view of the point cloud. Evidently PointNet\npresents much stronger robustness to missing points.\ntransformation network has the same architecture as the ﬁrst\none except that the output is a 64 × 64 matrix. The matrix\nis also initialized as an identity. A regularization loss (with\nweight 0.001) is added to the softmax classiﬁcation loss to\nmake the matrix close to orthogonal.\nWe use dropout with keep ratio 0.7 on the last fully\nconnected layer, whose output dimension 256, before class\nscore prediction. The decay rate for batch normalization\nstarts with 0.5 and is gradually increased to 0.99. We use\nadam optimizer with initial learning rate 0.001, momentum\n0.9 and batch size 32. The learning rate is divided by 2\nevery 20 epochs. Training on ModelNet takes 3-6 hours to\nconverge with TensorFlow and a GTX1080 GPU.\nPointNet Segmentation Network\nThe segmentation network is an extension to the classiﬁcation PointNet. Local\npoint features (the output after the second transformation\nnetwork) and global feature (output of the max pooling)\nare concatenated for each point. No dropout is used for\nsegmentation network. Training parameters are the same\nas the classiﬁcation network.\nAs to the task of shape part segmentation, we made\na few modiﬁcations to the basic segmentation network\narchitecture (Fig 2 in main paper) in order to achieve best\nperformance, as illustrated in Fig 9.\nWe add a one-hot\nvector indicating the class of the input and concatenate it\nwith the max pooling layer’s output.\nWe also increase\nneurons in some layers and add skip links to collect local\npoint features in different layers and concatenate them to\nform point feature input to the segmentation network.\nWhile [27] and [29] deal with each object category\nindependently, due to the lack of training data for some\ncategories (the total number of shapes for all the categories\nin the data set are shown in the ﬁrst line), we train our\nPointNet across categories (but with one-hot vector input to\nindicate category). To allow fair comparison, when testing\n\ninput points\npoint features\noutput scores\nmax\npool\nshared \nshared \nshared \nnx3\nnx3\nnx64\nnx64\nnx1024\nn x 1088\nnx128\nmlp (64,128,128)\nmlp (64,128,1024)\ninput\ntransform\nfeature\ntransform\nmlp\n(512,256,k)\nglobal feature\nmlp (512,256,128)\nT-Net\nmatrix\nmultiply\n3x3\ntransform\nT-Net\nmatrix\nmultiply\n64x64\ntransform\nshared \nmlp (128,m)\noutput scores\nnxm\nk\nClassification Network\nSegmentation Network\nnx3\nnx3\nT1\nnx64\nnx128\nnx128\nnx128\nT2\nnx512\nnx2048\nnx64\nnx128\nnx128\nnx128\nnx512\nn x 3024\n(256,256,128)\nnx50\none-hot\ninput points\npart scores\nFigure 9. Network architecture for part segmentation. T1 and\nT2 are alignment/transformation networks for input points and\nfeatures. FC is fully connected layer operating on each point. MLP\nis multi-layer perceptron on each point. One-hot is a vector of size\n16 indicating category of the input shape.\n32 filters \nof stride 1\n32\n32\n32\n5\n32 filters \nof stride 1\n32 filters \nof stride 1\n32 filters \nof stride 1\n32 filters \nof stride 1\n1\n64 filters \nof stride 1\n1\n64 filters \nof stride 1\n32\n1\n50 filters \nof stride 1\nin-category\nprediction\nFigure 10. Baseline 3D CNN segmentation network.\nThe\nnetwork is fully convolutional and predicts part scores for each\nvoxel.\nthese two models, we only predict part labels for the given\nspeciﬁc object category.\nAs to semantic segmentation task, we used the architecture as in Fig 2 in the main paper.\nIt takes around six to twelve hours to train the model on\nShapeNet part dataset and around half a day to train on the\nStanford semantic parsing dataset.\nBaseline 3D CNN Segmentation Network\nIn ShapeNet\npart segmentation experiment, we compare our proposed\nsegmentation version PointNet to two traditional methods\nas well as a 3D volumetric CNN network baseline.\nIn\nFig 10, we show the baseline 3D volumetric CNN network\nwe use. We generalize the well-known 3D CNN architectures, such as VoxNet [17] and 3DShapeNets [28] to a fully\nconvolutional 3D CNN segmentation network.\nFor a given point cloud, we ﬁrst convert it to the volumetric representation as a occupancy grid with resolution\n32 × 32 × 32. Then, ﬁve 3D convolution operations each\nwith 32 output channels and stride of 1 are sequentially\napplied to extract features. The receptive ﬁeld is 19 for each\nvoxel. Finally, a sequence of 3D convolutional layers with\nkernel size 1 × 1 × 1 is appended to the computed feature\nmap to predict segmentation label for each voxel. ReLU and\nbatch normalization are used for all layers except the last\none. The network is trained across categories, however, in\norder to compare with other baseline methods where object\ncategory is given, we only consider output scores in the\ngiven object category.\nD. Details on Detection Pipeline (Sec 5.1)\nWe build a simple 3D object detection system based on\nthe semantic segmentation results and our object classiﬁcation PointNet.\nWe use connected component with segmentation scores\nto get object proposals in scenes. Starting from a random\npoint in the scene, we ﬁnd its predicted label and use\nBFS to search nearby points with the same label, with\na search radius of 0.2 meter.\nIf the resulted cluster has\nmore than 200 points (assuming a 4096 point sample in\na 1m by 1m area), the cluster’s bounding box is marked\nas one object proposal.\nFor each proposed object, it’s\ndetection score is computed as the average point score for\nthat category. Before evaluation, proposals with extremely\nsmall areas/volumes are pruned.\nFor tables, chairs and\nsofas, the bounding boxes are extended to the ﬂoor in case\nthe legs are separated with the seat/surface.\nWe observe that in some rooms such as auditoriums\nlots of objects (e.g. chairs) are close to each other, where\nconnected component would fail to correctly segment out\nindividual ones. Therefore we leverage our classiﬁcation\nnetwork and uses sliding shape method to alleviate the\nproblem for the chair class. We train a binary classiﬁcation\nnetwork for each category and use the classiﬁer for sliding\nwindow detection.\nThe resulted boxes are pruned by\nnon-maximum suppression.\nThe proposed boxes from\nconnected component and sliding shapes are combined for\nﬁnal evaluation.\nIn Fig 11, we show the precision-recall curves for object\ndetection. We trained six models, where each one of them\nis trained on ﬁve areas and tested on the left area. At test\nphase, each model is tested on the area it has never seen.\nThe test results for all six areas are aggregated for the PR\ncurve generation.\nE. More Applications (Sec 5.1)\nModel Retrieval from Point Cloud\nOur PointNet learns\na global shape signature for every given input point cloud.\nWe expect geometrically similar shapes have similar global\nsignature.\nIn this section, we test our conjecture on the\nshape retrieval application. To be more speciﬁc, for every\ngiven query shape from ModelNet test split, we compute\nits global signature (output of the layer before the score\nprediction layer) given by our classiﬁcation PointNet and\nretrieve similar shapes in the train split by nearest neighbor\nsearch. Results are shown in Fig 12.\n\nFigure 11. Precision-recall curves for object detection in 3D\npoint cloud. We evaluated on all six areas for four categories:\ntable, chair, sofa and board. IoU threshold is 0.5 in volume.\nQuery\nPoint Cloud\nTop-5 Retrieval CAD Models\nFigure 12. Model retrieval from point cloud.\nFor every\ngiven point cloud, we retrieve the top-5 similar shapes from the\nModelNet test split. From top to bottom rows, we show examples\nof chair, plant, nightstand and bathtub queries. Retrieved results\nthat are in wrong category are marked by red boxes.\nShape Correspondence\nIn this section, we show that\npoint features learnt by PointNet can be potentially used\nto compute shape correspondences. Given two shapes, we\ncompute the correspondence between their critical point\nsets CS’s by matching the pairs of points that activate\nthe same dimensions in the global features.\nFig 13 and\nFig 14 show the detected shape correspondence between\ntwo similar chairs and tables.\nF. More Architecture Analysis (Sec 5.2)\nEffects of Bottleneck Dimension and Number of Input\nPoints\nHere we show our model’s performance change\nwith regard to the size of the ﬁrst max layer output as\nwell as the number of input points. In Fig 15 we see that\nperformance grows as we increase the number of points\nhowever it saturates at around 1K points. The max layer\nsize plays an important role, increasing the layer size from\nFigure 13. Shape correspondence between two chairs. For the\nclarity of the visualization, we only show 20 randomly picked\ncorrespondence pairs.\nFigure 14. Shape correspondence between two tables. For the\nclarity of the visualization, we only show 20 randomly picked\ncorrespondence pairs.\n64 to 1024 results in a 2−4% performance gain. It indicates\nthat we need enough point feature functions to cover the 3D\nspace in order to discriminate different shapes.\nIt’s worth notice that even with 64 points as input\n(obtained from furthest point sampling on meshes), our\nnetwork can achieve decent performance.\n82 \n84 \n86 \n88 \n200 \n600 \n1000 \nAccuracy (%) \nBottleneck size \n128 \n1024 \n#points\nFigure 15. Effects of bottleneck size and number of input\npoints. The metric is overall classiﬁcation accuracy on ModelNet40 test set.\nMNIST Digit Classiﬁcation\nWhile we focus on 3D point\ncloud learning, a sanity check experiment is to apply our\nnetwork on a 2D point clouds - pixel sets.\nTo convert an MNIST image into a 2D point set we\nthreshold pixel values and add the pixel (represented as a\n\npoint with (x, y) coordinate in the image) with values larger\nthan 128 to the set. We use a set size of 256. If there are\nmore than 256 pixels int he set, we randomly sub-sample it;\nif there are less, we pad the set with the one of the pixels in\nthe set (due to our max operation, which point to use for the\npadding will not affect outcome).\nAs seen in Table 7, we compare with a few baselines\nincluding multi-layer perceptron that considers input image\nas an ordered vector, a RNN that consider input as sequence\nfrom pixel (0,0) to pixel (27,27), and a vanilla version CNN.\nWhile the best performing model on MNIST is still well\nengineered CNNs (achieving less than 0.3% error rate),\nit’s interesting to see that our PointNet model can achieve\nreasonable performance by considering image as a 2D point\nset.\ninput\nerror (%)\nMulti-layer perceptron [22]\nvector\n1.60\nLeNet5 [12]\nimage\n0.80\nOurs PointNet\npoint set\n0.78\nTable 7. MNIST classiﬁcation results. We compare with vanilla\nversions of other deep architectures to show that our network based\non point sets input is achieving reasonable performance on this\ntraditional task.\nNormal Estimation\nIn segmentation version of PointNet,\nlocal point features and global feature are concatenated\nin order to provide context to local points.\nHowever,\nit’s unclear whether the context is learnt through this\nconcatenation. In this experiment, we validate our design\nby showing that our segmentation network can be trained\nto predict point normals, a local geometric property that is\ndetermined by a point’s neighborhood.\nWe train a modiﬁed version of our segmentation PointNet in a supervised manner to regress to the groundtruth point normals. We just change the last layer of our\nsegmentation PointNet to predict normal vector for each\npoint. We use absolute value of cosine distance as loss.\nFig. 16 compares our PointNet normal prediction results\n(the left columns) to the ground-truth normals computed\nfrom the mesh (the right columns).\nWe observe a\nreasonable normal reconstruction.\nOur predictions are\nmore smooth and continuous than the ground-truth which\nincludes ﬂipped normal directions in some region.\nSegmentation Robustness\nAs discussed in Sec 5.2 and\nSec B, our PointNet is less sensitive to data corruption and\nmissing points for classiﬁcation tasks since the global shape\nfeature is extracted from a collection of critical points from\nthe given input point cloud. In this section, we show that the\nrobustness holds for segmentation tasks too. The per-point\npart labels are predicted based on the combination of perpoint features and the learnt global shape feature. In Fig 17,\nGround-truth\nPrediction\nFigure 16. PointNet normal reconstrution results. In this ﬁgure,\nwe show the reconstructed normals for all the points in some\nsample point clouds and the ground-truth normals computed on\nthe mesh.\nwe illustrate the segmentation results for the given input\npoint clouds S (the left-most column), the critical point sets\nCS (the middle column) and the upper-bound shapes NS.\nNetwork Generalizability to Unseen Shape Categories\nIn Fig 18, we visualize the critical point sets and the upperbound shapes for new shapes from unseen categories (face,\nhouse, rabbit, teapot) that are not present in ModelNet or\nShapeNet. It shows that the learnt per-point functions are\ngeneralizable.\nHowever, since we train mostly on manmade objects with lots of planar structures, the reconstructed upper-bound shape in novel categories also contain\nmore planar surfaces.\nG. Proof of Theorem (Sec 4.3)\nLet X = {S : S ⊆ [0, 1] and |S| = n}.\nf : X → R is a continuous function on X w.r.t to\nHausdorff distance dH(·, ·) if the following condition is\nsatisﬁed:\n∀ϵ > 0, ∃δ > 0, for any S, S′ ∈ X, if dH(S, S′) < δ,\nthen |f(S) − f(S′)| < ϵ.\nWe show that f can be approximated arbitrarily by\ncomposing a symmetric function and a continuous function.\n\nInput Point Cloud\nCritical Point Sets\nUpper-bound Shapes\nFigure 17. The consistency of segmentation results.\nWe\nillustrate the segmentation results for some sample given point\nclouds S, their critical point sets CS and upper-bound shapes NS.\nWe observe that the shape family between the CS and NS share a\nconsistent segmentation results.\nOriginal Shape\nCritical Point Sets\nUpper-bound Shapes\nFigure 18. The critical point sets and the upper-bound shapes\nfor unseen objects. We visualize the critical point sets and the\nupper-bound shapes for teapot, bunny, hand and human body,\nwhich are not in the ModelNet or ShapeNet shape repository to\ntest the generalizability of the learnt per-point functions of our\nPointNet on other unseen objects. The images are color-coded\nto reﬂect the depth information.\nTheorem 1. Suppose f\n:\nX\n→\nR is a continuous\nset function w.r.t Hausdorff distance dH(·, ·).\n∀ϵ\n>\n0, ∃ a continuous function h and a symmetric function\ng(x1, . . . , xn) = γ◦MAX, where γ is a continuous function,\nMAX is a vector max operator that takes n vectors as input\nand returns a new vector of the element-wise maximum,\nsuch that for any S ∈ X,\n|f(S) − γ(MAX(h(x1), . . . , h(xn)))| < ϵ\nwhere x1, . . . , xn are the elements of S extracted in certain\norder,\nProof. By the continuity of f, we take δϵ so that |f(S) −\nf(S′)| < ϵ for any S, S′ ∈ X if dH(S, S′) < δϵ.\nDeﬁne K = ⌈1/δϵ⌉, which split [0, 1] into K intervals\nevenly and deﬁne an auxiliary function that maps a point to\nthe left end of the interval it lies in:\nσ(x) = ⌊Kx⌋\nK\nLet ˜S = {σ(x) : x ∈ S}, then\n|f(S) − f( ˜S)| < ϵ\nbecause dH(S, ˜S) < 1/K ≤ δϵ.\nLet hk(x) = e−d(x,[ k−1\nK , k\nK ]) be a soft indicator function\nwhere d(x, I) is the point to set (interval) distance. Let\nh(x) = [h1(x); . . . ; hK(x)], then h : R → RK.\nLet vj(x1, . . . , xn) = max{˜hj(x1), . . . , ˜hj(xn)}, indicating the occupancy of the j-th interval by points in S.\nLet v = [v1; . . . ; vK], then v : R × . . . × R\n\n\n\nn\n→ {0, 1}K\nis a symmetric function, indicating the occupancy of each\ninterval by points in S.\nDeﬁne τ : {0, 1}K → X as τ(v) = { k−1\nK\n: vk ≥ 1},\nwhich maps the occupancy vector to a set which contains\nthe left end of each occupied interval. It is easy to show:\nτ(v(x1, . . . , xn)) ≡ ˜S\nwhere x1, . . . , xn are the elements of S extracted in certain\norder.\nLet γ : RK → R be a continuous function such that\nγ(v) = f(τ(v)) for v ∈ {0, 1}K. Then,\n|γ(v(x1, . . . , xn)) − f(S)|\n=|f(τ(v(x1, . . . , xn))) − f(S)| < ϵ\nNote that γ(v(x1, . . . , xn)) can be rewritten as follows:\nγ(v(x1, . . . , xn)) =γ(MAX(h(x1), . . . , h(xn)))\n=(γ ◦ MAX)(h(x1), . . . , h(xn))\nObviously γ ◦ MAX is a symmetric function.\nNext we give the proof of Theorem 2.\nWe deﬁne\nu = MAX\nxi∈S {h(xi)} to be the sub-network of f which\nmaps a point set in [0, 1]m to a K-dimensional vector. The\nfollowing theorem tells us that small corruptions or extra\nnoise points in the input set is not likely to change the output\nof our network:\nTheorem 2. Suppose u : X\n→ RK such that u =\nxi∈S{h(xi)} and f = γ ◦ u. Then,\n\n(a) ∀S, ∃ CS, NS ⊆ X, f(T) = f(S) if CS ⊆ T ⊆ NS;\n(b) |CS| ≤ K\nProof. Obviously, ∀S ∈ X, f(S) is determined by u(S).\nSo we only need to prove that ∀S, ∃ CS, NS ⊆ X, f(T) =\nf(S) if CS ⊆ T ⊆ NS.\nFor the jth dimension as the output of u, there exists at\nleast one xj ∈ X such that hj(xj) = uj, where hj is the\njth dimension of the output vector from h. Take CS as the\nunion of all xj for j = 1, . . . , K. Then, CS satisﬁes the\nabove condition.\nAdding any additional points x such that h(x) ≤ u(S) at\nall dimensions to CS does not change u, hence f. Therefore,\nTS can be obtained adding the union of all such points to\nNS.\nFigure 19. Point function visualization.\nFor each per-point\nfunction h, we calculate the values h(p) for all the points p in a\ncube of diameter two located at the origin, which spatially covers\nthe unit sphere to which our input shapes are normalized when\ntraining our PointNet. In this ﬁgure, we visualize all the points\np that give h(p) > 0.5 with function values color-coded by the\nbrightness of the voxel. We randomly pick 15 point functions and\nvisualize the activation regions for them.\nH. More Visualizations\nClassiﬁcation Visualization\nWe use t-SNE[15] to embed\npoint cloud global signature (1024-dim) from our classiﬁcation PointNet into a 2D space. Fig 20 shows the embedding\nspace of ModelNet 40 test split shapes. Similar shapes are\nclustered together according to their semantic categories.\nSegmentation Visualization\nWe present more segmentation results on both complete CAD models and simulated\nKinect partial scans. We also visualize failure cases with\nerror analysis. Fig 21 and Fig 22 show more segmentation\nresults generated on complete CAD models and their\nsimulated Kinect scans.\nFig 23 illustrates some failure\ncases. Please read the caption for the error analysis.\nScene Semantic Parsing Visualization\nWe give a visualization of semantic parsing in Fig 24 where we show input\npoint cloud, prediction and ground truth for both semantic\nsegmentation and object detection for two ofﬁce rooms and\none conference room. The area and the rooms are unseen in\nthe training set.\nPoint Function Visualization\nOur classiﬁcation PointNet computes K (we take K = 1024 in this visualization)\ndimension point features for each point and aggregates\nall the per-point local features via a max pooling layer\ninto a single K-dim vector, which forms the global shape\ndescriptor.\nTo gain more insights on what the learnt per-point\nfunctions h’s detect, we visualize the points pi’s that\ngive high per-point function value f(pi) in Fig 19. This\nvisualization clearly shows that different point functions\nlearn to detect for points in different regions with various\nshapes scattered in the whole space.\n\nFigure 20. 2D embedding of learnt shape global features. We use t-SNE technique to visualize the learnt global shape features for the\nshapes in ModelNet40 test split.\n\nairplane\nbag\ncap\ncar\nearphone chair\nguitar\nknife\nrocket\npistol\ntable\nskate\nboard\nmotor\nbike\nmug\nlaptop\nlamp\nFigure 21. PointNet segmentation results on complete CAD models.\nairplane\nbag\ncap\ncar\nearphone chair\nguitar\nknife\nrocket\npistol\ntable\nskate\nboard\nmotor\nbike\nmug\nlaptop",
        "other": "",
        "literature": "Point Cloud Features\nMost existing features for point\ncloud are handcrafted towards speciﬁc tasks. Point features\noften encode certain statistical properties of points and are\ndesigned to be invariant to certain transformations, which\nare typically classiﬁed as intrinsic [2, 24, 3] or extrinsic\n[20, 19, 14, 10, 5]. They can also be categorized as local\nfeatures and global features. For a speciﬁc task, it is not\ntrivial to ﬁnd the optimal feature combination.\nDeep Learning on 3D Data\n3D data has multiple popular\nrepresentations, leading to various approaches for learning.\nVolumetric CNNs: [28, 17, 18] are the pioneers applying\n3D convolutional neural networks on voxelized shapes.\nHowever, volumetric representation is constrained by its\nresolution due to data sparsity and computation cost of\n3D convolution.\nFPNN [13] and Vote3D [26] proposed\nspecial methods to deal with the sparsity problem; however,\ntheir operations are still on sparse volumes, it’s challenging\nfor them to process very large point clouds.\nMultiview\nCNNs: [23, 18] have tried to render 3D point cloud or\nshapes into 2D images and then apply 2D conv nets to\nclassify them.\nWith well engineered image CNNs, this\nline of methods have achieved dominating performance on\nshape classiﬁcation and retrieval tasks [21]. However, it’s\nnontrivial to extend them to scene understanding or other\n3D tasks such as point classiﬁcation and shape completion.\nSpectral CNNs: Some latest works [4, 16] use spectral\nCNNs on meshes. However, these methods are currently\nconstrained on manifold meshes such as organic objects\nand it’s not obvious how to extend them to non-isometric\nshapes such as furniture.\nFeature-based DNNs: [6, 8]\nﬁrstly convert the 3D data into a vector, by extracting\ntraditional shape features and then use a fully connected net\nto classify the shape. We think they are constrained by the\nrepresentation power of the features extracted.\nDeep Learning on Unordered Sets\nFrom a data structure\npoint of view, a point cloud is an unordered set of vectors.\nWhile most works in deep learning focus on regular input\nrepresentations like sequences (in speech and language\nprocessing), images and volumes (video or 3D data), not\nmuch work has been done in deep learning on point sets.\nOne recent work from Oriol Vinyals et al [25] looks\ninto this problem. They use a read-process-write network\nwith attention mechanism to consume unordered input sets\nand show that their network has the ability to sort numbers.\nHowever, since their work focuses on generic sets and NLP\napplications, there lacks the role of geometry in the sets.\n3. Problem Statement\nWe design a deep learning framework that directly\nconsumes unordered point sets as inputs. A point cloud is\nrepresented as a set of 3D points {Pi| i = 1, ..., n}, where\neach point Pi is a vector of its (x, y, z) coordinate plus extra\nfeature channels such as color, normal etc. For simplicity\nand clarity, unless otherwise noted, we only use the (x, y, z)\ncoordinate as our point’s channels.\nFor the object classiﬁcation task, the input point cloud is\neither directly sampled from a shape or pre-segmented from\na scene point cloud. Our proposed deep network outputs\nk scores for all the k candidate classes.\nFor semantic\nsegmentation, the input can be a single object for part region\nsegmentation, or a sub-volume from a 3D scene for object\nregion segmentation. Our model will output n × m scores\nfor each of the n points and each of the m semantic subcategories.\n\ninput points\npoint features\noutput scores\nmax\npool\nshared \nshared \nshared \nnx3\nnx3\nnx64\nnx64\nnx1024\nn x 1088\nnx128\nmlp (64,64)\nmlp (64,128,1024)\ninput\ntransform\nfeature\ntransform\nmlp\n(512,256,k)\nglobal feature\nmlp (512,256,128)\nT-Net\nmatrix\nmultiply\n3x3\ntransform\nT-Net\nmatrix\nmultiply\n64x64\ntransform\nshared \nmlp (128,m)\noutput scores\nnxm\nk\nClassification Network\nSegmentation Network\nFigure 2. PointNet Architecture. The classiﬁcation network takes n points as input, applies input and feature transformations, and then\naggregates point features by max pooling. The output is classiﬁcation scores for k classes. The segmentation network is an extension to the\nclassiﬁcation net. It concatenates global and local features and outputs per point scores. “mlp” stands for multi-layer perceptron, numbers\nin bracket are layer sizes. Batchnorm is used for all layers with ReLU. Dropout layers are used for the last mlp in classiﬁcation net.\n4. Deep Learning on Point Sets\nThe architecture of our network (Sec 4.2) is inspired by\nthe properties of point sets in Rn (Sec 4.1).\n4.1. Properties of Point Sets in Rn\nOur input is a subset of points from an Euclidean space.\nIt has three main properties:\n• Unordered.\nUnlike pixel arrays in images or voxel\narrays in volumetric grids, point cloud is a set of points\nwithout speciﬁc order. In other words, a network that\nconsumes N 3D point sets needs to be invariant to N!\npermutations of the input set in data feeding order.\n• Interaction among points. The points are from a space\nwith a distance metric. It means that points are not\nisolated, and neighboring points form a meaningful\nsubset.\nTherefore, the model needs to be able to\ncapture local structures from nearby points, and the\ncombinatorial interactions among local structures.\n• Invariance under transformations.\nAs a geometric\nobject, the learned representation of the point set\nshould be invariant to certain transformations.\nFor\nexample, rotating and translating points all together\nshould not modify the global point cloud category nor\nthe segmentation of the points.\n4.2. PointNet Architecture\nOur full network architecture is visualized in Fig 2,\nwhere the classiﬁcation network and the segmentation\nnetwork share a great portion of structures. Please read the\ncaption of Fig 2 for the pipeline.\nOur network has three key modules: the max pooling\nlayer as a symmetric function to aggregate information from\nall the points, a local and global information combination\nstructure, and two joint alignment networks that align both\ninput points and point features.\nWe will discuss our reason behind these design choices\nin separate paragraphs below.\nSymmetry Function for Unordered Input\nIn order\nto make a model invariant to input permutation, three\nstrategies exist: 1) sort input into a canonical order; 2) treat\nthe input as a sequence to train an RNN, but augment the\ntraining data by all kinds of permutations; 3) use a simple\nsymmetric function to aggregate the information from each\npoint. Here, a symmetric function takes n vectors as input\nand outputs a new vector that is invariant to the input\norder. For example, + and ∗ operators are symmetric binary\nfunctions.\nWhile sorting sounds like a simple solution, in high\ndimensional space there in fact does not exist an ordering\nthat is stable w.r.t.\npoint perturbations in the general\nsense.\nThis can be easily shown by contradiction.\nIf\nsuch an ordering strategy exists, it deﬁnes a bijection map\nbetween a high-dimensional space and a 1d real line. It\nis not hard to see, to require an ordering to be stable w.r.t\npoint perturbations is equivalent to requiring that this map\npreserves spatial proximity as the dimension reduces, a task\nthat cannot be achieved in the general case.\nTherefore,\nsorting does not fully resolve the ordering issue, and it’s\nhard for a network to learn a consistent mapping from\ninput to output as the ordering issue persists. As shown in\nexperiments (Fig 5), we ﬁnd that applying a MLP directly\non the sorted point set performs poorly, though slightly\nbetter than directly processing an unsorted input.\nThe idea to use RNN considers the point set as a\nsequential signal and hopes that by training the RNN\n\nwith randomly permuted sequences, the RNN will become\ninvariant to input order. However in “OrderMatters” [25]\nthe authors have shown that order does matter and cannot be\ntotally omitted. While RNN has relatively good robustness\nto input ordering for sequences with small length (dozens),\nit’s hard to scale to thousands of input elements, which is\nthe common size for point sets. Empirically, we have also\nshown that model based on RNN does not perform as well\nas our proposed method (Fig 5).\nOur idea is to approximate a general function deﬁned on\na point set by applying a symmetric function on transformed\nelements in the set:\nf({x1, . . . , xn}) ≈ g(h(x1), . . . , h(xn)),\n(1)\nwhere f\n:\n2RN\n→\nR, h\n:\n→\nRK and g\n:\nRK × · · · × RK\n\n\n\nn\n→ R is a symmetric function.\nEmpirically, our basic module is very simple:\nwe\napproximate h by a multi-layer perceptron network and\ng by a composition of a single variable function and a\nmax pooling function.\nThis is found to work well by\nexperiments. Through a collection of h, we can learn a\nnumber of f’s to capture different properties of the set.\nWhile our key module seems simple, it has interesting\nproperties (see Sec 5.3) and can achieve strong performace\n(see Sec 5.1) in a few different applications. Due to the\nsimplicity of our module, we are also able to provide\ntheoretical analysis as in Sec 4.3.\nLocal and Global Information Aggregation\nThe output\nfrom the above section forms a vector [f1, . . . , fK], which\nis a global signature of the input set.\nWe can easily\ntrain a SVM or multi-layer perceptron classiﬁer on the\nshape global features for classiﬁcation.\nHowever, point\nsegmentation requires a combination of local and global\nknowledge. We can achieve this by a simple yet highly\neffective manner.\nOur solution can be seen in Fig 2 (Segmentation Network). After computing the global point cloud feature vector, we feed it back to per point features by concatenating\nthe global feature with each of the point features. Then we\nextract new per point features based on the combined point\nfeatures - this time the per point feature is aware of both the\nlocal and global information.\nWith this modiﬁcation our network is able to predict\nper point quantities that rely on both local geometry and\nglobal semantics. For example we can accurately predict\nper-point normals (ﬁg in supplementary), validating that the\nnetwork is able to summarize information from the point’s\nlocal neighborhood. In experiment session, we also show\nthat our model can achieve state-of-the-art performance on\nshape part segmentation and scene segmentation.\nJoint Alignment Network\nThe semantic labeling of a\npoint cloud has to be invariant if the point cloud undergoes\ncertain geometric transformations, such as rigid transformation. We therefore expect that the learnt representation by\nour point set is invariant to these transformations.\nA natural solution is to align all input set to a canonical\nspace before feature extraction.\nJaderberg et al. [9]\nintroduces the idea of spatial transformer to align 2D\nimages through sampling and interpolation, achieved by a\nspeciﬁcally tailored layer implemented on GPU.\nOur input form of point clouds allows us to achieve this\ngoal in a much simpler way compared with [9]. We do not\nneed to invent any new layers and no alias is introduced as in\nthe image case. We predict an afﬁne transformation matrix\nby a mini-network (T-net in Fig 2) and directly apply this\ntransformation to the coordinates of input points. The mininetwork itself resembles the big network and is composed\nby basic modules of point independent feature extraction,\nmax pooling and fully connected layers. More details about\nthe T-net are in the supplementary.\nThis idea can be further extended to the alignment of\nfeature space, as well. We can insert another alignment network on point features and predict a feature transformation\nmatrix to align features from different input point clouds.\nHowever, transformation matrix in the feature space has\nmuch higher dimension than the spatial transform matrix,\nwhich greatly increases the difﬁculty of optimization. We\ntherefore add a regularization term to our softmax training\nloss. We constrain the feature transformation matrix to be\nclose to orthogonal matrix:\nLreg = ∥I − AAT ∥2\nF ,\n(2)\nwhere A is the feature alignment matrix predicted by a\nmini-network. An orthogonal transformation will not lose\ninformation in the input, thus is desired. We ﬁnd that by\nadding the regularization term, the optimization becomes\nmore stable and our model achieves better performance.\n4.3. Theoretical Analysis\nUniversal approximation\nWe ﬁrst show the universal\napproximation ability of our neural network to continuous\nset functions. By the continuity of set functions, intuitively,\na small perturbation to the input point set should not\ngreatly change the function values, such as classiﬁcation or\nsegmentation scores.\nFormally, let X = {S : S ⊆ [0, 1]m and |S| = n}, f :\nX → R is a continuous set function on X w.r.t to Hausdorff\ndistance dH(·, ·), i.e., ∀ϵ > 0, ∃δ > 0, for any S, S′ ∈ X,\nif dH(S, S′) < δ, then |f(S) − f(S′)| < ϵ. Our theorem\nsays that f can be arbitrarily approximated by our network\ngiven enough neurons at the max pooling layer, i.e., K in\n(1) is sufﬁciently large.\n\nPartial Inputs\nComplete Inputs\nairplane\ncar\nchair\nlamp\nguitar\nmotorbike\nmug\ntable\nbag\nrocket\nearphone\nlaptop\ncap\nknife\npistol\nskateboard\nFigure 3. Qualitative results for part segmentation.\nWe\nvisualize the CAD part segmentation results across all 16 object\ncategories. We show both results for partial simulated Kinect scans\n(left block) and complete ShapeNet CAD models (right block).\nTheorem 1. Suppose f\n:\nX\n→\nR is a continuous\nset function w.r.t Hausdorff distance dH(·, ·).\n∀ϵ\n>\n0, ∃ a continuous function h and a symmetric function\ng(x1, . . . , xn) = γ ◦ MAX, such that for any S ∈ X,\nf(S) − γ\n\nxi∈S {h(xi)}\n < ϵ\nwhere x1, . . . , xn is the full list of elements in S ordered\narbitrarily, γ is a continuous function, and MAX is a vector\nmax operator that takes n vectors as input and returns a\nnew vector of the element-wise maximum.\nThe proof to this theorem can be found in our supplementary material. The key idea is that in the worst case the\nnetwork can learn to convert a point cloud into a volumetric\nrepresentation, by partitioning the space into equal-sized\nvoxels. In practice, however, the network learns a much\nsmarter strategy to probe the space, as we shall see in point\nfunction visualizations.\nBottleneck dimension and stability\nTheoretically and\nexperimentally we ﬁnd that the expressiveness of our\nnetwork is strongly affected by the dimension of the max\npooling layer, i.e., K in (1). Here we provide an analysis,\nwhich also reveals properties related to the stability of our\nmodel.\nWe deﬁne u = MAX\nxi∈S {h(xi)} to be the sub-network of f\nwhich maps a point set in [0, 1]m to a K-dimensional vector.\nThe following theorem tells us that small corruptions or\nextra noise points in the input set are not likely to change\nthe output of our network:\nTheorem 2. Suppose u : X\n→ RK such that u =\nxi∈S{h(xi)} and f = γ ◦ u. Then,\n(a) ∀S, ∃ CS, NS ⊆ X, f(T) = f(S) if CS ⊆ T ⊆ NS;\n(b) |CS| ≤ K\ninput\n#views\naccuracy\naccuracy\navg. class\noverall\nSPH [11]\nmesh\n-\n68.2\n-\n3DShapeNets [28]\nvolume\n77.3\n84.7\nVoxNet [17]\nvolume\n83.0\n85.9\nSubvolume [18]\nvolume\n86.0\n89.2\nLFD [28]\nimage\n75.5\n-\nMVCNN [23]\nimage\n90.1\n-\nOurs baseline\npoint\n-\n72.6\n77.4\nOurs PointNet\npoint\n86.2\n89.2\nTable 1. Classiﬁcation results on ModelNet40. Our net achieves\nstate-of-the-art among deep nets on 3D input.\nWe explain the implications of the theorem. (a) says that\nf(S) is unchanged up to the input corruption if all points\nin CS are preserved; it is also unchanged with extra noise\npoints up to NS. (b) says that CS only contains a bounded\nnumber of points, determined by K in (1). In other words,\nf(S) is in fact totally determined by a ﬁnite subset CS ⊆ S\nof less or equal to K elements. We therefore call CS the\ncritical point set of S and K the bottleneck dimension of f.\nCombined with the continuity of h, this explains the\nrobustness of our model w.r.t point perturbation, corruption\nand extra noise points. The robustness is gained in analogy\nto the sparsity principle in machine learning models.\nIntuitively, our network learns to summarize a shape by\na sparse set of key points. In experiment section we see\nthat the key points form the skeleton of an object.\n5. Experiment\nExperiments are divided into four parts. First, we show\nPointNets can be applied to multiple 3D recognition tasks\n(Sec 5.1).\nSecond, we provide detailed experiments to\nvalidate our network design (Sec 5.2). At last we visualize\nwhat the network learns (Sec 5.3) and analyze time and\nspace complexity (Sec 5.4).\n5.1. Applications\nIn this section we show how our network can be\ntrained to perform 3D object classiﬁcation, object part\nsegmentation and semantic scene segmentation 1.\nEven\nthough we are working on a brand new data representation\n(point sets), we are able to achieve comparable or even\nbetter performance on benchmarks for several tasks.\n3D Object Classiﬁcation\nOur network learns global\npoint cloud feature that can be used for object classiﬁcation.\nWe evaluate our model on the ModelNet40 [28] shape\nclassiﬁcation benchmark. There are 12,311 CAD models\nfrom 40 man-made object categories, split into 9,843 for\n1More application examples such as correspondence and point cloud\nbased CAD model retrieval are included in supplementary material.\n\nmean\naero\nbag\ncap\ncar\nchair\near\nguitar knife\nlamp\nlaptop motor\nmug pistol rocket skate\ntable\nphone\nboard\n# shapes\n76\n898\n69\n392\n451\n184 283\n152\nWu [27]\n-\n63.2\n-\n-\n-\n73.5\n-\n-\n-\n74.4\n-\n-\n-\n-\n-\n-\n74.8\nYi [29]\n81.4\n81.0\n78.4\n77.7\n75.7\n87.6\n61.9\n92.0\n85.4\n82.5\n95.7\n70.6\n91.9 85.9\n53.1\n69.8\n75.3\n3DCNN\n79.4\n75.1\n72.8\n73.3\n70.0\n87.2\n63.5\n88.4\n79.6\n74.4\n93.9\n58.7\n91.8 76.4\n51.2\n65.3\n77.1\nOurs\n83.7\n83.4\n78.7\n82.5\n74.9\n89.6\n73.0\n91.5\n85.9\n80.8\n95.3\n65.2\n93.0 81.2\n57.9\n72.8\n80.6\nTable 2. Segmentation results on ShapeNet part dataset. Metric is mIoU(%) on points. We compare with two traditional methods [27]\nand [29] and a 3D fully convolutional network baseline proposed by us. Our PointNet method achieved the state-of-the-art in mIoU.\ntraining and 2,468 for testing.\nWhile previous methods\nfocus on volumetric and mult-view image representations,\nwe are the ﬁrst to directly work on raw point cloud.\nWe uniformly sample 1024 points on mesh faces according to face area and normalize them into a unit sphere.\nDuring training we augment the point cloud on-the-ﬂy by\nrandomly rotating the object along the up-axis and jitter the\nposition of each points by a Gaussian noise with zero mean\nand 0.02 standard deviation.\nIn Table 1, we compare our model with previous works\nas well as our baseline using MLP on traditional features\nextracted from point cloud (point density, D2, shape contour\netc.).\nOur model achieved state-of-the-art performance\namong methods based on 3D input (volumetric and point\ncloud). With only fully connected layers and max pooling,\nour net gains a strong lead in inference speed and can be\neasily parallelized in CPU as well. There is still a small\ngap between our method and multi-view based method\n(MVCNN [23]), which we think is due to the loss of ﬁne\ngeometry details that can be captured by rendered images.\n3D Object Part Segmentation\nPart segmentation is a\nchallenging ﬁne-grained 3D recognition task. Given a 3D\nscan or a mesh model, the task is to assign part category\nlabel (e.g. chair leg, cup handle) to each point or face.\nWe evaluate on ShapeNet part data set from [29], which\ncontains 16,881 shapes from 16 categories, annotated with\n50 parts in total. Most object categories are labeled with\ntwo to ﬁve parts. Ground truth annotations are labeled on\nsampled points on the shapes.\nWe formulate part segmentation as a per-point classiﬁcation problem. Evaluation metric is mIoU on points. For\neach shape S of category C, to calculate the shape’s mIoU:\nFor each part type in category C, compute IoU between\ngroundtruth and prediction. If the union of groundtruth and\nprediction points is empty, then count part IoU as 1. Then\nwe average IoUs for all part types in category C to get mIoU\nfor that shape. To calculate mIoU for the category, we take\naverage of mIoUs for all shapes in that category.\nIn this section, we compare our segmentation version\nPointNet (a modiﬁed version of Fig 2, Segmentation\nNetwork) with two traditional methods [27] and [29] that\nboth take advantage of point-wise geometry features and\ncorrespondences between shapes, as well as our own\n3D CNN baseline.\nSee supplementary for the detailed\nmodiﬁcations and network architecture for the 3D CNN.\nIn Table 2, we report per-category and mean IoU(%)\nscores. We observe a 2.3% mean IoU improvement and our\nnet beats the baseline methods in most categories.\nWe also perform experiments on simulated Kinect scans\nto test the robustness of these methods. For every CAD\nmodel in the ShapeNet part data set, we use Blensor Kinect\nSimulator [7] to generate incomplete point clouds from six\nrandom viewpoints. We train our PointNet on the complete\nshapes and partial scans with the same network architecture\nand training setting. Results show that we lose only 5.3%\nmean IoU. In Fig 3, we present qualitative results on both\ncomplete and partial data. One can see that though partial\ndata is fairly challenging, our predictions are reasonable.\nSemantic Segmentation in Scenes\nOur network on part\nsegmentation can be easily extended to semantic scene\nsegmentation, where point labels become semantic object\nclasses instead of object part labels.\nWe experiment on the Stanford 3D semantic parsing data\nset [1].\nThe dataset contains 3D scans from Matterport\nscanners in 6 areas including 271 rooms. Each point in the\nscan is annotated with one of the semantic labels from 13\ncategories (chair, table, ﬂoor, wall etc. plus clutter).\nTo prepare training data, we ﬁrstly split points by room,\nand then sample rooms into blocks with area 1m by 1m.\nWe train our segmentation version of PointNet to predict\nmean IoU\noverall accuracy\nOurs baseline\n20.12\n53.19\nOurs PointNet\n47.71\n78.62\nTable 3. Results on semantic segmentation in scenes. Metric is\naverage IoU over 13 classes (structural and furniture elements plus\nclutter) and classiﬁcation accuracy calculated on points.\ntable\nchair\nsofa\nboard\nmean\n# instance\n1363\n137\nArmeni et al. [1]\n46.02\n16.15\n6.78\n3.91\n18.22\nOurs\n46.67\n33.80\n4.76\n11.72\n24.24\nTable 4. Results on 3D object detection in scenes. Metric is\naverage precision with threshold IoU 0.5 computed in 3D volumes.\n\nInput\nOutput\nFigure 4. Qualitative results for semantic segmentation. Top\nrow is input point cloud with color. Bottom row is output semantic\nsegmentation result (on points) displayed in the same camera\nviewpoint as input.\nper point class in each block. Each point is represented by\na 9-dim vector of XYZ, RGB and normalized location as\nto the room (from 0 to 1). At training time, we randomly\nsample 4096 points in each block on-the-ﬂy. At test time,\nwe test on all the points. We follow the same protocol as [1]\nto use k-fold strategy for train and test.\nWe compare our method with a baseline using handcrafted point features. The baseline extracts the same 9dim local features and three additional ones: local point\ndensity, local curvature and normal. We use standard MLP\nas the classiﬁer.\nResults are shown in Table 3, where\nour PointNet method signiﬁcantly outperforms the baseline\nmethod. In Fig 4, we show qualitative segmentation results.\nOur network is able to output smooth predictions and is\nrobust to missing points and occlusions.\nBased on the semantic segmentation output from our\nnetwork, we further build a 3D object detection system\nusing connected component for object proposal (see supplementary for details). We compare with previous stateof-the-art method in Table 4. The previous method is based\non a sliding shape method (with CRF post processing) with\nSVMs trained on local geometric features and global room\ncontext feature in voxel grids. Our method outperforms it\nby a large margin on the furniture categories reported.\n5.2. Architecture Design Analysis\nIn this section we validate our design choices by control\nexperiments. We also show the effects of our network’s\nhyperparameters.\nComparison with Alternative Order-invariant Methods\nAs mentioned in Sec 4.2, there are at least three options for\nconsuming unordered set inputs. We use the ModelNet40\nshape classiﬁcation problem as a test bed for comparisons\nof those options, the following two control experiment will\nalso use this task.\nThe baselines (illustrated in Fig 5) we compared with\ninclude multi-layer perceptron on unsorted and sorted\n(1,2,3)\n(2,3,4)\n(1,3,1)\nrnn \ncell\nrnn \ncell\nrnn \ncell\n...\n(1,2,3)\n(2,3,4)\n(1,3,1)\n...\n...\n(1,2,3)\n(1,3,1)\n(2,3,4)\n...\nsorting\nsequential model\nsymmetry function\nsorted\nFigure 5. Three approaches to achieve order invariance. Multilayer perceptron (MLP) applied on points consists of 5 hidden\nlayers with neuron sizes 64,64,64,128,1024, all points share a\nsingle copy of MLP. The MLP close to the output consists of two\nlayers with sizes 512,256.\npoints as n×3 arrays, RNN model that considers input point\nas a sequence, and a model based on symmetry functions.\nThe symmetry operation we experimented include max\npooling, average pooling and an attention based weighted\nsum. The attention method is similar to that in [25], where\na scalar score is predicted from each point feature, then the\nscore is normalized across points by computing a softmax.\nThe weighted sum is then computed on the normalized\nscores and the point features. As shown in Fig 5, maxpooling operation achieves the best performance by a large\nwinning margin, which validates our choice.\nEffectiveness of Input and Feature Transformations\nIn\nTable 5 we demonstrate the positive effects of our input\nand feature transformations (for alignment). It’s interesting\nto see that the most basic architecture already achieves\nquite reasonable results. Using input transformation gives\na 0.8% performance boost.\nThe regularization loss is\nnecessary for the higher dimension transform to work.\nBy combining both transformations and the regularization\nterm, we achieve the best performance.\nRobustness Test\nWe show our PointNet, while simple\nand effective, is robust to various kinds of input corruptions.\nWe use the same architecture as in Fig 5’s max pooling\nnetwork. Input points are normalized into a unit sphere.\nResults are in Fig 6.\nAs to missing points, when there are 50% points missing,\nthe accuracy only drops by 2.4% and 3.8% w.r.t. furthest\nand random input sampling. Our net is also robust to outlier\nTransform\naccuracy\nnone\n87.1\ninput (3x3)\n87.9\nfeature (64x64)\n86.9\nfeature (64x64) + reg.\n87.4\nboth\n89.2\nTable 5. Effects of input feature transforms. Metric is overall\nclassiﬁcation accuracy on ModelNet40 test set.\n\n30 \n50 \n70 \n90 \n0.05 \n0.1 \nAccuracy (%) \nPerturbation noise std \n40 \n60 \n80 \n100 \n0.2 \n0.4 \n0.6 \n0.8 \nAccuracy (%) \nMissing data ratio \nFurthest \nRandom \n30 \n50 \n70 \n90 \n0.1 \n0.2 \n0.3 \n0.4 \n0.5 \nAccuracy (%) \nOutlier ratio \nXYZ+density \nFigure 6. PointNet robustness test.\nThe metric is overall\nclassiﬁcation accuracy on ModelNet40 test set.\nLeft: Delete\npoints. Furthest means the original 1024 points are sampled with\nfurthest sampling. Middle: Insertion. Outliers uniformly scattered\nin the unit sphere. Right: Perturbation. Add Gaussian noise to\neach point independently.\npoints, if it has seen those during training. We evaluate two\nmodels: one trained on points with (x, y, z) coordinates; the\nother on (x, y, z) plus point density. The net has more than\n80% accuracy even when 20% of the points are outliers.\nFig 6 right shows the net is robust to point perturbations.\n5.3. Visualizing PointNet\nIn Fig 7, we visualize critical point sets CS and upperbound shapes NS (as discussed in Thm 2) for some sample\nshapes S. The point sets between the two shapes will give\nexactly the same global shape feature f(S).\nWe can see clearly from Fig 7 that the critical point\nsets CS, those contributed to the max pooled feature,\nsummarizes the skeleton of the shape. The upper-bound\nshapes NS illustrates the largest possible point cloud that\ngive the same global shape feature f(S) as the input point\ncloud S. CS and NS reﬂect the robustness of PointNet,\nmeaning that losing some non-critical points does not\nchange the global shape signature f(S) at all.\nThe NS is constructed by forwarding all the points in a\nedge-length-2 cube through the network and select points p\nwhose point function values (h1(p), h2(p), · · · , hK(p)) are\nno larger than the global shape descriptor.\nOriginal Shape\nCritical Point Sets\nUpper-bound Shapes\nFigure 7. Critical points and upper bound shape. While critical\npoints jointly determine the global shape feature for a given shape,\nany point cloud that falls between the critical points set and the\nupper bound shape gives exactly the same feature. We color-code\nall ﬁgures to show the depth information.\n5.4. Time and Space Complexity Analysis\nTable 6 summarizes space (number of parameters in\nthe network) and time (ﬂoating-point operations/sample)\ncomplexity of our classiﬁcation PointNet. We also compare\nPointNet to a representative set of volumetric and multiview based architectures in previous works.\nWhile MVCNN [23] and Subvolume (3D CNN) [18]\nachieve high performance, PointNet is orders more efﬁcient\nin computational cost (measured in FLOPs/sample: 141x\nand 8x more efﬁcient, respectively).\nBesides, PointNet\nis much more space efﬁcient than MVCNN in terms of\n#param in the network (17x less parameters). Moreover,\nPointNet is much more scalable - it’s space and time\ncomplexity is O(N) - linear in the number of input points.\nHowever, since convolution dominates computing time,\nmulti-view method’s time complexity grows squarely on\nimage resolution and volumetric convolution based method\ngrows cubically with the volume size.\nEmpirically, PointNet is able to process more than\none million points per second for point cloud classiﬁcation (around 1K objects/second) or semantic segmentation\n(around 2 rooms/second) with a 1080X GPU on TensorFlow, showing great potential for real-time applications.\n#params\nFLOPs/sample\nPointNet (vanilla)\n0.8M\n148M\nPointNet\n3.5M\n440M\nSubvolume [18]\n16.6M\n3633M\nMVCNN [23]\n60.0M\n62057M\nTable 6. Time and space complexity of deep architectures for\n3D data classiﬁcation.\nPointNet (vanilla) is the classiﬁcation\nPointNet without input and feature transformations.\nstands for ﬂoating-point operation. The “M” stands for million.\nSubvolume and MVCNN used pooling on input data from multiple\nrotations or views, without which they have much inferior\nperformance."
      },
      "saved_path": "C:\\Users\\ASUS\\Documents\\GitHub\\AI-System-to-automatically-Review-and-Summarize-Research-Papers\\data\\extracted_text\\PointNet_Deep_Learning_on_Point_Sets_for_3D_Classi_extracted.json"
    },
    {
      "file_name": "Xception_Deep_Learning_with_Depthwise_Separable_Co_20260129.pdf",
      "file_path": "C:\\Users\\ASUS\\Documents\\GitHub\\AI-System-to-automatically-Review-and-Summarize-Research-Papers\\data\\papers\\Xception_Deep_Learning_with_Depthwise_Separable_Co_20260129.pdf",
      "file_size": 804466,
      "extraction_date": "2026-01-29T22:01:09.612929",
      "pdf_type": "text_based",
      "total_pages": 8,
      "total_chars": 28999,
      "total_words": 4401,
      "extraction_method": "pymupdf",
      "pages": [
        {
          "page_number": 1,
          "text": "Xception: Deep Learning with Depthwise Separable Convolutions\nFranc¸ois Chollet\nGoogle, Inc.\n\nAbstract\nWe present an interpretation of Inception modules in convolutional neural networks as being an intermediate step\nin-between regular convolution and the depthwise separable\nconvolution operation (a depthwise convolution followed by\na pointwise convolution). In this light, a depthwise separable\nconvolution can be understood as an Inception module with\na maximally large number of towers. This observation leads\nus to propose a novel deep convolutional neural network\narchitecture inspired by Inception, where Inception modules\nhave been replaced with depthwise separable convolutions.\nWe show that this architecture, dubbed Xception, slightly\noutperforms Inception V3 on the ImageNet dataset (which\nInception V3 was designed for), and signiﬁcantly outperforms Inception V3 on a larger image classiﬁcation dataset\ncomprising 350 million images and 17,000 classes. Since\nthe Xception architecture has the same number of parameters as Inception V3, the performance gains are not due\nto increased capacity but rather to a more efﬁcient use of\nmodel parameters.\n1. Introduction\nConvolutional neural networks have emerged as the master algorithm in computer vision in recent years, and developing recipes for designing them has been a subject of\nconsiderable attention. The history of convolutional neural\nnetwork design started with LeNet-style models [10], which\nwere simple stacks of convolutions for feature extraction\nand max-pooling operations for spatial sub-sampling. In\n2012, these ideas were reﬁned into the AlexNet architecture [9], where convolution operations were being repeated\nmultiple times in-between max-pooling operations, allowing\nthe network to learn richer features at every spatial scale.\nWhat followed was a trend to make this style of network\nincreasingly deeper, mostly driven by the yearly ILSVRC\ncompetition; ﬁrst with Zeiler and Fergus in 2013 [25] and\nthen with the VGG architecture in 2014 [18].\nAt this point a new style of network emerged, the Inception architecture, introduced by Szegedy et al. in 2014 [20]\nas GoogLeNet (Inception V1), later reﬁned as Inception V2\n[7], Inception V3 [21], and most recently Inception-ResNet\n[19]. Inception itself was inspired by the earlier NetworkIn-Network architecture [11]. Since its ﬁrst introduction,\nInception has been one of the best performing family of\nmodels on the ImageNet dataset [14], as well as internal\ndatasets in use at Google, in particular JFT [5].\nThe fundamental building block of Inception-style models is the Inception module, of which several different versions exist. In ﬁgure 1 we show the canonical form of an\nInception module, as found in the Inception V3 architecture. An Inception model can be understood as a stack of\nsuch modules. This is a departure from earlier VGG-style\nnetworks which were stacks of simple convolution layers.\nWhile Inception modules are conceptually similar to convolutions (they are convolutional feature extractors), they\nempirically appear to be capable of learning richer representations with less parameters. How do they work, and\nhow do they differ from regular convolutions? What design\nstrategies come after Inception?\n1.1. The Inception hypothesis\nA convolution layer attempts to learn ﬁlters in a 3D space,\nwith 2 spatial dimensions (width and height) and a channel dimension; thus a single convolution kernel is tasked\nwith simultaneously mapping cross-channel correlations and\nspatial correlations.\nThis idea behind the Inception module is to make this\nprocess easier and more efﬁcient by explicitly factoring it\ninto a series of operations that would independently look at\ncross-channel correlations and at spatial correlations. More\nprecisely, the typical Inception module ﬁrst looks at crosschannel correlations via a set of 1x1 convolutions, mapping\nthe input data into 3 or 4 separate spaces that are smaller than\nthe original input space, and then maps all correlations in\nthese smaller 3D spaces, via regular 3x3 or 5x5 convolutions.\nThis is illustrated in ﬁgure 1. In effect, the fundamental hypothesis behind Inception is that cross-channel correlations\nand spatial correlations are sufﬁciently decoupled that it is\npreferable not to map them jointly 1.\n1A variant of the process is to independently look at width-wise correarXiv:1610.02357v3 [cs.CV] 4 Apr 2017",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 2,
          "text": "Consider a simpliﬁed version of an Inception module that\nonly uses one size of convolution (e.g. 3x3) and does not\ninclude an average pooling tower (ﬁgure 2). This Inception module can be reformulated as a large 1x1 convolution\nfollowed by spatial convolutions that would operate on nonoverlapping segments of the output channels (ﬁgure 3). This\nobservation naturally raises the question: what is the effect of the number of segments in the partition (and their\nsize)? Would it be reasonable to make a much stronger\nhypothesis than the Inception hypothesis, and assume that\ncross-channel correlations and spatial correlations can be\nmapped completely separately?\nFigure 1. A canonical Inception module (Inception V3).\nFigure 2. A simpliﬁed Inception module.\n1.2. The continuum between convolutions and separable convolutions\nAn “extreme” version of an Inception module, based on\nthis stronger hypothesis, would ﬁrst use a 1x1 convolution to\nmap cross-channel correlations, and would then separately\nmap the spatial correlations of every output channel. This\nis shown in ﬁgure 4. We remark that this extreme form of\nan Inception module is almost identical to a depthwise separable convolution, an operation that has been used in neural\nlations and height-wise correlations. This is implemented by some of the\nmodules found in Inception V3, which alternate 7x1 and 1x7 convolutions.\nThe use of such spatially separable convolutions has a long history in image processing and has been used in some convolutional neural network\nimplementations since at least 2012 (possibly earlier).\nFigure 3. A strictly equivalent reformulation of the simpliﬁed Inception module.\nFigure 4. An “extreme” version of our Inception module, with one\nspatial convolution per output channel of the 1x1 convolution.\nnetwork design as early as 2014 [15] and has become more\npopular since its inclusion in the TensorFlow framework [1]\nin 2016.\nA depthwise separable convolution, commonly called\n“separable convolution” in deep learning frameworks such as\nTensorFlow and Keras, consists in a depthwise convolution,\ni.e. a spatial convolution performed independently over each\nchannel of an input, followed by a pointwise convolution,\ni.e. a 1x1 convolution, projecting the channels output by the\ndepthwise convolution onto a new channel space. This is\nnot to be confused with a spatially separable convolution,\nwhich is also commonly called “separable convolution” in\nthe image processing community.\nTwo minor differences between and “extreme” version of\nan Inception module and a depthwise separable convolution\nwould be:\n• The order of the operations: depthwise separable convolutions as usually implemented (e.g. in TensorFlow)\nperform ﬁrst channel-wise spatial convolution and then\nperform 1x1 convolution, whereas Inception performs\nthe 1x1 convolution ﬁrst.\n• The presence or absence of a non-linearity after the\nﬁrst operation. In Inception, both operations are followed by a ReLU non-linearity, however depthwise",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 4
        },
        {
          "page_number": 3,
          "text": "separable convolutions are usually implemented without non-linearities.\nWe argue that the ﬁrst difference is unimportant, in particular because these operations are meant to be used in a\nstacked setting. The second difference might matter, and we\ninvestigate it in the experimental section (in particular see\nﬁgure 10).\nWe also note that other intermediate formulations of Inception modules that lie in between regular Inception modules and depthwise separable convolutions are also possible:\nin effect, there is a discrete spectrum between regular convolutions and depthwise separable convolutions, parametrized\nby the number of independent channel-space segments used\nfor performing spatial convolutions. A regular convolution\n(preceded by a 1x1 convolution), at one extreme of this\nspectrum, corresponds to the single-segment case; a depthwise separable convolution corresponds to the other extreme\nwhere there is one segment per channel; Inception modules\nlie in between, dividing a few hundreds of channels into 3\nor 4 segments. The properties of such intermediate modules\nappear not to have been explored yet.\nHaving made these observations, we suggest that it may\nbe possible to improve upon the Inception family of architectures by replacing Inception modules with depthwise separable convolutions, i.e. by building models that would be\nstacks of depthwise separable convolutions. This is made\npractical by the efﬁcient depthwise convolution implementation available in TensorFlow. In what follows, we present a\nconvolutional neural network architecture based on this idea,\nwith a similar number of parameters as Inception V3, and\nwe evaluate its performance against Inception V3 on two\nlarge-scale image classiﬁcation task.\n2. Prior work\nThe present work relies heavily on prior efforts in the\nfollowing areas:\n• Convolutional neural networks [10, 9, 25], in particular\nthe VGG-16 architecture [18], which is schematically\nsimilar to our proposed architecture in a few respects.\n• The Inception architecture family of convolutional neural networks [20, 7, 21, 19], which ﬁrst demonstrated\nthe advantages of factoring convolutions into multiple\nbranches operating successively on channels and then\non space.\n• Depthwise separable convolutions, which our proposed\narchitecture is entirely based upon. While the use of spatially separable convolutions in neural networks has a\nlong history, going back to at least 2012 [12] (but likely\neven earlier), the depthwise version is more recent. Laurent Sifre developed depthwise separable convolutions\nduring an internship at Google Brain in 2013, and used\nthem in AlexNet to obtain small gains in accuracy and\nlarge gains in convergence speed, as well as a signiﬁcant\nreduction in model size. An overview of his work was\nﬁrst made public in a presentation at ICLR 2014 [23].\nDetailed experimental results are reported in Sifre’s thesis, section 6.2 [15]. This initial work on depthwise separable convolutions was inspired by prior research from\nSifre and Mallat on transformation-invariant scattering\n[16, 15]. Later, a depthwise separable convolution was\nused as the ﬁrst layer of Inception V1 and Inception\nV2 [20, 7]. Within Google, Andrew Howard [6] has\nintroduced efﬁcient mobile models called MobileNets\nusing depthwise separable convolutions. Jin et al. in\n2014 [8] and Wang et al. in 2016 [24] also did related\nwork aiming at reducing the size and computational\ncost of convolutional neural networks using separable\nconvolutions. Additionally, our work is only possible\ndue to the inclusion of an efﬁcient implementation of\ndepthwise separable convolutions in the TensorFlow\nframework [1].\n• Residual connections, introduced by He et al. in [4],\nwhich our proposed architecture uses extensively.\n3. The Xception architecture\nWe propose a convolutional neural network architecture\nbased entirely on depthwise separable convolution layers.\nIn effect, we make the following hypothesis: that the mapping of cross-channels correlations and spatial correlations\nin the feature maps of convolutional neural networks can be\nentirely decoupled. Because this hypothesis is a stronger version of the hypothesis underlying the Inception architecture,\nwe name our proposed architecture Xception, which stands\nfor “Extreme Inception”.\nA complete description of the speciﬁcations of the network is given in ﬁgure 5. The Xception architecture has\n36 convolutional layers forming the feature extraction base\nof the network. In our experimental evaluation we will exclusively investigate image classiﬁcation and therefore our\nconvolutional base will be followed by a logistic regression\nlayer. Optionally one may insert fully-connected layers before the logistic regression layer, which is explored in the\nexperimental evaluation section (in particular, see ﬁgures\n7 and 8). The 36 convolutional layers are structured into\n14 modules, all of which have linear residual connections\naround them, except for the ﬁrst and last modules.\nIn short, the Xception architecture is a linear stack of\ndepthwise separable convolution layers with residual connections. This makes the architecture very easy to deﬁne\nand modify; it takes only 30 to 40 lines of code using a highlevel library such as Keras [2] or TensorFlow-Slim [17], not\nunlike an architecture such as VGG-16 [18], but rather un-",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 4,
          "text": "like architectures such as Inception V2 or V3 which are far\nmore complex to deﬁne. An open-source implementation of\nXception using Keras and TensorFlow is provided as part of\nthe Keras Applications module2, under the MIT license.\n4. Experimental evaluation\nWe choose to compare Xception to the Inception V3 architecture, due to their similarity of scale: Xception and\nInception V3 have nearly the same number of parameters\n(table 3), and thus any performance gap could not be attributed to a difference in network capacity. We conduct\nour comparison on two image classiﬁcation tasks: one is\nthe well-known 1000-class single-label classiﬁcation task on\nthe ImageNet dataset [14], and the other is a 17,000-class\nmulti-label classiﬁcation task on the large-scale JFT dataset.\n4.1. The JFT dataset\nJFT is an internal Google dataset for large-scale image\nclassiﬁcation dataset, ﬁrst introduced by Hinton et al. in [5],\nwhich comprises over 350 million high-resolution images\nannotated with labels from a set of 17,000 classes. To evaluate the performance of a model trained on JFT, we use an\nauxiliary dataset, FastEval14k.\nFastEval14k is a dataset of 14,000 images with dense\nannotations from about 6,000 classes (36.5 labels per image on average). On this dataset we evaluate performance\nusing Mean Average Precision for top 100 predictions\n and we weight the contribution of each class\nto  with a score estimating how common (and\ntherefore important) the class is among social media images.\nThis evaluation procedure is meant to capture performance\non frequently occurring labels from social media, which is\ncrucial for production models at Google.\n4.2. Optimization conﬁguration\nA different optimization conﬁguration was used for ImageNet and JFT:\n• On ImageNet:\n- Optimizer: SGD\n- Momentum: 0.9\n- Initial learning rate: 0.045\n- Learning rate decay: decay of rate 0.94 every 2\nepochs\n• On JFT:\n- Optimizer: RMSprop [22]\n- Momentum: 0.9\n- Initial learning rate: 0.001\n2\n- Learning rate decay: decay of rate 0.9 every\n3,000,000 samples\nFor both datasets, the same exact same optimization conﬁguration was used for both Xception and Inception V3.\nNote that this conﬁguration was tuned for best performance\nwith Inception V3; we did not attempt to tune optimization\nhyperparameters for Xception. Since the networks have different training proﬁles (ﬁgure 6), this may be suboptimal, especially on the ImageNet dataset, on which the optimization\nconﬁguration used had been carefully tuned for Inception\nV3.\nAdditionally, all models were evaluated using Polyak\naveraging [13] at inference time.\n4.3. Regularization conﬁguration\n• Weight decay: The Inception V3 model uses a weight\ndecay (L2 regularization) rate of 4e − 5, which has\nbeen carefully tuned for performance on ImageNet. We\nfound this rate to be quite suboptimal for Xception\nand instead settled for 1e − 5. We did not perform\nan extensive search for the optimal weight decay rate.\nThe same weight decay rates were used both for the\nImageNet experiments and the JFT experiments.\n• Dropout: For the ImageNet experiments, both models\ninclude a dropout layer of rate 0.5 before the logistic\nregression layer. For the JFT experiments, no dropout\nwas included due to the large size of the dataset which\nmade overﬁtting unlikely in any reasonable amount of\ntime.\n• Auxiliary loss tower: The Inception V3 architecture\nmay optionally include an auxiliary tower which backpropagates the classiﬁcation loss earlier in the network,\nserving as an additional regularization mechanism. For\nsimplicity, we choose not to include this auxiliary tower\nin any of our models.\n4.4. Training infrastructure\nAll networks were implemented using the TensorFlow\nframework [1] and trained on 60 NVIDIA K80 GPUs each.\nFor the ImageNet experiments, we used data parallelism\nwith synchronous gradient descent to achieve the best classiﬁcation performance, while for JFT we used asynchronous\ngradient descent so as to speed up training. The ImageNet\nexperiments took approximately 3 days each, while the JFT\nexperiments took over one month each. The JFT models\nwere not trained to full convergence, which would have\ntaken over three month per experiment.",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": false,
          "image_count": 0
        },
        {
          "page_number": 5,
          "text": "Figure 5. The Xception architecture: the data ﬁrst goes through the entry ﬂow, then through the middle ﬂow which is repeated eight times,\nand ﬁnally through the exit ﬂow. Note that all Convolution and SeparableConvolution layers are followed by batch normalization [7] (not\nincluded in the diagram). All SeparableConvolution layers use a depth multiplier of 1 (no depth expansion).\n4.5. Comparison with Inception V3\n4.5.1\nClassiﬁcation performance\nAll evaluations were run with a single crop of the inputs\nimages and a single model. ImageNet results are reported\non the validation set rather than the test set (i.e. on the\nnon-blacklisted images from the validation set of ILSVRC\n2012). JFT results are reported after 30 million iterations\n(one month of training) rather than after full convergence.\nResults are provided in table 1 and table 2, as well as ﬁgure\n6, ﬁgure 7, ﬁgure 8. On JFT, we tested both versions of our\nnetworks that did not include any fully-connected layers, and\nversions that included two fully-connected layers of 4096\nunits each before the logistic regression layer.\nOn ImageNet, Xception shows marginally better results\nthan Inception V3. On JFT, Xception shows a 4.3% relative improvement on the FastEval14k  metric.\nWe also note that Xception outperforms ImageNet results\nreported by He et al. for ResNet-50, ResNet-101 and ResNet152 [4].\nTable 1. Classiﬁcation performance comparison on ImageNet (single crop, single model). VGG-16 and ResNet-152 numbers are\nonly included as a reminder. The version of Inception V3 being\nbenchmarked does not include the auxiliary tower.\nTop-1 accuracy\nTop-5 accuracy\nVGG-16\n0.715\n0.901\nResNet-152\n0.770\n0.933\nInception V3\n0.782\n0.941\nXception\n0.790\n0.945\nThe Xception architecture shows a much larger performance improvement on the JFT dataset compared to the\nImageNet dataset. We believe this may be due to the fact\nthat Inception V3 was developed with a focus on ImageNet\nand may thus be by design over-ﬁt to this speciﬁc task. On\nthe other hand, neither architecture was tuned for JFT. It is\nlikely that a search for better hyperparameters for Xception\non ImageNet (in particular optimization parameters and reg-",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 6,
          "text": "Table 2. Classiﬁcation performance comparison on JFT (single\ncrop, single model).\nFastEval14k \nInception V3 - no FC layers\n6.36\nXception - no FC layers\n6.70\nInception V3 with FC layers\n6.50\nXception with FC layers\n6.78\nFigure 6. Training proﬁle on ImageNet\nFigure 7. Training proﬁle on JFT, without fully-connected layers\nularization parameters) would yield signiﬁcant additional\nimprovement.\n4.5.2\nSize and speed\nTable 3. Size and training speed comparison.\nParameter count\nSteps/second\nInception V3\n23,626,728\nXception\n22,855,952\nIn table 3 we compare the size and speed of Inception\nFigure 8. Training proﬁle on JFT, with fully-connected layers\nV3 and Xception. Parameter count is reported on ImageNet\n(1000 classes, no fully-connected layers) and the number of\ntraining steps (gradient updates) per second is reported on\nImageNet with 60 K80 GPUs running synchronous gradient\ndescent. Both architectures have approximately the same\nsize (within 3.5%), and Xception is marginally slower. We\nexpect that engineering optimizations at the level of the\ndepthwise convolution operations can make Xception faster\nthan Inception V3 in the near future. The fact that both\narchitectures have almost the same number of parameters\nindicates that the improvement seen on ImageNet and JFT\ndoes not come from added capacity but rather from a more\nefﬁcient use of the model parameters.\n4.6. Effect of the residual connections\nFigure 9. Training proﬁle with and without residual connections.\nTo quantify the beneﬁts of residual connections in the\nXception architecture, we benchmarked on ImageNet a modiﬁed version of Xception that does not include any residual",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 4
        },
        {
          "page_number": 7,
          "text": "connections. Results are shown in ﬁgure 9. Residual connections are clearly essential in helping with convergence,\nboth in terms of speed and ﬁnal classiﬁcation performance.\nHowever we will note that benchmarking the non-residual\nmodel with the same optimization conﬁguration as the residual model may be uncharitable and that better optimization\nconﬁgurations might yield more competitive results.\nAdditionally, let us note that this result merely shows the\nimportance of residual connections for this speciﬁc architecture, and that residual connections are in no way required\nin order to build models that are stacks of depthwise separable convolutions. We also obtained excellent results with\nnon-residual VGG-style models where all convolution layers\nwere replaced with depthwise separable convolutions (with\na depth multiplier of 1), superior to Inception V3 on JFT at\nequal parameter count.\n4.7. Effect of an intermediate activation after pointwise convolutions\nFigure 10. Training proﬁle with different activations between the\ndepthwise and pointwise operations of the separable convolution\nlayers.\nWe mentioned earlier that the analogy between depthwise separable convolutions and Inception modules suggests\nthat depthwise separable convolutions should potentially include a non-linearity between the depthwise and pointwise\noperations. In the experiments reported so far, no such nonlinearity was included. However we also experimentally\ntested the inclusion of either ReLU or ELU [3] as intermediate non-linearity. Results are reported on ImageNet in ﬁgure\n10, and show that the absence of any non-linearity leads to\nboth faster convergence and better ﬁnal performance.\nThis is a remarkable observation, since Szegedy et al. report the opposite result in [21] for Inception modules. It may\nbe that the depth of the intermediate feature spaces on which\nspatial convolutions are applied is critical to the usefulness\nof the non-linearity: for deep feature spaces (e.g. those\nfound in Inception modules) the non-linearity is helpful, but\nfor shallow ones (e.g. the 1-channel deep feature spaces\nof depthwise separable convolutions) it becomes harmful,\npossibly due to a loss of information.\n5. Future directions\nWe noted earlier the existence of a discrete spectrum between regular convolutions and depthwise separable convolutions, parametrized by the number of independent channelspace segments used for performing spatial convolutions. Inception modules are one point on this spectrum. We showed\nin our empirical evaluation that the extreme formulation of\nan Inception module, the depthwise separable convolution,\nmay have advantages over regular a regular Inception module. However, there is no reason to believe that depthwise\nseparable convolutions are optimal. It may be that intermediate points on the spectrum, lying between regular Inception\nmodules and depthwise separable convolutions, hold further\nadvantages. This question is left for future investigation.\n6. Conclusions\nWe showed how convolutions and depthwise separable\nconvolutions lie at both extremes of a discrete spectrum,\nwith Inception modules being an intermediate point in between. This observation has led to us to propose replacing\nInception modules with depthwise separable convolutions in\nneural computer vision architectures. We presented a novel\narchitecture based on this idea, named Xception, which has\na similar parameter count as Inception V3. Compared to\nInception V3, Xception shows small gains in classiﬁcation\nperformance on the ImageNet dataset and large gains on the\nJFT dataset. We expect depthwise separable convolutions\nto become a cornerstone of convolutional neural network\narchitecture design in the future, since they offer similar\nproperties as Inception modules, yet are as easy to use as\nregular convolution layers.\nReferences\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,\nR. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,\nR. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,\nJ. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. Warden,\nM. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorﬂow.org.\n[2] F. Chollet. Keras.  2015.\n[3] D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and\naccurate deep network learning by exponential linear units\n(elus). arXiv preprint arXiv:1511.07289, 2015.",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": true,
          "image_count": 1
        },
        {
          "page_number": 8,
          "text": "[4] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition.\narXiv preprint arXiv:1512.03385,\n2015.\n[5] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge\nin a neural network, 2015.\n[6] A. Howard. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. Forthcoming.\n[7] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift.\nIn Proceedings of The 32nd International Conference on\nMachine Learning, pages 448-456, 2015.\n[8] J. Jin, A. Dundar, and E. Culurciello. Flattened convolutional\nneural networks for feedforward acceleration. arXiv preprint\narXiv:1412.5474, 2014.\n[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\nclassiﬁcation with deep convolutional neural networks. In\nAdvances in neural information processing systems, pages\n1097-1105, 2012.\n[10] Y. LeCun, L. Jackel, L. Bottou, C. Cortes, J. S. Denker,\nH. Drucker, I. Guyon, U. Muller, E. Sackinger, P. Simard,\net al. Learning algorithms for classiﬁcation: A comparison on\nhandwritten digit recognition. Neural networks: the statistical\nmechanics perspective, 261:276, 1995.\n[11] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv\npreprint arXiv:1312.4400, 2013.\n[12] F. Mamalet and C. Garcia. Simplifying ConvNets for Fast\nLearning. In International Conference on Artiﬁcial Neural\nNetworks (ICANN 2012), pages 58-65. Springer, 2012.\n[13] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM J. Control Optim.,\n30(4):838-855, July 1992.\n[14] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. 2014.\n[15] L. Sifre. Rigid-motion scattering for image classiﬁcation,\n2014. Ph.D. thesis.\n[16] L. Sifre and S. Mallat. Rotation, scaling and deformation\ninvariant scattering for texture discrimination. In 2013 IEEE\nConference on Computer Vision and Pattern Recognition,\nPortland, OR, USA, June 23-28, 2013, pages 1233-1240,\n2013.\n[17] N. Silberman and S. Guadarrama. Tf-slim, 2016.\n[18] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014.\n[19] C. Szegedy, S. Ioffe, and V. Vanhoucke.\nInception-v4,\ninception-resnet and the impact of residual connections on\nlearning. arXiv preprint arXiv:1602.07261, 2016.\n[20] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper\nwith convolutions. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 1-9, 2015.\n[21] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.\nRethinking the inception architecture for computer vision.\narXiv preprint arXiv:1512.00567, 2015.\n[22] T. Tieleman and G. Hinton. Divide the gradient by a running average of its recent magnitude. COURSERA: Neural\nNetworks for Machine Learning, 4, 2012. Accessed: 201511-05.\n[23] V. Vanhoucke. Learning visual representations at scale. ICLR,\n2014.\n[24] M. Wang, B. Liu, and H. Foroosh. Factorized convolutional\nneural networks. arXiv preprint arXiv:1608.04337, 2016.\n[25] M. D. Zeiler and R. Fergus. Visualizing and understanding\nconvolutional networks. In Computer Vision-ECCV 2014,\npages 818-833. Springer, 2014.",
          "method": "pymupdf",
          "page_width": 612.0,
          "page_height": 792.0,
          "has_images": false,
          "image_count": 0
        }
      ],
      "sections": {
        "title": "(elus). arXiv preprint arXiv:1511.07289, 2015.\n\n[4] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition.\narXiv preprint arXiv:1512.03385,\n2015.\n[5] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge\nin a neural network, 2015.\n[6] A. Howard. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. Forthcoming.\n[7] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift.\nIn Proceedings of The 32nd International Conference on\nMachine Learning, pages 448-456, 2015.\n[8] J. Jin, A. Dundar, and E. Culurciello. Flattened convolutional\nneural networks for feedforward acceleration. arXiv preprint\narXiv:1412.5474, 2014.\n[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\nclassiﬁcation with deep convolutional neural networks. In\nAdvances in neural information processing systems, pages\n1097-1105, 2012.\n[10] Y. LeCun, L. Jackel, L. Bottou, C. Cortes, J. S. Denker,\nH. Drucker, I. Guyon, U. Muller, E. Sackinger, P. Simard,\net al. Learning algorithms for classiﬁcation: A comparison on\nhandwritten digit recognition. Neural networks: the statistical\nmechanics perspective, 261:276, 1995.\n[11] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv\npreprint arXiv:1312.4400, 2013.\n[12] F. Mamalet and C. Garcia. Simplifying ConvNets for Fast\nLearning. In International Conference on Artiﬁcial Neural\nNetworks (ICANN 2012), pages 58-65. Springer, 2012.\n[13] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM J. Control Optim.,\n30(4):838-855, July 1992.\n[14] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. 2014.\n[15] L. Sifre. Rigid-motion scattering for image classiﬁcation,\n2014. Ph.D. thesis.\n[16] L. Sifre and S. Mallat. Rotation, scaling and deformation\ninvariant scattering for texture discrimination. In 2013 IEEE\nConference on Computer Vision and Pattern Recognition,\nPortland, OR, USA, June 23-28, 2013, pages 1233-1240,\n2013.\n[17] N. Silberman and S. Guadarrama. Tf-slim, 2016.\n[18] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014.\n[19] C. Szegedy, S. Ioffe, and V. Vanhoucke.\nInception-v4,\ninception-resnet and the impact of residual connections on\nlearning. arXiv preprint arXiv:1602.07261, 2016.\n[20] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper\nwith convolutions. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 1-9, 2015.\n[21] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.\nRethinking the inception architecture for computer vision.\narXiv preprint arXiv:1512.00567, 2015.\n[22] T. Tieleman and G. Hinton. Divide the gradient by a running average of its recent magnitude. COURSERA: Neural\nNetworks for Machine Learning, 4, 2012. Accessed: 201511-05.\n[23] V. Vanhoucke. Learning visual representations at scale. ICLR,\n2014.\n[24] M. Wang, B. Liu, and H. Foroosh. Factorized convolutional\nneural networks. arXiv preprint arXiv:1608.04337, 2016.\n[25] M. D. Zeiler and R. Fergus. Visualizing and understanding\nconvolutional networks. In Computer Vision-ECCV 2014,\npages 818-833. Springer, 2014.",
        "abstract": "Xception: Deep Learning with Depthwise Separable Convolutions\nFranc¸ois Chollet\nGoogle, Inc.",
        "introduction": "Convolutional neural networks have emerged as the master algorithm in computer vision in recent years, and developing recipes for designing them has been a subject of\nconsiderable attention. The history of convolutional neural\nnetwork design started with LeNet-style models [10], which\nwere simple stacks of convolutions for feature extraction\nand max-pooling operations for spatial sub-sampling. In\n2012, these ideas were reﬁned into the AlexNet architecture [9], where convolution operations were being repeated\nmultiple times in-between max-pooling operations, allowing\nthe network to learn richer features at every spatial scale.\nWhat followed was a trend to make this style of network\nincreasingly deeper, mostly driven by the yearly ILSVRC\ncompetition; ﬁrst with Zeiler and Fergus in 2013 [25] and\nthen with the VGG architecture in 2014 [18].\nAt this point a new style of network emerged, the Inception architecture, introduced by Szegedy et al. in 2014 [20]\nas GoogLeNet (Inception V1), later reﬁned as Inception V2\n[7], Inception V3 [21], and most recently Inception-ResNet\n[19]. Inception itself was inspired by the earlier NetworkIn-Network architecture [11]. Since its ﬁrst introduction,\nInception has been one of the best performing family of\nmodels on the ImageNet dataset [14], as well as internal\ndatasets in use at Google, in particular JFT [5].\nThe fundamental building block of Inception-style models is the Inception module, of which several different versions exist. In ﬁgure 1 we show the canonical form of an\nInception module, as found in the Inception V3 architecture. An Inception model can be understood as a stack of\nsuch modules. This is a departure from earlier VGG-style\nnetworks which were stacks of simple convolution layers.\nWhile Inception modules are conceptually similar to convolutions (they are convolutional feature extractors), they\nempirically appear to be capable of learning richer representations with less parameters. How do they work, and\nhow do they differ from regular convolutions? What design\nstrategies come after Inception?\n1.1. The Inception hypothesis\nA convolution layer attempts to learn ﬁlters in a 3D space,\nwith 2 spatial dimensions (width and height) and a channel dimension; thus a single convolution kernel is tasked\nwith simultaneously mapping cross-channel correlations and\nspatial correlations.\nThis idea behind the Inception module is to make this\nprocess easier and more efﬁcient by explicitly factoring it\ninto a series of operations that would independently look at\ncross-channel correlations and at spatial correlations. More\nprecisely, the typical Inception module ﬁrst looks at crosschannel correlations via a set of 1x1 convolutions, mapping\nthe input data into 3 or 4 separate spaces that are smaller than\nthe original input space, and then maps all correlations in\nthese smaller 3D spaces, via regular 3x3 or 5x5 convolutions.\nThis is illustrated in ﬁgure 1. In effect, the fundamental hypothesis behind Inception is that cross-channel correlations\nand spatial correlations are sufﬁciently decoupled that it is\npreferable not to map them jointly 1.\n1A variant of the process is to independently look at width-wise correarXiv:1610.02357v3 [cs.CV] 4 Apr 2017\n\nConsider a simpliﬁed version of an Inception module that\nonly uses one size of convolution (e.g. 3x3) and does not\ninclude an average pooling tower (ﬁgure 2). This Inception module can be reformulated as a large 1x1 convolution\nfollowed by spatial convolutions that would operate on nonoverlapping segments of the output channels (ﬁgure 3). This\nobservation naturally raises the question: what is the effect of the number of segments in the partition (and their\nsize)? Would it be reasonable to make a much stronger\nhypothesis than the Inception hypothesis, and assume that\ncross-channel correlations and spatial correlations can be\nmapped completely separately?\nFigure 1. A canonical Inception module (Inception V3).\nFigure 2. A simpliﬁed Inception module.\n1.2. The continuum between convolutions and separable convolutions\nAn “extreme” version of an Inception module, based on\nthis stronger hypothesis, would ﬁrst use a 1x1 convolution to\nmap cross-channel correlations, and would then separately\nmap the spatial correlations of every output channel. This\nis shown in ﬁgure 4. We remark that this extreme form of\nan Inception module is almost identical to a depthwise separable convolution, an operation that has been used in neural\nlations and height-wise correlations. This is implemented by some of the\nmodules found in Inception V3, which alternate 7x1 and 1x7 convolutions.\nThe use of such spatially separable convolutions has a long history in image processing and has been used in some convolutional neural network\nimplementations since at least 2012 (possibly earlier).\nFigure 3. A strictly equivalent reformulation of the simpliﬁed Inception module.\nFigure 4. An “extreme” version of our Inception module, with one\nspatial convolution per output channel of the 1x1 convolution.\nnetwork design as early as 2014 [15] and has become more\npopular since its inclusion in the TensorFlow framework [1]\nin 2016.\nA depthwise separable convolution, commonly called\n“separable convolution” in deep learning frameworks such as\nTensorFlow and Keras, consists in a depthwise convolution,\ni.e. a spatial convolution performed independently over each\nchannel of an input, followed by a pointwise convolution,\ni.e. a 1x1 convolution, projecting the channels output by the\ndepthwise convolution onto a new channel space. This is\nnot to be confused with a spatially separable convolution,\nwhich is also commonly called “separable convolution” in\nthe image processing community.\nTwo minor differences between and “extreme” version of\nan Inception module and a depthwise separable convolution\nwould be:\n• The order of the operations: depthwise separable convolutions as usually implemented (e.g. in TensorFlow)\nperform ﬁrst channel-wise spatial convolution and then\nperform 1x1 convolution, whereas Inception performs\nthe 1x1 convolution ﬁrst.\n• The presence or absence of a non-linearity after the\nﬁrst operation. In Inception, both operations are followed by a ReLU non-linearity, however depthwise\n\nseparable convolutions are usually implemented without non-linearities.\nWe argue that the ﬁrst difference is unimportant, in particular because these operations are meant to be used in a\nstacked setting. The second difference might matter, and we\ninvestigate it in the experimental section (in particular see\nﬁgure 10).\nWe also note that other intermediate formulations of Inception modules that lie in between regular Inception modules and depthwise separable convolutions are also possible:\nin effect, there is a discrete spectrum between regular convolutions and depthwise separable convolutions, parametrized\nby the number of independent channel-space segments used\nfor performing spatial convolutions. A regular convolution\n(preceded by a 1x1 convolution), at one extreme of this\nspectrum, corresponds to the single-segment case; a depthwise separable convolution corresponds to the other extreme\nwhere there is one segment per channel; Inception modules\nlie in between, dividing a few hundreds of channels into 3\nor 4 segments. The properties of such intermediate modules\nappear not to have been explored yet.\nHaving made these observations, we suggest that it may\nbe possible to improve upon the Inception family of architectures by replacing Inception modules with depthwise separable convolutions, i.e. by building models that would be\nstacks of depthwise separable convolutions. This is made\npractical by the efﬁcient depthwise convolution implementation available in TensorFlow. In what follows, we present a\nconvolutional neural network architecture based on this idea,\nwith a similar number of parameters as Inception V3, and\nwe evaluate its performance against Inception V3 on two\nlarge-scale image classiﬁcation task.\n2. Prior work\nThe present work relies heavily on prior efforts in the\nfollowing areas:\n• Convolutional neural networks [10, 9, 25], in particular\nthe VGG-16 architecture [18], which is schematically\nsimilar to our proposed architecture in a few respects.\n• The Inception architecture family of convolutional neural networks [20, 7, 21, 19], which ﬁrst demonstrated\nthe advantages of factoring convolutions into multiple\nbranches operating successively on channels and then\non space.\n• Depthwise separable convolutions, which our proposed\narchitecture is entirely based upon. While the use of spatially separable convolutions in neural networks has a\nlong history, going back to at least 2012 [12] (but likely\neven earlier), the depthwise version is more recent. Laurent Sifre developed depthwise separable convolutions\nduring an internship at Google Brain in 2013, and used\nthem in AlexNet to obtain small gains in accuracy and\nlarge gains in convergence speed, as well as a signiﬁcant\nreduction in model size. An overview of his work was\nﬁrst made public in a presentation at ICLR 2014 [23].\nDetailed experimental results are reported in Sifre’s thesis, section 6.2 [15]. This initial work on depthwise separable convolutions was inspired by prior research from\nSifre and Mallat on transformation-invariant scattering\n[16, 15]. Later, a depthwise separable convolution was\nused as the ﬁrst layer of Inception V1 and Inception\nV2 [20, 7]. Within Google, Andrew Howard [6] has\nintroduced efﬁcient mobile models called MobileNets\nusing depthwise separable convolutions. Jin et al. in\n2014 [8] and Wang et al. in 2016 [24] also did related\nwork aiming at reducing the size and computational\ncost of convolutional neural networks using separable\nconvolutions. Additionally, our work is only possible\ndue to the inclusion of an efﬁcient implementation of\ndepthwise separable convolutions in the TensorFlow\nframework [1].\n• Residual connections, introduced by He et al. in [4],\nwhich our proposed architecture uses extensively.\n3. The Xception architecture\nWe propose a convolutional neural network architecture\nbased entirely on depthwise separable convolution layers.\nIn effect, we make the following hypothesis: that the mapping of cross-channels correlations and spatial correlations\nin the feature maps of convolutional neural networks can be\nentirely decoupled. Because this hypothesis is a stronger version of the hypothesis underlying the Inception architecture,\nwe name our proposed architecture Xception, which stands\nfor “Extreme Inception”.\nA complete description of the speciﬁcations of the network is given in ﬁgure 5. The Xception architecture has\n36 convolutional layers forming the feature extraction base\nof the network. In our experimental evaluation we will exclusively investigate image classiﬁcation and therefore our\nconvolutional base will be followed by a logistic regression\nlayer. Optionally one may insert fully-connected layers before the logistic regression layer, which is explored in the\nexperimental evaluation section (in particular, see ﬁgures\n7 and 8). The 36 convolutional layers are structured into\n14 modules, all of which have linear residual connections\naround them, except for the ﬁrst and last modules.\nIn short, the Xception architecture is a linear stack of\ndepthwise separable convolution layers with residual connections. This makes the architecture very easy to deﬁne\nand modify; it takes only 30 to 40 lines of code using a highlevel library such as Keras [2] or TensorFlow-Slim [17], not\nunlike an architecture such as VGG-16 [18], but rather un-\n\nlike architectures such as Inception V2 or V3 which are far\nmore complex to deﬁne. An open-source implementation of\nXception using Keras and TensorFlow is provided as part of\nthe Keras Applications module2, under the MIT license.\n4. Experimental evaluation\nWe choose to compare Xception to the Inception V3 architecture, due to their similarity of scale: Xception and\nInception V3 have nearly the same number of parameters\n(table 3), and thus any performance gap could not be attributed to a difference in network capacity. We conduct\nour comparison on two image classiﬁcation tasks: one is\nthe well-known 1000-class single-label classiﬁcation task on\nthe ImageNet dataset [14], and the other is a 17,000-class\nmulti-label classiﬁcation task on the large-scale JFT dataset.\n4.1. The JFT dataset\nJFT is an internal Google dataset for large-scale image\nclassiﬁcation dataset, ﬁrst introduced by Hinton et al. in [5],\nwhich comprises over 350 million high-resolution images\nannotated with labels from a set of 17,000 classes. To evaluate the performance of a model trained on JFT, we use an\nauxiliary dataset, FastEval14k.\nFastEval14k is a dataset of 14,000 images with dense\nannotations from about 6,000 classes (36.5 labels per image on average). On this dataset we evaluate performance\nusing Mean Average Precision for top 100 predictions\n and we weight the contribution of each class\nto  with a score estimating how common (and\ntherefore important) the class is among social media images.\nThis evaluation procedure is meant to capture performance\non frequently occurring labels from social media, which is\ncrucial for production models at Google.\n4.2. Optimization conﬁguration\nA different optimization conﬁguration was used for ImageNet and JFT:\n• On ImageNet:\n- Optimizer: SGD\n- Momentum: 0.9\n- Initial learning rate: 0.045\n- Learning rate decay: decay of rate 0.94 every 2\nepochs\n• On JFT:\n- Optimizer: RMSprop [22]\n- Momentum: 0.9\n- Initial learning rate: 0.001\n2\n- Learning rate decay: decay of rate 0.9 every\n3,000,000 samples\nFor both datasets, the same exact same optimization conﬁguration was used for both Xception and Inception V3.\nNote that this conﬁguration was tuned for best performance\nwith Inception V3; we did not attempt to tune optimization\nhyperparameters for Xception. Since the networks have different training proﬁles (ﬁgure 6), this may be suboptimal, especially on the ImageNet dataset, on which the optimization\nconﬁguration used had been carefully tuned for Inception\nV3.\nAdditionally, all models were evaluated using Polyak\naveraging [13] at inference time.\n4.3. Regularization conﬁguration\n• Weight decay: The Inception V3 model uses a weight\ndecay (L2 regularization) rate of 4e − 5, which has\nbeen carefully tuned for performance on ImageNet. We\nfound this rate to be quite suboptimal for Xception\nand instead settled for 1e − 5. We did not perform\nan extensive search for the optimal weight decay rate.\nThe same weight decay rates were used both for the\nImageNet experiments and the JFT experiments.\n• Dropout: For the ImageNet experiments, both models\ninclude a dropout layer of rate 0.5 before the logistic\nregression layer. For the JFT experiments, no dropout\nwas included due to the large size of the dataset which\nmade overﬁtting unlikely in any reasonable amount of\ntime.\n• Auxiliary loss tower: The Inception V3 architecture\nmay optionally include an auxiliary tower which backpropagates the classiﬁcation loss earlier in the network,\nserving as an additional regularization mechanism. For\nsimplicity, we choose not to include this auxiliary tower\nin any of our models.\n4.4. Training infrastructure\nAll networks were implemented using the TensorFlow\nframework [1] and trained on 60 NVIDIA K80 GPUs each.\nFor the ImageNet experiments, we used data parallelism\nwith synchronous gradient descent to achieve the best classiﬁcation performance, while for JFT we used asynchronous\ngradient descent so as to speed up training. The ImageNet\nexperiments took approximately 3 days each, while the JFT\nexperiments took over one month each. The JFT models\nwere not trained to full convergence, which would have\ntaken over three month per experiment.\n\nFigure 5. The Xception architecture: the data ﬁrst goes through the entry ﬂow, then through the middle ﬂow which is repeated eight times,\nand ﬁnally through the exit ﬂow. Note that all Convolution and SeparableConvolution layers are followed by batch normalization [7] (not\nincluded in the diagram). All SeparableConvolution layers use a depth multiplier of 1 (no depth expansion).\n4.5. Comparison with Inception V3\n4.5.1\nClassiﬁcation performance\nAll evaluations were run with a single crop of the inputs\nimages and a single model. ImageNet results are reported\non the validation set rather than the test set (i.e. on the\nnon-blacklisted images from the validation set of ILSVRC\n2012). JFT results are reported after 30 million iterations\n(one month of training) rather than after full convergence.\nResults are provided in table 1 and table 2, as well as ﬁgure\n6, ﬁgure 7, ﬁgure 8. On JFT, we tested both versions of our\nnetworks that did not include any fully-connected layers, and\nversions that included two fully-connected layers of 4096\nunits each before the logistic regression layer.\nOn ImageNet, Xception shows marginally better results\nthan Inception V3. On JFT, Xception shows a 4.3% relative improvement on the FastEval14k  metric.\nWe also note that Xception outperforms ImageNet results\nreported by He et al. for ResNet-50, ResNet-101 and ResNet152 [4].\nTable 1. Classiﬁcation performance comparison on ImageNet (single crop, single model). VGG-16 and ResNet-152 numbers are\nonly included as a reminder. The version of Inception V3 being\nbenchmarked does not include the auxiliary tower.\nTop-1 accuracy\nTop-5 accuracy\nVGG-16\n0.715\n0.901\nResNet-152\n0.770\n0.933\nInception V3\n0.782\n0.941\nXception\n0.790\n0.945\nThe Xception architecture shows a much larger performance improvement on the JFT dataset compared to the\nImageNet dataset. We believe this may be due to the fact\nthat Inception V3 was developed with a focus on ImageNet\nand may thus be by design over-ﬁt to this speciﬁc task. On\nthe other hand, neither architecture was tuned for JFT. It is\nlikely that a search for better hyperparameters for Xception\non ImageNet (in particular optimization parameters and reg-\n\nTable 2. Classiﬁcation performance comparison on JFT (single\ncrop, single model).\nFastEval14k \nInception V3 - no FC layers\n6.36\nXception - no FC layers\n6.70\nInception V3 with FC layers\n6.50\nXception with FC layers\n6.78\nFigure 6. Training proﬁle on ImageNet\nFigure 7. Training proﬁle on JFT, without fully-connected layers\nularization parameters) would yield signiﬁcant additional\nimprovement.\n4.5.2\nSize and speed\nTable 3. Size and training speed comparison.\nParameter count\nSteps/second\nInception V3\n23,626,728\nXception\n22,855,952\nIn table 3 we compare the size and speed of Inception\nFigure 8. Training proﬁle on JFT, with fully-connected layers\nV3 and Xception. Parameter count is reported on ImageNet\n(1000 classes, no fully-connected layers) and the number of\ntraining steps (gradient updates) per second is reported on\nImageNet with 60 K80 GPUs running synchronous gradient\ndescent. Both architectures have approximately the same\nsize (within 3.5%), and Xception is marginally slower. We\nexpect that engineering optimizations at the level of the\ndepthwise convolution operations can make Xception faster\nthan Inception V3 in the near future. The fact that both\narchitectures have almost the same number of parameters\nindicates that the improvement seen on ImageNet and JFT\ndoes not come from added capacity but rather from a more\nefﬁcient use of the model parameters.\n4.6. Effect of the residual connections\nFigure 9. Training proﬁle with and without residual connections.\nTo quantify the beneﬁts of residual connections in the\nXception architecture, we benchmarked on ImageNet a modiﬁed version of Xception that does not include any residual\n\nconnections. Results are shown in ﬁgure 9. Residual connections are clearly essential in helping with convergence,\nboth in terms of speed and ﬁnal classiﬁcation performance.\nHowever we will note that benchmarking the non-residual\nmodel with the same optimization conﬁguration as the residual model may be uncharitable and that better optimization\nconﬁgurations might yield more competitive results.\nAdditionally, let us note that this result merely shows the\nimportance of residual connections for this speciﬁc architecture, and that residual connections are in no way required\nin order to build models that are stacks of depthwise separable convolutions. We also obtained excellent results with\nnon-residual VGG-style models where all convolution layers\nwere replaced with depthwise separable convolutions (with\na depth multiplier of 1), superior to Inception V3 on JFT at\nequal parameter count.\n4.7. Effect of an intermediate activation after pointwise convolutions\nFigure 10. Training proﬁle with different activations between the\ndepthwise and pointwise operations of the separable convolution\nlayers.\nWe mentioned earlier that the analogy between depthwise separable convolutions and Inception modules suggests\nthat depthwise separable convolutions should potentially include a non-linearity between the depthwise and pointwise\noperations. In the experiments reported so far, no such nonlinearity was included. However we also experimentally\ntested the inclusion of either ReLU or ELU [3] as intermediate non-linearity. Results are reported on ImageNet in ﬁgure\n10, and show that the absence of any non-linearity leads to\nboth faster convergence and better ﬁnal performance.\nThis is a remarkable observation, since Szegedy et al. report the opposite result in [21] for Inception modules. It may\nbe that the depth of the intermediate feature spaces on which\nspatial convolutions are applied is critical to the usefulness\nof the non-linearity: for deep feature spaces (e.g. those\nfound in Inception modules) the non-linearity is helpful, but\nfor shallow ones (e.g. the 1-channel deep feature spaces\nof depthwise separable convolutions) it becomes harmful,\npossibly due to a loss of information.\n5. Future directions\nWe noted earlier the existence of a discrete spectrum between regular convolutions and depthwise separable convolutions, parametrized by the number of independent channelspace segments used for performing spatial convolutions. Inception modules are one point on this spectrum. We showed\nin our empirical evaluation that the extreme formulation of\nan Inception module, the depthwise separable convolution,\nmay have advantages over regular a regular Inception module. However, there is no reason to believe that depthwise\nseparable convolutions are optimal. It may be that intermediate points on the spectrum, lying between regular Inception\nmodules and depthwise separable convolutions, hold further\nadvantages. This question is left for future investigation.",
        "literature_review": "",
        "methodology": "",
        "results": "",
        "discussion": "",
        "conclusion": "We showed how convolutions and depthwise separable\nconvolutions lie at both extremes of a discrete spectrum,\nwith Inception modules being an intermediate point in between. This observation has led to us to propose replacing\nInception modules with depthwise separable convolutions in\nneural computer vision architectures. We presented a novel\narchitecture based on this idea, named Xception, which has\na similar parameter count as Inception V3. Compared to\nInception V3, Xception shows small gains in classiﬁcation\nperformance on the ImageNet dataset and large gains on the\nJFT dataset. We expect depthwise separable convolutions\nto become a cornerstone of convolutional neural network\narchitecture design in the future, since they offer similar\nproperties as Inception modules, yet are as easy to use as\nregular convolution layers.",
        "references": "[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,\nR. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,\nR. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,\nJ. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. Warden,\nM. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorﬂow.org.\n[2] F. Chollet. Keras.  2015.\n[3] D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and",
        "other": ""
      },
      "saved_path": "C:\\Users\\ASUS\\Documents\\GitHub\\AI-System-to-automatically-Review-and-Summarize-Research-Papers\\data\\extracted_text\\Xception_Deep_Learning_with_Depthwise_Separable_Co_extracted.json"
    }
  ]
}