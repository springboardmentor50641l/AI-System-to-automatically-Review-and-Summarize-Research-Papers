{
  "file_name": "PointNet_Deep_Learning_on_Point_Sets_for_3D_Classi_20260129.pdf",
  "file_path": "C:\\Users\\ASUS\\Documents\\GitHub\\AI-System-to-automatically-Review-and-Summarize-Research-Papers\\data\\papers\\PointNet_Deep_Learning_on_Point_Sets_for_3D_Classi_20260129.pdf",
  "file_size": 9083131,
  "extraction_date": "2026-01-29T22:01:09.545951",
  "pdf_type": "text_based",
  "total_pages": 19,
  "total_chars": 67708,
  "total_words": 11324,
  "extraction_method": "pymupdf",
  "pages": [
    {
      "page_number": 1,
      "text": "PointNet: Deep Learning on Point Sets for 3D Classiﬁcation and Segmentation\nCharles R. Qi*\nHao Su*\nKaichun Mo\nLeonidas J. Guibas\nStanford University\nAbstract\nPoint cloud is an important type of geometric data\nstructure.\nDue to its irregular format, most researchers\ntransform such data to regular 3D voxel grids or collections\nof images.\nThis, however, renders data unnecessarily\nvoluminous and causes issues. In this paper, we design a\nnovel type of neural network that directly consumes point\nclouds, which well respects the permutation invariance of\npoints in the input.\nOur network, named PointNet, provides a uniﬁed architecture for applications ranging from\nobject classiﬁcation, part segmentation, to scene semantic\nparsing. Though simple, PointNet is highly efﬁcient and\neffective.\nEmpirically, it shows strong performance on\npar or even better than state of the art.\nTheoretically,\nwe provide analysis towards understanding of what the\nnetwork has learnt and why the network is robust with\nrespect to input perturbation and corruption.\n1. Introduction\nIn this paper we explore deep learning architectures\ncapable of reasoning about 3D geometric data such as\npoint clouds or meshes. Typical convolutional architectures\nrequire highly regular input data formats, like those of\nimage grids or 3D voxels, in order to perform weight\nsharing and other kernel optimizations. Since point clouds\nor meshes are not in a regular format, most researchers\ntypically transform such data to regular 3D voxel grids or\ncollections of images (e.g, views) before feeding them to\na deep net architecture. This data representation transformation, however, renders the resulting data unnecessarily\nvoluminous - while also introducing quantization artifacts\nthat can obscure natural invariances of the data.\nFor this reason we focus on a different input representation for 3D geometry using simply point clouds\n- and name our resulting deep nets PointNets.\nPoint\nclouds are simple and uniﬁed structures that avoid the\ncombinatorial irregularities and complexities of meshes,\nand thus are easier to learn from. The PointNet, however,\n* indicates equal contributions.\nmug?\ntable?\ncar?\nClassification\nPart Segmentation\nPointNet\nSemantic Segmentation\nInput Point Cloud (point set representation)\nFigure 1. Applications of PointNet. We propose a novel deep net\narchitecture that consumes raw point cloud (set of points) without\nvoxelization or rendering. It is a uniﬁed architecture that learns\nboth global and local point features, providing a simple, efﬁcient\nand effective approach for a number of 3D recognition tasks.\nstill has to respect the fact that a point cloud is just a\nset of points and therefore invariant to permutations of its\nmembers, necessitating certain symmetrizations in the net\ncomputation. Further invariances to rigid motions also need\nto be considered.\nOur PointNet is a uniﬁed architecture that directly\ntakes point clouds as input and outputs either class labels\nfor the entire input or per point segment/part labels for\neach point of the input.\nThe basic architecture of our\nnetwork is surprisingly simple as in the initial stages each\npoint is processed identically and independently.\nIn the\nbasic setting each point is represented by just its three\ncoordinates (x, y, z). Additional dimensions may be added\nby computing normals and other local or global features.\nKey to our approach is the use of a single symmetric\nfunction, max pooling.\nEffectively the network learns a\nset of optimization functions/criteria that select interesting\nor informative points of the point cloud and encode the\nreason for their selection. The ﬁnal fully connected layers\nof the network aggregate these learnt optimal values into the\nglobal descriptor for the entire shape as mentioned above\n(shape classiﬁcation) or are used to predict per point labels\n(shape segmentation).\nOur input format is easy to apply rigid or afﬁne transformations to, as each point transforms independently. Thus\nwe can add a data-dependent spatial transformer network\nthat attempts to canonicalize the data before the PointNet\nprocesses them, so as to further improve the results.\narXiv:1612.00593v2 [cs.CV] 10 Apr 2017",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 7
    },
    {
      "page_number": 2,
      "text": "We provide both a theoretical analysis and an experimental evaluation of our approach.\nWe show that\nour network can approximate any set function that is\ncontinuous. More interestingly, it turns out that our network\nlearns to summarize an input point cloud by a sparse set of\nkey points, which roughly corresponds to the skeleton of\nobjects according to visualization. The theoretical analysis\nprovides an understanding why our PointNet is highly\nrobust to small perturbation of input points as well as\nto corruption through point insertion (outliers) or deletion\n(missing data).\nOn a number of benchmark datasets ranging from shape\nclassiﬁcation, part segmentation to scene segmentation,\nwe experimentally compare our PointNet with state-ofthe-art approaches based upon multi-view and volumetric\nrepresentations. Under a uniﬁed architecture, not only is\nour PointNet much faster in speed, but it also exhibits strong\nperformance on par or even better than state of the art.\nThe key contributions of our work are as follows:\n• We design a novel deep net architecture suitable for\nconsuming unordered point sets in 3D;\n• We show how such a net can be trained to perform\n3D shape classiﬁcation, shape part segmentation and\nscene semantic parsing tasks;\n• We provide thorough empirical and theoretical analysis on the stability and efﬁciency of our method;\n• We illustrate the 3D features computed by the selected\nneurons in the net and develop intuitive explanations\nfor its performance.\nThe problem of processing unordered sets by neural nets\nis a very general and fundamental problem - we expect that\nour ideas can be transferred to other domains as well.\n2. Related Work\nPoint Cloud Features\nMost existing features for point\ncloud are handcrafted towards speciﬁc tasks. Point features\noften encode certain statistical properties of points and are\ndesigned to be invariant to certain transformations, which\nare typically classiﬁed as intrinsic [2, 24, 3] or extrinsic\n[20, 19, 14, 10, 5]. They can also be categorized as local\nfeatures and global features. For a speciﬁc task, it is not\ntrivial to ﬁnd the optimal feature combination.\nDeep Learning on 3D Data\n3D data has multiple popular\nrepresentations, leading to various approaches for learning.\nVolumetric CNNs: [28, 17, 18] are the pioneers applying\n3D convolutional neural networks on voxelized shapes.\nHowever, volumetric representation is constrained by its\nresolution due to data sparsity and computation cost of\n3D convolution.\nFPNN [13] and Vote3D [26] proposed\nspecial methods to deal with the sparsity problem; however,\ntheir operations are still on sparse volumes, it’s challenging\nfor them to process very large point clouds.\nMultiview\nCNNs: [23, 18] have tried to render 3D point cloud or\nshapes into 2D images and then apply 2D conv nets to\nclassify them.\nWith well engineered image CNNs, this\nline of methods have achieved dominating performance on\nshape classiﬁcation and retrieval tasks [21]. However, it’s\nnontrivial to extend them to scene understanding or other\n3D tasks such as point classiﬁcation and shape completion.\nSpectral CNNs: Some latest works [4, 16] use spectral\nCNNs on meshes. However, these methods are currently\nconstrained on manifold meshes such as organic objects\nand it’s not obvious how to extend them to non-isometric\nshapes such as furniture.\nFeature-based DNNs: [6, 8]\nﬁrstly convert the 3D data into a vector, by extracting\ntraditional shape features and then use a fully connected net\nto classify the shape. We think they are constrained by the\nrepresentation power of the features extracted.\nDeep Learning on Unordered Sets\nFrom a data structure\npoint of view, a point cloud is an unordered set of vectors.\nWhile most works in deep learning focus on regular input\nrepresentations like sequences (in speech and language\nprocessing), images and volumes (video or 3D data), not\nmuch work has been done in deep learning on point sets.\nOne recent work from Oriol Vinyals et al [25] looks\ninto this problem. They use a read-process-write network\nwith attention mechanism to consume unordered input sets\nand show that their network has the ability to sort numbers.\nHowever, since their work focuses on generic sets and NLP\napplications, there lacks the role of geometry in the sets.\n3. Problem Statement\nWe design a deep learning framework that directly\nconsumes unordered point sets as inputs. A point cloud is\nrepresented as a set of 3D points {Pi| i = 1, ..., n}, where\neach point Pi is a vector of its (x, y, z) coordinate plus extra\nfeature channels such as color, normal etc. For simplicity\nand clarity, unless otherwise noted, we only use the (x, y, z)\ncoordinate as our point’s channels.\nFor the object classiﬁcation task, the input point cloud is\neither directly sampled from a shape or pre-segmented from\na scene point cloud. Our proposed deep network outputs\nk scores for all the k candidate classes.\nFor semantic\nsegmentation, the input can be a single object for part region\nsegmentation, or a sub-volume from a 3D scene for object\nregion segmentation. Our model will output n × m scores\nfor each of the n points and each of the m semantic subcategories.",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": false,
      "image_count": 0
    },
    {
      "page_number": 3,
      "text": "input points\npoint features\noutput scores\nmax\npool\nshared \nshared \nshared \nnx3\nnx3\nnx64\nnx64\nnx1024\nn x 1088\nnx128\nmlp (64,64)\nmlp (64,128,1024)\ninput\ntransform\nfeature\ntransform\nmlp\n(512,256,k)\nglobal feature\nmlp (512,256,128)\nT-Net\nmatrix\nmultiply\n3x3\ntransform\nT-Net\nmatrix\nmultiply\n64x64\ntransform\nshared \nmlp (128,m)\noutput scores\nnxm\nk\nClassification Network\nSegmentation Network\nFigure 2. PointNet Architecture. The classiﬁcation network takes n points as input, applies input and feature transformations, and then\naggregates point features by max pooling. The output is classiﬁcation scores for k classes. The segmentation network is an extension to the\nclassiﬁcation net. It concatenates global and local features and outputs per point scores. “mlp” stands for multi-layer perceptron, numbers\nin bracket are layer sizes. Batchnorm is used for all layers with ReLU. Dropout layers are used for the last mlp in classiﬁcation net.\n4. Deep Learning on Point Sets\nThe architecture of our network (Sec 4.2) is inspired by\nthe properties of point sets in Rn (Sec 4.1).\n4.1. Properties of Point Sets in Rn\nOur input is a subset of points from an Euclidean space.\nIt has three main properties:\n• Unordered.\nUnlike pixel arrays in images or voxel\narrays in volumetric grids, point cloud is a set of points\nwithout speciﬁc order. In other words, a network that\nconsumes N 3D point sets needs to be invariant to N!\npermutations of the input set in data feeding order.\n• Interaction among points. The points are from a space\nwith a distance metric. It means that points are not\nisolated, and neighboring points form a meaningful\nsubset.\nTherefore, the model needs to be able to\ncapture local structures from nearby points, and the\ncombinatorial interactions among local structures.\n• Invariance under transformations.\nAs a geometric\nobject, the learned representation of the point set\nshould be invariant to certain transformations.\nFor\nexample, rotating and translating points all together\nshould not modify the global point cloud category nor\nthe segmentation of the points.\n4.2. PointNet Architecture\nOur full network architecture is visualized in Fig 2,\nwhere the classiﬁcation network and the segmentation\nnetwork share a great portion of structures. Please read the\ncaption of Fig 2 for the pipeline.\nOur network has three key modules: the max pooling\nlayer as a symmetric function to aggregate information from\nall the points, a local and global information combination\nstructure, and two joint alignment networks that align both\ninput points and point features.\nWe will discuss our reason behind these design choices\nin separate paragraphs below.\nSymmetry Function for Unordered Input\nIn order\nto make a model invariant to input permutation, three\nstrategies exist: 1) sort input into a canonical order; 2) treat\nthe input as a sequence to train an RNN, but augment the\ntraining data by all kinds of permutations; 3) use a simple\nsymmetric function to aggregate the information from each\npoint. Here, a symmetric function takes n vectors as input\nand outputs a new vector that is invariant to the input\norder. For example, + and ∗ operators are symmetric binary\nfunctions.\nWhile sorting sounds like a simple solution, in high\ndimensional space there in fact does not exist an ordering\nthat is stable w.r.t.\npoint perturbations in the general\nsense.\nThis can be easily shown by contradiction.\nIf\nsuch an ordering strategy exists, it deﬁnes a bijection map\nbetween a high-dimensional space and a 1d real line. It\nis not hard to see, to require an ordering to be stable w.r.t\npoint perturbations is equivalent to requiring that this map\npreserves spatial proximity as the dimension reduces, a task\nthat cannot be achieved in the general case.\nTherefore,\nsorting does not fully resolve the ordering issue, and it’s\nhard for a network to learn a consistent mapping from\ninput to output as the ordering issue persists. As shown in\nexperiments (Fig 5), we ﬁnd that applying a MLP directly\non the sorted point set performs poorly, though slightly\nbetter than directly processing an unsorted input.\nThe idea to use RNN considers the point set as a\nsequential signal and hopes that by training the RNN",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": false,
      "image_count": 0
    },
    {
      "page_number": 4,
      "text": "with randomly permuted sequences, the RNN will become\ninvariant to input order. However in “OrderMatters” [25]\nthe authors have shown that order does matter and cannot be\ntotally omitted. While RNN has relatively good robustness\nto input ordering for sequences with small length (dozens),\nit’s hard to scale to thousands of input elements, which is\nthe common size for point sets. Empirically, we have also\nshown that model based on RNN does not perform as well\nas our proposed method (Fig 5).\nOur idea is to approximate a general function deﬁned on\na point set by applying a symmetric function on transformed\nelements in the set:\nf({x1, . . . , xn}) ≈ g(h(x1), . . . , h(xn)),\n(1)\nwhere f\n:\n2RN\n→\nR, h\n:\n→\nRK and g\n:\nRK × · · · × RK\n\n\n\nn\n→ R is a symmetric function.\nEmpirically, our basic module is very simple:\nwe\napproximate h by a multi-layer perceptron network and\ng by a composition of a single variable function and a\nmax pooling function.\nThis is found to work well by\nexperiments. Through a collection of h, we can learn a\nnumber of f’s to capture different properties of the set.\nWhile our key module seems simple, it has interesting\nproperties (see Sec 5.3) and can achieve strong performace\n(see Sec 5.1) in a few different applications. Due to the\nsimplicity of our module, we are also able to provide\ntheoretical analysis as in Sec 4.3.\nLocal and Global Information Aggregation\nThe output\nfrom the above section forms a vector [f1, . . . , fK], which\nis a global signature of the input set.\nWe can easily\ntrain a SVM or multi-layer perceptron classiﬁer on the\nshape global features for classiﬁcation.\nHowever, point\nsegmentation requires a combination of local and global\nknowledge. We can achieve this by a simple yet highly\neffective manner.\nOur solution can be seen in Fig 2 (Segmentation Network). After computing the global point cloud feature vector, we feed it back to per point features by concatenating\nthe global feature with each of the point features. Then we\nextract new per point features based on the combined point\nfeatures - this time the per point feature is aware of both the\nlocal and global information.\nWith this modiﬁcation our network is able to predict\nper point quantities that rely on both local geometry and\nglobal semantics. For example we can accurately predict\nper-point normals (ﬁg in supplementary), validating that the\nnetwork is able to summarize information from the point’s\nlocal neighborhood. In experiment session, we also show\nthat our model can achieve state-of-the-art performance on\nshape part segmentation and scene segmentation.\nJoint Alignment Network\nThe semantic labeling of a\npoint cloud has to be invariant if the point cloud undergoes\ncertain geometric transformations, such as rigid transformation. We therefore expect that the learnt representation by\nour point set is invariant to these transformations.\nA natural solution is to align all input set to a canonical\nspace before feature extraction.\nJaderberg et al. [9]\nintroduces the idea of spatial transformer to align 2D\nimages through sampling and interpolation, achieved by a\nspeciﬁcally tailored layer implemented on GPU.\nOur input form of point clouds allows us to achieve this\ngoal in a much simpler way compared with [9]. We do not\nneed to invent any new layers and no alias is introduced as in\nthe image case. We predict an afﬁne transformation matrix\nby a mini-network (T-net in Fig 2) and directly apply this\ntransformation to the coordinates of input points. The mininetwork itself resembles the big network and is composed\nby basic modules of point independent feature extraction,\nmax pooling and fully connected layers. More details about\nthe T-net are in the supplementary.\nThis idea can be further extended to the alignment of\nfeature space, as well. We can insert another alignment network on point features and predict a feature transformation\nmatrix to align features from different input point clouds.\nHowever, transformation matrix in the feature space has\nmuch higher dimension than the spatial transform matrix,\nwhich greatly increases the difﬁculty of optimization. We\ntherefore add a regularization term to our softmax training\nloss. We constrain the feature transformation matrix to be\nclose to orthogonal matrix:\nLreg = ∥I − AAT ∥2\nF ,\n(2)\nwhere A is the feature alignment matrix predicted by a\nmini-network. An orthogonal transformation will not lose\ninformation in the input, thus is desired. We ﬁnd that by\nadding the regularization term, the optimization becomes\nmore stable and our model achieves better performance.\n4.3. Theoretical Analysis\nUniversal approximation\nWe ﬁrst show the universal\napproximation ability of our neural network to continuous\nset functions. By the continuity of set functions, intuitively,\na small perturbation to the input point set should not\ngreatly change the function values, such as classiﬁcation or\nsegmentation scores.\nFormally, let X = {S : S ⊆ [0, 1]m and |S| = n}, f :\nX → R is a continuous set function on X w.r.t to Hausdorff\ndistance dH(·, ·), i.e., ∀ϵ > 0, ∃δ > 0, for any S, S′ ∈ X,\nif dH(S, S′) < δ, then |f(S) − f(S′)| < ϵ. Our theorem\nsays that f can be arbitrarily approximated by our network\ngiven enough neurons at the max pooling layer, i.e., K in\n(1) is sufﬁciently large.",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": false,
      "image_count": 0
    },
    {
      "page_number": 5,
      "text": "Partial Inputs\nComplete Inputs\nairplane\ncar\nchair\nlamp\nguitar\nmotorbike\nmug\ntable\nbag\nrocket\nearphone\nlaptop\ncap\nknife\npistol\nskateboard\nFigure 3. Qualitative results for part segmentation.\nWe\nvisualize the CAD part segmentation results across all 16 object\ncategories. We show both results for partial simulated Kinect scans\n(left block) and complete ShapeNet CAD models (right block).\nTheorem 1. Suppose f\n:\nX\n→\nR is a continuous\nset function w.r.t Hausdorff distance dH(·, ·).\n∀ϵ\n>\n0, ∃ a continuous function h and a symmetric function\ng(x1, . . . , xn) = γ ◦ MAX, such that for any S ∈ X,\nf(S) − γ\n\nxi∈S {h(xi)}\n < ϵ\nwhere x1, . . . , xn is the full list of elements in S ordered\narbitrarily, γ is a continuous function, and MAX is a vector\nmax operator that takes n vectors as input and returns a\nnew vector of the element-wise maximum.\nThe proof to this theorem can be found in our supplementary material. The key idea is that in the worst case the\nnetwork can learn to convert a point cloud into a volumetric\nrepresentation, by partitioning the space into equal-sized\nvoxels. In practice, however, the network learns a much\nsmarter strategy to probe the space, as we shall see in point\nfunction visualizations.\nBottleneck dimension and stability\nTheoretically and\nexperimentally we ﬁnd that the expressiveness of our\nnetwork is strongly affected by the dimension of the max\npooling layer, i.e., K in (1). Here we provide an analysis,\nwhich also reveals properties related to the stability of our\nmodel.\nWe deﬁne u = MAX\nxi∈S {h(xi)} to be the sub-network of f\nwhich maps a point set in [0, 1]m to a K-dimensional vector.\nThe following theorem tells us that small corruptions or\nextra noise points in the input set are not likely to change\nthe output of our network:\nTheorem 2. Suppose u : X\n→ RK such that u =\nxi∈S{h(xi)} and f = γ ◦ u. Then,\n(a) ∀S, ∃ CS, NS ⊆ X, f(T) = f(S) if CS ⊆ T ⊆ NS;\n(b) |CS| ≤ K\ninput\n#views\naccuracy\naccuracy\navg. class\noverall\nSPH [11]\nmesh\n-\n68.2\n-\n3DShapeNets [28]\nvolume\n77.3\n84.7\nVoxNet [17]\nvolume\n83.0\n85.9\nSubvolume [18]\nvolume\n86.0\n89.2\nLFD [28]\nimage\n75.5\n-\nMVCNN [23]\nimage\n90.1\n-\nOurs baseline\npoint\n-\n72.6\n77.4\nOurs PointNet\npoint\n86.2\n89.2\nTable 1. Classiﬁcation results on ModelNet40. Our net achieves\nstate-of-the-art among deep nets on 3D input.\nWe explain the implications of the theorem. (a) says that\nf(S) is unchanged up to the input corruption if all points\nin CS are preserved; it is also unchanged with extra noise\npoints up to NS. (b) says that CS only contains a bounded\nnumber of points, determined by K in (1). In other words,\nf(S) is in fact totally determined by a ﬁnite subset CS ⊆ S\nof less or equal to K elements. We therefore call CS the\ncritical point set of S and K the bottleneck dimension of f.\nCombined with the continuity of h, this explains the\nrobustness of our model w.r.t point perturbation, corruption\nand extra noise points. The robustness is gained in analogy\nto the sparsity principle in machine learning models.\nIntuitively, our network learns to summarize a shape by\na sparse set of key points. In experiment section we see\nthat the key points form the skeleton of an object.\n5. Experiment\nExperiments are divided into four parts. First, we show\nPointNets can be applied to multiple 3D recognition tasks\n(Sec 5.1).\nSecond, we provide detailed experiments to\nvalidate our network design (Sec 5.2). At last we visualize\nwhat the network learns (Sec 5.3) and analyze time and\nspace complexity (Sec 5.4).\n5.1. Applications\nIn this section we show how our network can be\ntrained to perform 3D object classiﬁcation, object part\nsegmentation and semantic scene segmentation 1.\nEven\nthough we are working on a brand new data representation\n(point sets), we are able to achieve comparable or even\nbetter performance on benchmarks for several tasks.\n3D Object Classiﬁcation\nOur network learns global\npoint cloud feature that can be used for object classiﬁcation.\nWe evaluate our model on the ModelNet40 [28] shape\nclassiﬁcation benchmark. There are 12,311 CAD models\nfrom 40 man-made object categories, split into 9,843 for\n1More application examples such as correspondence and point cloud\nbased CAD model retrieval are included in supplementary material.",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 16
    },
    {
      "page_number": 6,
      "text": "mean\naero\nbag\ncap\ncar\nchair\near\nguitar knife\nlamp\nlaptop motor\nmug pistol rocket skate\ntable\nphone\nboard\n# shapes\n76\n898\n69\n392\n451\n184 283\n152\nWu [27]\n-\n63.2\n-\n-\n-\n73.5\n-\n-\n-\n74.4\n-\n-\n-\n-\n-\n-\n74.8\nYi [29]\n81.4\n81.0\n78.4\n77.7\n75.7\n87.6\n61.9\n92.0\n85.4\n82.5\n95.7\n70.6\n91.9 85.9\n53.1\n69.8\n75.3\n3DCNN\n79.4\n75.1\n72.8\n73.3\n70.0\n87.2\n63.5\n88.4\n79.6\n74.4\n93.9\n58.7\n91.8 76.4\n51.2\n65.3\n77.1\nOurs\n83.7\n83.4\n78.7\n82.5\n74.9\n89.6\n73.0\n91.5\n85.9\n80.8\n95.3\n65.2\n93.0 81.2\n57.9\n72.8\n80.6\nTable 2. Segmentation results on ShapeNet part dataset. Metric is mIoU(%) on points. We compare with two traditional methods [27]\nand [29] and a 3D fully convolutional network baseline proposed by us. Our PointNet method achieved the state-of-the-art in mIoU.\ntraining and 2,468 for testing.\nWhile previous methods\nfocus on volumetric and mult-view image representations,\nwe are the ﬁrst to directly work on raw point cloud.\nWe uniformly sample 1024 points on mesh faces according to face area and normalize them into a unit sphere.\nDuring training we augment the point cloud on-the-ﬂy by\nrandomly rotating the object along the up-axis and jitter the\nposition of each points by a Gaussian noise with zero mean\nand 0.02 standard deviation.\nIn Table 1, we compare our model with previous works\nas well as our baseline using MLP on traditional features\nextracted from point cloud (point density, D2, shape contour\netc.).\nOur model achieved state-of-the-art performance\namong methods based on 3D input (volumetric and point\ncloud). With only fully connected layers and max pooling,\nour net gains a strong lead in inference speed and can be\neasily parallelized in CPU as well. There is still a small\ngap between our method and multi-view based method\n(MVCNN [23]), which we think is due to the loss of ﬁne\ngeometry details that can be captured by rendered images.\n3D Object Part Segmentation\nPart segmentation is a\nchallenging ﬁne-grained 3D recognition task. Given a 3D\nscan or a mesh model, the task is to assign part category\nlabel (e.g. chair leg, cup handle) to each point or face.\nWe evaluate on ShapeNet part data set from [29], which\ncontains 16,881 shapes from 16 categories, annotated with\n50 parts in total. Most object categories are labeled with\ntwo to ﬁve parts. Ground truth annotations are labeled on\nsampled points on the shapes.\nWe formulate part segmentation as a per-point classiﬁcation problem. Evaluation metric is mIoU on points. For\neach shape S of category C, to calculate the shape’s mIoU:\nFor each part type in category C, compute IoU between\ngroundtruth and prediction. If the union of groundtruth and\nprediction points is empty, then count part IoU as 1. Then\nwe average IoUs for all part types in category C to get mIoU\nfor that shape. To calculate mIoU for the category, we take\naverage of mIoUs for all shapes in that category.\nIn this section, we compare our segmentation version\nPointNet (a modiﬁed version of Fig 2, Segmentation\nNetwork) with two traditional methods [27] and [29] that\nboth take advantage of point-wise geometry features and\ncorrespondences between shapes, as well as our own\n3D CNN baseline.\nSee supplementary for the detailed\nmodiﬁcations and network architecture for the 3D CNN.\nIn Table 2, we report per-category and mean IoU(%)\nscores. We observe a 2.3% mean IoU improvement and our\nnet beats the baseline methods in most categories.\nWe also perform experiments on simulated Kinect scans\nto test the robustness of these methods. For every CAD\nmodel in the ShapeNet part data set, we use Blensor Kinect\nSimulator [7] to generate incomplete point clouds from six\nrandom viewpoints. We train our PointNet on the complete\nshapes and partial scans with the same network architecture\nand training setting. Results show that we lose only 5.3%\nmean IoU. In Fig 3, we present qualitative results on both\ncomplete and partial data. One can see that though partial\ndata is fairly challenging, our predictions are reasonable.\nSemantic Segmentation in Scenes\nOur network on part\nsegmentation can be easily extended to semantic scene\nsegmentation, where point labels become semantic object\nclasses instead of object part labels.\nWe experiment on the Stanford 3D semantic parsing data\nset [1].\nThe dataset contains 3D scans from Matterport\nscanners in 6 areas including 271 rooms. Each point in the\nscan is annotated with one of the semantic labels from 13\ncategories (chair, table, ﬂoor, wall etc. plus clutter).\nTo prepare training data, we ﬁrstly split points by room,\nand then sample rooms into blocks with area 1m by 1m.\nWe train our segmentation version of PointNet to predict\nmean IoU\noverall accuracy\nOurs baseline\n20.12\n53.19\nOurs PointNet\n47.71\n78.62\nTable 3. Results on semantic segmentation in scenes. Metric is\naverage IoU over 13 classes (structural and furniture elements plus\nclutter) and classiﬁcation accuracy calculated on points.\ntable\nchair\nsofa\nboard\nmean\n# instance\n1363\n137\nArmeni et al. [1]\n46.02\n16.15\n6.78\n3.91\n18.22\nOurs\n46.67\n33.80\n4.76\n11.72\n24.24\nTable 4. Results on 3D object detection in scenes. Metric is\naverage precision with threshold IoU 0.5 computed in 3D volumes.",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": false,
      "image_count": 0
    },
    {
      "page_number": 7,
      "text": "Input\nOutput\nFigure 4. Qualitative results for semantic segmentation. Top\nrow is input point cloud with color. Bottom row is output semantic\nsegmentation result (on points) displayed in the same camera\nviewpoint as input.\nper point class in each block. Each point is represented by\na 9-dim vector of XYZ, RGB and normalized location as\nto the room (from 0 to 1). At training time, we randomly\nsample 4096 points in each block on-the-ﬂy. At test time,\nwe test on all the points. We follow the same protocol as [1]\nto use k-fold strategy for train and test.\nWe compare our method with a baseline using handcrafted point features. The baseline extracts the same 9dim local features and three additional ones: local point\ndensity, local curvature and normal. We use standard MLP\nas the classiﬁer.\nResults are shown in Table 3, where\nour PointNet method signiﬁcantly outperforms the baseline\nmethod. In Fig 4, we show qualitative segmentation results.\nOur network is able to output smooth predictions and is\nrobust to missing points and occlusions.\nBased on the semantic segmentation output from our\nnetwork, we further build a 3D object detection system\nusing connected component for object proposal (see supplementary for details). We compare with previous stateof-the-art method in Table 4. The previous method is based\non a sliding shape method (with CRF post processing) with\nSVMs trained on local geometric features and global room\ncontext feature in voxel grids. Our method outperforms it\nby a large margin on the furniture categories reported.\n5.2. Architecture Design Analysis\nIn this section we validate our design choices by control\nexperiments. We also show the effects of our network’s\nhyperparameters.\nComparison with Alternative Order-invariant Methods\nAs mentioned in Sec 4.2, there are at least three options for\nconsuming unordered set inputs. We use the ModelNet40\nshape classiﬁcation problem as a test bed for comparisons\nof those options, the following two control experiment will\nalso use this task.\nThe baselines (illustrated in Fig 5) we compared with\ninclude multi-layer perceptron on unsorted and sorted\n(1,2,3)\n(2,3,4)\n(1,3,1)\nrnn \ncell\nrnn \ncell\nrnn \ncell\n...\n(1,2,3)\n(2,3,4)\n(1,3,1)\n...\n...\n(1,2,3)\n(1,3,1)\n(2,3,4)\n...\nsorting\nsequential model\nsymmetry function\nsorted\nFigure 5. Three approaches to achieve order invariance. Multilayer perceptron (MLP) applied on points consists of 5 hidden\nlayers with neuron sizes 64,64,64,128,1024, all points share a\nsingle copy of MLP. The MLP close to the output consists of two\nlayers with sizes 512,256.\npoints as n×3 arrays, RNN model that considers input point\nas a sequence, and a model based on symmetry functions.\nThe symmetry operation we experimented include max\npooling, average pooling and an attention based weighted\nsum. The attention method is similar to that in [25], where\na scalar score is predicted from each point feature, then the\nscore is normalized across points by computing a softmax.\nThe weighted sum is then computed on the normalized\nscores and the point features. As shown in Fig 5, maxpooling operation achieves the best performance by a large\nwinning margin, which validates our choice.\nEffectiveness of Input and Feature Transformations\nIn\nTable 5 we demonstrate the positive effects of our input\nand feature transformations (for alignment). It’s interesting\nto see that the most basic architecture already achieves\nquite reasonable results. Using input transformation gives\na 0.8% performance boost.\nThe regularization loss is\nnecessary for the higher dimension transform to work.\nBy combining both transformations and the regularization\nterm, we achieve the best performance.\nRobustness Test\nWe show our PointNet, while simple\nand effective, is robust to various kinds of input corruptions.\nWe use the same architecture as in Fig 5’s max pooling\nnetwork. Input points are normalized into a unit sphere.\nResults are in Fig 6.\nAs to missing points, when there are 50% points missing,\nthe accuracy only drops by 2.4% and 3.8% w.r.t. furthest\nand random input sampling. Our net is also robust to outlier\nTransform\naccuracy\nnone\n87.1\ninput (3x3)\n87.9\nfeature (64x64)\n86.9\nfeature (64x64) + reg.\n87.4\nboth\n89.2\nTable 5. Effects of input feature transforms. Metric is overall\nclassiﬁcation accuracy on ModelNet40 test set.",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 7
    },
    {
      "page_number": 8,
      "text": "30 \n50 \n70 \n90 \n0.05 \n0.1 \nAccuracy (%) \nPerturbation noise std \n40 \n60 \n80 \n100 \n0.2 \n0.4 \n0.6 \n0.8 \nAccuracy (%) \nMissing data ratio \nFurthest \nRandom \n30 \n50 \n70 \n90 \n0.1 \n0.2 \n0.3 \n0.4 \n0.5 \nAccuracy (%) \nOutlier ratio \nXYZ+density \nFigure 6. PointNet robustness test.\nThe metric is overall\nclassiﬁcation accuracy on ModelNet40 test set.\nLeft: Delete\npoints. Furthest means the original 1024 points are sampled with\nfurthest sampling. Middle: Insertion. Outliers uniformly scattered\nin the unit sphere. Right: Perturbation. Add Gaussian noise to\neach point independently.\npoints, if it has seen those during training. We evaluate two\nmodels: one trained on points with (x, y, z) coordinates; the\nother on (x, y, z) plus point density. The net has more than\n80% accuracy even when 20% of the points are outliers.\nFig 6 right shows the net is robust to point perturbations.\n5.3. Visualizing PointNet\nIn Fig 7, we visualize critical point sets CS and upperbound shapes NS (as discussed in Thm 2) for some sample\nshapes S. The point sets between the two shapes will give\nexactly the same global shape feature f(S).\nWe can see clearly from Fig 7 that the critical point\nsets CS, those contributed to the max pooled feature,\nsummarizes the skeleton of the shape. The upper-bound\nshapes NS illustrates the largest possible point cloud that\ngive the same global shape feature f(S) as the input point\ncloud S. CS and NS reﬂect the robustness of PointNet,\nmeaning that losing some non-critical points does not\nchange the global shape signature f(S) at all.\nThe NS is constructed by forwarding all the points in a\nedge-length-2 cube through the network and select points p\nwhose point function values (h1(p), h2(p), · · · , hK(p)) are\nno larger than the global shape descriptor.\nOriginal Shape\nCritical Point Sets\nUpper-bound Shapes\nFigure 7. Critical points and upper bound shape. While critical\npoints jointly determine the global shape feature for a given shape,\nany point cloud that falls between the critical points set and the\nupper bound shape gives exactly the same feature. We color-code\nall ﬁgures to show the depth information.\n5.4. Time and Space Complexity Analysis\nTable 6 summarizes space (number of parameters in\nthe network) and time (ﬂoating-point operations/sample)\ncomplexity of our classiﬁcation PointNet. We also compare\nPointNet to a representative set of volumetric and multiview based architectures in previous works.\nWhile MVCNN [23] and Subvolume (3D CNN) [18]\nachieve high performance, PointNet is orders more efﬁcient\nin computational cost (measured in FLOPs/sample: 141x\nand 8x more efﬁcient, respectively).\nBesides, PointNet\nis much more space efﬁcient than MVCNN in terms of\n#param in the network (17x less parameters). Moreover,\nPointNet is much more scalable - it’s space and time\ncomplexity is O(N) - linear in the number of input points.\nHowever, since convolution dominates computing time,\nmulti-view method’s time complexity grows squarely on\nimage resolution and volumetric convolution based method\ngrows cubically with the volume size.\nEmpirically, PointNet is able to process more than\none million points per second for point cloud classiﬁcation (around 1K objects/second) or semantic segmentation\n(around 2 rooms/second) with a 1080X GPU on TensorFlow, showing great potential for real-time applications.\n#params\nFLOPs/sample\nPointNet (vanilla)\n0.8M\n148M\nPointNet\n3.5M\n440M\nSubvolume [18]\n16.6M\n3633M\nMVCNN [23]\n60.0M\n62057M\nTable 6. Time and space complexity of deep architectures for\n3D data classiﬁcation.\nPointNet (vanilla) is the classiﬁcation\nPointNet without input and feature transformations.\nstands for ﬂoating-point operation. The “M” stands for million.\nSubvolume and MVCNN used pooling on input data from multiple\nrotations or views, without which they have much inferior\nperformance.\n6. Conclusion\nIn this work, we propose a novel deep neural network\nPointNet that directly consumes point cloud. Our network\nprovides a uniﬁed approach to a number of 3D recognition\ntasks including object classiﬁcation, part segmentation and\nsemantic segmentation, while obtaining on par or better\nresults than state of the arts on standard benchmarks. We\nalso provide theoretical analysis and visualizations towards\nunderstanding of our network.\nAcknowledgement.\nThe authors gratefully acknowledge\nthe support of a Samsung GRO grant, ONR MURI N0001413-1-0341 grant, NSF grant IIS-1528025, a Google Focused Research Award, a gift from the Adobe corporation\nand hardware donations by NVIDIA.",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 17
    },
    {
      "page_number": 9,
      "text": "References\n[1] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis,\nM. Fischer, and S. Savarese.\n3d semantic parsing of\nlarge-scale indoor spaces.\nIn Proceedings of the IEEE\nInternational Conference on Computer Vision and Pattern\nRecognition, 2016. 6, 7\n[2] M. Aubry, U. Schlickewei, and D. Cremers.\nThe wave\nkernel signature: A quantum mechanical approach to shape\nanalysis. In Computer Vision Workshops (ICCV Workshops),\n2011 IEEE International Conference on, pages 1626-1633.\nIEEE, 2011. 2\n[3] M. M. Bronstein and I. Kokkinos.\nScale-invariant heat\nkernel signatures for non-rigid shape recognition.\nIn\nComputer Vision and Pattern Recognition (CVPR), 2010\nIEEE Conference on, pages 1704-1711. IEEE, 2010. 2\n[4] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral\nnetworks and locally connected networks on graphs. arXiv\npreprint arXiv:1312.6203, 2013. 2\n[5] D.-Y. Chen, X.-P. Tian, Y.-T. Shen, and M. Ouhyoung. On\nvisual similarity based 3d model retrieval.\nIn Computer\ngraphics forum, volume 22, pages 223-232. Wiley Online\nLibrary, 2003. 2\n[6] Y. Fang, J. Xie, G. Dai, M. Wang, F. Zhu, T. Xu, and\nE. Wong.\n3d deep shape descriptor.\nIn Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 2319-2328, 2015. 2\n[7] M. Gschwandtner, R. Kwitt, A. Uhl, and W. Pree. BlenSor:\nBlender Sensor Simulation Toolbox Advances in Visual\nComputing.\nvolume 6939 of Lecture Notes in Computer\nScience, chapter 20, pages 199-208. Springer Berlin /\nHeidelberg, Berlin, Heidelberg, 2011. 6\n[8] K. Guo, D. Zou, and X. Chen.\n3d mesh labeling via\ndeep convolutional neural networks. ACM Transactions on\nGraphics (TOG), 35(1):3, 2015. 2\n[9] M. Jaderberg, K. Simonyan, A. Zisserman, et al.\nSpatial\ntransformer networks. In NIPS 2015. 4\n[10] A. E. Johnson and M. Hebert. Using spin images for efﬁcient\nobject recognition in cluttered 3d scenes. IEEE Transactions\non pattern analysis and machine intelligence, 21(5):433-\n449, 1999. 2\n[11] M. Kazhdan, T. Funkhouser, and S. Rusinkiewicz. Rotation\ninvariant spherical harmonic representation of 3 d shape descriptors. In Symposium on geometry processing, volume 6,\npages 156-164, 2003. 5\n[12] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. 13\n[13] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas.\nFpnn:\nField probing neural networks for 3d data. arXiv preprint\narXiv:1605.06240, 2016. 2\n[14] H. Ling and D. W. Jacobs. Shape classiﬁcation using the\ninner-distance. IEEE transactions on pattern analysis and\nmachine intelligence, 29(2):286-299, 2007. 2\n[15] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne.\nJournal of Machine Learning Research, 9(Nov):2579-2605,\n2008. 15\n[16] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst.\nGeodesic convolutional neural networks on riemannian manifolds. In Proceedings of the IEEE International Conference\non Computer Vision Workshops, pages 37-45, 2015. 2\n[17] D. Maturana and S. Scherer. Voxnet: A 3d convolutional\nneural network for real-time object recognition. In IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems,\nSeptember 2015. 2, 5, 10, 11\n[18] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. Guibas.\nVolumetric and multi-view cnns for object classiﬁcation on\n3d data. In Proc. Computer Vision and Pattern Recognition\n(CVPR), IEEE, 2016. 2, 5, 8\n[19] R. B. Rusu, N. Blodow, and M. Beetz. Fast point feature\nhistograms (fpfh) for 3d registration.\nIn Robotics and\nAutomation, 2009. ICRA’09. IEEE International Conference\non, pages 3212-3217. IEEE, 2009. 2\n[20] R. B. Rusu, N. Blodow, Z. C. Marton, and M. Beetz. Aligning point cloud views using persistent feature histograms.\nIn 2008 IEEE/RSJ International Conference on Intelligent\nRobots and Systems, pages 3384-3391. IEEE, 2008. 2\n[21] M. Savva, F. Yu, H. Su, M. Aono, B. Chen, D. Cohen-Or,\nW. Deng, H. Su, S. Bai, X. Bai, et al. Shrec16 track largescale 3d shape retrieval from shapenet core55. 2\n[22] P. Y. Simard, D. Steinkraus, and J. C. Platt. Best practices for\nconvolutional neural networks applied to visual document\nanalysis. In ICDAR, volume 3, pages 958-962, 2003. 13\n[23] H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller.\nMulti-view convolutional neural networks for 3d shape\nrecognition. In Proc. ICCV, to appear, 2015. 2, 5, 6, 8\n[24] J. Sun, M. Ovsjanikov, and L. Guibas.\nA concise and\nprovably informative multi-scale signature based on heat\ndiffusion. In Computer graphics forum, volume 28, pages\n1383-1392. Wiley Online Library, 2009. 2\n[25] O. Vinyals, S. Bengio, and M. Kudlur.\nOrder matters:\nSequence to sequence for sets.\narXiv preprint\narXiv:1511.06391, 2015. 2, 4, 7\n[26] D. Z. Wang and I. Posner. Voting for voting in online point\ncloud object detection. Proceedings of the Robotics: Science\nand Systems, Rome, Italy, 1317, 2015. 2\n[27] Z. Wu, R. Shou, Y. Wang, and X. Liu. Interactive shape cosegmentation via label propagation. Computers & Graphics,\n38:248-254, 2014. 6, 10\n[28] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and\nJ. Xiao. 3d shapenets: A deep representation for volumetric\nshapes. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 1912-1920, 2015. 2,\n5, 11\n[29] L. Yi, V. G. Kim, D. Ceylan, I.-C. Shen, M. Yan, H. Su,\nC. Lu, Q. Huang, A. Sheffer, and L. Guibas. A scalable active\nframework for region annotation in 3d shape collections.\nSIGGRAPH Asia, 2016. 6, 10, 18",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": false,
      "image_count": 0
    },
    {
      "page_number": 10,
      "text": "Supplementary\nA. Overview\nThis document provides additional quantitative results,\ntechnical details and more qualitative test examples to the\nmain paper.\nIn Sec B we extend the robustness test to compare\nPointNet with VoxNet on incomplete input.\nIn Sec C\nwe provide more details on neural network architectures,\ntraining parameters and in Sec D we describe our detection\npipeline in scenes. Then Sec E illustrates more applications\nof PointNet, while Sec F shows more analysis experiments.\nSec G provides a proof for our theory on PointNet. At last,\nwe show more visualization results in Sec H.\nB. Comparison between PointNet and VoxNet\n(Sec 5.2)\nWe extend the experiments in Sec 5.2 Robustness Test\nto compare PointNet and VoxNet [17] (a representative\narchitecture for volumetric representation) on robustness to\nmissing data in the input point cloud. Both networks are\ntrained on the same train test split with 1024 number of\npoints as input. For VoxNet we voxelize the point cloud\nto 32 × 32 × 32 occupancy grids and augment the training\ndata by random rotation around up-axis and jittering.\nAt test time, input points are randomly dropped out\nby a certain ratio.\nAs VoxNet is sensitive to rotations,\nits prediction uses average scores from 12 viewpoints of\na point cloud.\nAs shown in Fig 8, we see that our\nPointNet is much more robust to missing points. VoxNet’s\naccuracy dramatically drops when half of the input points\nare missing, from 86.3% to 46.0% with a 40.3% difference,\nwhile our PointNet only has a 3.7% performance drop. This\ncan be explained by the theoretical analysis and explanation\nof our PointNet - it is learning to use a collection of critical\npoints to summarize the shape, thus it is very robust to\nmissing data.\nC. Network Architecture and Training Details\n(Sec 5.1)\nPointNet Classiﬁcation Network\nAs the basic architecture is already illustrated in the main paper, here we\nprovides more details on the joint alignment/transformation\nnetwork and training parameters.\nThe ﬁrst transformation network is a mini-PointNet that\ntakes raw point cloud as input and regresses to a 3 × 3\nmatrix.\nIt’s composed of a shared MLP(64, 128, 1024)\nnetwork (with layer output sizes 64, 128, 1024) on each\npoint, a max pooling across points and two fully connected\nlayers with output sizes 512, 256.\nThe output matrix is\ninitialized as an identity matrix. All layers, except the last\none, include ReLU and batch normalization. The second\nPointNet\nVoxNet\n87.1\n86.3\n0.5\n83.3\n0.75\n18.5\n0.875\n59.2\n13.3\n0.9375\n33.2\n10.2\n20 \n60 \n100 \n0.2 \n0.4 \n0.6 \n0.8 \nAccuracy (%) \nMissing Data Ratio \nPointNet \nVoxNet \nFigure 8. PointNet v.s. VoxNet [17] on incomplete input data.\nMetric is overall classiﬁcation accurcacy on ModelNet40 test set.\nNote that VoxNet is using 12 viewpoints averaging while PointNet\nis using only one view of the point cloud. Evidently PointNet\npresents much stronger robustness to missing points.\ntransformation network has the same architecture as the ﬁrst\none except that the output is a 64 × 64 matrix. The matrix\nis also initialized as an identity. A regularization loss (with\nweight 0.001) is added to the softmax classiﬁcation loss to\nmake the matrix close to orthogonal.\nWe use dropout with keep ratio 0.7 on the last fully\nconnected layer, whose output dimension 256, before class\nscore prediction. The decay rate for batch normalization\nstarts with 0.5 and is gradually increased to 0.99. We use\nadam optimizer with initial learning rate 0.001, momentum\n0.9 and batch size 32. The learning rate is divided by 2\nevery 20 epochs. Training on ModelNet takes 3-6 hours to\nconverge with TensorFlow and a GTX1080 GPU.\nPointNet Segmentation Network\nThe segmentation network is an extension to the classiﬁcation PointNet. Local\npoint features (the output after the second transformation\nnetwork) and global feature (output of the max pooling)\nare concatenated for each point. No dropout is used for\nsegmentation network. Training parameters are the same\nas the classiﬁcation network.\nAs to the task of shape part segmentation, we made\na few modiﬁcations to the basic segmentation network\narchitecture (Fig 2 in main paper) in order to achieve best\nperformance, as illustrated in Fig 9.\nWe add a one-hot\nvector indicating the class of the input and concatenate it\nwith the max pooling layer’s output.\nWe also increase\nneurons in some layers and add skip links to collect local\npoint features in different layers and concatenate them to\nform point feature input to the segmentation network.\nWhile [27] and [29] deal with each object category\nindependently, due to the lack of training data for some\ncategories (the total number of shapes for all the categories\nin the data set are shown in the ﬁrst line), we train our\nPointNet across categories (but with one-hot vector input to\nindicate category). To allow fair comparison, when testing",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 5
    },
    {
      "page_number": 11,
      "text": "input points\npoint features\noutput scores\nmax\npool\nshared \nshared \nshared \nnx3\nnx3\nnx64\nnx64\nnx1024\nn x 1088\nnx128\nmlp (64,128,128)\nmlp (64,128,1024)\ninput\ntransform\nfeature\ntransform\nmlp\n(512,256,k)\nglobal feature\nmlp (512,256,128)\nT-Net\nmatrix\nmultiply\n3x3\ntransform\nT-Net\nmatrix\nmultiply\n64x64\ntransform\nshared \nmlp (128,m)\noutput scores\nnxm\nk\nClassification Network\nSegmentation Network\nnx3\nnx3\nT1\nnx64\nnx128\nnx128\nnx128\nT2\nnx512\nnx2048\nnx64\nnx128\nnx128\nnx128\nnx512\nn x 3024\n(256,256,128)\nnx50\none-hot\ninput points\npart scores\nFigure 9. Network architecture for part segmentation. T1 and\nT2 are alignment/transformation networks for input points and\nfeatures. FC is fully connected layer operating on each point. MLP\nis multi-layer perceptron on each point. One-hot is a vector of size\n16 indicating category of the input shape.\n32 filters \nof stride 1\n32\n32\n32\n5\n32 filters \nof stride 1\n32 filters \nof stride 1\n32 filters \nof stride 1\n32 filters \nof stride 1\n1\n64 filters \nof stride 1\n1\n64 filters \nof stride 1\n32\n1\n50 filters \nof stride 1\nin-category\nprediction\nFigure 10. Baseline 3D CNN segmentation network.\nThe\nnetwork is fully convolutional and predicts part scores for each\nvoxel.\nthese two models, we only predict part labels for the given\nspeciﬁc object category.\nAs to semantic segmentation task, we used the architecture as in Fig 2 in the main paper.\nIt takes around six to twelve hours to train the model on\nShapeNet part dataset and around half a day to train on the\nStanford semantic parsing dataset.\nBaseline 3D CNN Segmentation Network\nIn ShapeNet\npart segmentation experiment, we compare our proposed\nsegmentation version PointNet to two traditional methods\nas well as a 3D volumetric CNN network baseline.\nIn\nFig 10, we show the baseline 3D volumetric CNN network\nwe use. We generalize the well-known 3D CNN architectures, such as VoxNet [17] and 3DShapeNets [28] to a fully\nconvolutional 3D CNN segmentation network.\nFor a given point cloud, we ﬁrst convert it to the volumetric representation as a occupancy grid with resolution\n32 × 32 × 32. Then, ﬁve 3D convolution operations each\nwith 32 output channels and stride of 1 are sequentially\napplied to extract features. The receptive ﬁeld is 19 for each\nvoxel. Finally, a sequence of 3D convolutional layers with\nkernel size 1 × 1 × 1 is appended to the computed feature\nmap to predict segmentation label for each voxel. ReLU and\nbatch normalization are used for all layers except the last\none. The network is trained across categories, however, in\norder to compare with other baseline methods where object\ncategory is given, we only consider output scores in the\ngiven object category.\nD. Details on Detection Pipeline (Sec 5.1)\nWe build a simple 3D object detection system based on\nthe semantic segmentation results and our object classiﬁcation PointNet.\nWe use connected component with segmentation scores\nto get object proposals in scenes. Starting from a random\npoint in the scene, we ﬁnd its predicted label and use\nBFS to search nearby points with the same label, with\na search radius of 0.2 meter.\nIf the resulted cluster has\nmore than 200 points (assuming a 4096 point sample in\na 1m by 1m area), the cluster’s bounding box is marked\nas one object proposal.\nFor each proposed object, it’s\ndetection score is computed as the average point score for\nthat category. Before evaluation, proposals with extremely\nsmall areas/volumes are pruned.\nFor tables, chairs and\nsofas, the bounding boxes are extended to the ﬂoor in case\nthe legs are separated with the seat/surface.\nWe observe that in some rooms such as auditoriums\nlots of objects (e.g. chairs) are close to each other, where\nconnected component would fail to correctly segment out\nindividual ones. Therefore we leverage our classiﬁcation\nnetwork and uses sliding shape method to alleviate the\nproblem for the chair class. We train a binary classiﬁcation\nnetwork for each category and use the classiﬁer for sliding\nwindow detection.\nThe resulted boxes are pruned by\nnon-maximum suppression.\nThe proposed boxes from\nconnected component and sliding shapes are combined for\nﬁnal evaluation.\nIn Fig 11, we show the precision-recall curves for object\ndetection. We trained six models, where each one of them\nis trained on ﬁve areas and tested on the left area. At test\nphase, each model is tested on the area it has never seen.\nThe test results for all six areas are aggregated for the PR\ncurve generation.\nE. More Applications (Sec 5.1)\nModel Retrieval from Point Cloud\nOur PointNet learns\na global shape signature for every given input point cloud.\nWe expect geometrically similar shapes have similar global\nsignature.\nIn this section, we test our conjecture on the\nshape retrieval application. To be more speciﬁc, for every\ngiven query shape from ModelNet test split, we compute\nits global signature (output of the layer before the score\nprediction layer) given by our classiﬁcation PointNet and\nretrieve similar shapes in the train split by nearest neighbor\nsearch. Results are shown in Fig 12.",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 2
    },
    {
      "page_number": 12,
      "text": "Figure 11. Precision-recall curves for object detection in 3D\npoint cloud. We evaluated on all six areas for four categories:\ntable, chair, sofa and board. IoU threshold is 0.5 in volume.\nQuery\nPoint Cloud\nTop-5 Retrieval CAD Models\nFigure 12. Model retrieval from point cloud.\nFor every\ngiven point cloud, we retrieve the top-5 similar shapes from the\nModelNet test split. From top to bottom rows, we show examples\nof chair, plant, nightstand and bathtub queries. Retrieved results\nthat are in wrong category are marked by red boxes.\nShape Correspondence\nIn this section, we show that\npoint features learnt by PointNet can be potentially used\nto compute shape correspondences. Given two shapes, we\ncompute the correspondence between their critical point\nsets CS’s by matching the pairs of points that activate\nthe same dimensions in the global features.\nFig 13 and\nFig 14 show the detected shape correspondence between\ntwo similar chairs and tables.\nF. More Architecture Analysis (Sec 5.2)\nEffects of Bottleneck Dimension and Number of Input\nPoints\nHere we show our model’s performance change\nwith regard to the size of the ﬁrst max layer output as\nwell as the number of input points. In Fig 15 we see that\nperformance grows as we increase the number of points\nhowever it saturates at around 1K points. The max layer\nsize plays an important role, increasing the layer size from\nFigure 13. Shape correspondence between two chairs. For the\nclarity of the visualization, we only show 20 randomly picked\ncorrespondence pairs.\nFigure 14. Shape correspondence between two tables. For the\nclarity of the visualization, we only show 20 randomly picked\ncorrespondence pairs.\n64 to 1024 results in a 2−4% performance gain. It indicates\nthat we need enough point feature functions to cover the 3D\nspace in order to discriminate different shapes.\nIt’s worth notice that even with 64 points as input\n(obtained from furthest point sampling on meshes), our\nnetwork can achieve decent performance.\n82 \n84 \n86 \n88 \n200 \n600 \n1000 \nAccuracy (%) \nBottleneck size \n128 \n1024 \n#points\nFigure 15. Effects of bottleneck size and number of input\npoints. The metric is overall classiﬁcation accuracy on ModelNet40 test set.\nMNIST Digit Classiﬁcation\nWhile we focus on 3D point\ncloud learning, a sanity check experiment is to apply our\nnetwork on a 2D point clouds - pixel sets.\nTo convert an MNIST image into a 2D point set we\nthreshold pixel values and add the pixel (represented as a",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 32
    },
    {
      "page_number": 13,
      "text": "point with (x, y) coordinate in the image) with values larger\nthan 128 to the set. We use a set size of 256. If there are\nmore than 256 pixels int he set, we randomly sub-sample it;\nif there are less, we pad the set with the one of the pixels in\nthe set (due to our max operation, which point to use for the\npadding will not affect outcome).\nAs seen in Table 7, we compare with a few baselines\nincluding multi-layer perceptron that considers input image\nas an ordered vector, a RNN that consider input as sequence\nfrom pixel (0,0) to pixel (27,27), and a vanilla version CNN.\nWhile the best performing model on MNIST is still well\nengineered CNNs (achieving less than 0.3% error rate),\nit’s interesting to see that our PointNet model can achieve\nreasonable performance by considering image as a 2D point\nset.\ninput\nerror (%)\nMulti-layer perceptron [22]\nvector\n1.60\nLeNet5 [12]\nimage\n0.80\nOurs PointNet\npoint set\n0.78\nTable 7. MNIST classiﬁcation results. We compare with vanilla\nversions of other deep architectures to show that our network based\non point sets input is achieving reasonable performance on this\ntraditional task.\nNormal Estimation\nIn segmentation version of PointNet,\nlocal point features and global feature are concatenated\nin order to provide context to local points.\nHowever,\nit’s unclear whether the context is learnt through this\nconcatenation. In this experiment, we validate our design\nby showing that our segmentation network can be trained\nto predict point normals, a local geometric property that is\ndetermined by a point’s neighborhood.\nWe train a modiﬁed version of our segmentation PointNet in a supervised manner to regress to the groundtruth point normals. We just change the last layer of our\nsegmentation PointNet to predict normal vector for each\npoint. We use absolute value of cosine distance as loss.\nFig. 16 compares our PointNet normal prediction results\n(the left columns) to the ground-truth normals computed\nfrom the mesh (the right columns).\nWe observe a\nreasonable normal reconstruction.\nOur predictions are\nmore smooth and continuous than the ground-truth which\nincludes ﬂipped normal directions in some region.\nSegmentation Robustness\nAs discussed in Sec 5.2 and\nSec B, our PointNet is less sensitive to data corruption and\nmissing points for classiﬁcation tasks since the global shape\nfeature is extracted from a collection of critical points from\nthe given input point cloud. In this section, we show that the\nrobustness holds for segmentation tasks too. The per-point\npart labels are predicted based on the combination of perpoint features and the learnt global shape feature. In Fig 17,\nGround-truth\nPrediction\nFigure 16. PointNet normal reconstrution results. In this ﬁgure,\nwe show the reconstructed normals for all the points in some\nsample point clouds and the ground-truth normals computed on\nthe mesh.\nwe illustrate the segmentation results for the given input\npoint clouds S (the left-most column), the critical point sets\nCS (the middle column) and the upper-bound shapes NS.\nNetwork Generalizability to Unseen Shape Categories\nIn Fig 18, we visualize the critical point sets and the upperbound shapes for new shapes from unseen categories (face,\nhouse, rabbit, teapot) that are not present in ModelNet or\nShapeNet. It shows that the learnt per-point functions are\ngeneralizable.\nHowever, since we train mostly on manmade objects with lots of planar structures, the reconstructed upper-bound shape in novel categories also contain\nmore planar surfaces.\nG. Proof of Theorem (Sec 4.3)\nLet X = {S : S ⊆ [0, 1] and |S| = n}.\nf : X → R is a continuous function on X w.r.t to\nHausdorff distance dH(·, ·) if the following condition is\nsatisﬁed:\n∀ϵ > 0, ∃δ > 0, for any S, S′ ∈ X, if dH(S, S′) < δ,\nthen |f(S) − f(S′)| < ϵ.\nWe show that f can be approximated arbitrarily by\ncomposing a symmetric function and a continuous function.",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 6
    },
    {
      "page_number": 14,
      "text": "Input Point Cloud\nCritical Point Sets\nUpper-bound Shapes\nFigure 17. The consistency of segmentation results.\nWe\nillustrate the segmentation results for some sample given point\nclouds S, their critical point sets CS and upper-bound shapes NS.\nWe observe that the shape family between the CS and NS share a\nconsistent segmentation results.\nOriginal Shape\nCritical Point Sets\nUpper-bound Shapes\nFigure 18. The critical point sets and the upper-bound shapes\nfor unseen objects. We visualize the critical point sets and the\nupper-bound shapes for teapot, bunny, hand and human body,\nwhich are not in the ModelNet or ShapeNet shape repository to\ntest the generalizability of the learnt per-point functions of our\nPointNet on other unseen objects. The images are color-coded\nto reﬂect the depth information.\nTheorem 1. Suppose f\n:\nX\n→\nR is a continuous\nset function w.r.t Hausdorff distance dH(·, ·).\n∀ϵ\n>\n0, ∃ a continuous function h and a symmetric function\ng(x1, . . . , xn) = γ◦MAX, where γ is a continuous function,\nMAX is a vector max operator that takes n vectors as input\nand returns a new vector of the element-wise maximum,\nsuch that for any S ∈ X,\n|f(S) − γ(MAX(h(x1), . . . , h(xn)))| < ϵ\nwhere x1, . . . , xn are the elements of S extracted in certain\norder,\nProof. By the continuity of f, we take δϵ so that |f(S) −\nf(S′)| < ϵ for any S, S′ ∈ X if dH(S, S′) < δϵ.\nDeﬁne K = ⌈1/δϵ⌉, which split [0, 1] into K intervals\nevenly and deﬁne an auxiliary function that maps a point to\nthe left end of the interval it lies in:\nσ(x) = ⌊Kx⌋\nK\nLet ˜S = {σ(x) : x ∈ S}, then\n|f(S) − f( ˜S)| < ϵ\nbecause dH(S, ˜S) < 1/K ≤ δϵ.\nLet hk(x) = e−d(x,[ k−1\nK , k\nK ]) be a soft indicator function\nwhere d(x, I) is the point to set (interval) distance. Let\nh(x) = [h1(x); . . . ; hK(x)], then h : R → RK.\nLet vj(x1, . . . , xn) = max{˜hj(x1), . . . , ˜hj(xn)}, indicating the occupancy of the j-th interval by points in S.\nLet v = [v1; . . . ; vK], then v : R × . . . × R\n\n\n\nn\n→ {0, 1}K\nis a symmetric function, indicating the occupancy of each\ninterval by points in S.\nDeﬁne τ : {0, 1}K → X as τ(v) = { k−1\nK\n: vk ≥ 1},\nwhich maps the occupancy vector to a set which contains\nthe left end of each occupied interval. It is easy to show:\nτ(v(x1, . . . , xn)) ≡ ˜S\nwhere x1, . . . , xn are the elements of S extracted in certain\norder.\nLet γ : RK → R be a continuous function such that\nγ(v) = f(τ(v)) for v ∈ {0, 1}K. Then,\n|γ(v(x1, . . . , xn)) − f(S)|\n=|f(τ(v(x1, . . . , xn))) − f(S)| < ϵ\nNote that γ(v(x1, . . . , xn)) can be rewritten as follows:\nγ(v(x1, . . . , xn)) =γ(MAX(h(x1), . . . , h(xn)))\n=(γ ◦ MAX)(h(x1), . . . , h(xn))\nObviously γ ◦ MAX is a symmetric function.\nNext we give the proof of Theorem 2.\nWe deﬁne\nu = MAX\nxi∈S {h(xi)} to be the sub-network of f which\nmaps a point set in [0, 1]m to a K-dimensional vector. The\nfollowing theorem tells us that small corruptions or extra\nnoise points in the input set is not likely to change the output\nof our network:\nTheorem 2. Suppose u : X\n→ RK such that u =\nxi∈S{h(xi)} and f = γ ◦ u. Then,",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 15
    },
    {
      "page_number": 15,
      "text": "(a) ∀S, ∃ CS, NS ⊆ X, f(T) = f(S) if CS ⊆ T ⊆ NS;\n(b) |CS| ≤ K\nProof. Obviously, ∀S ∈ X, f(S) is determined by u(S).\nSo we only need to prove that ∀S, ∃ CS, NS ⊆ X, f(T) =\nf(S) if CS ⊆ T ⊆ NS.\nFor the jth dimension as the output of u, there exists at\nleast one xj ∈ X such that hj(xj) = uj, where hj is the\njth dimension of the output vector from h. Take CS as the\nunion of all xj for j = 1, . . . , K. Then, CS satisﬁes the\nabove condition.\nAdding any additional points x such that h(x) ≤ u(S) at\nall dimensions to CS does not change u, hence f. Therefore,\nTS can be obtained adding the union of all such points to\nNS.\nFigure 19. Point function visualization.\nFor each per-point\nfunction h, we calculate the values h(p) for all the points p in a\ncube of diameter two located at the origin, which spatially covers\nthe unit sphere to which our input shapes are normalized when\ntraining our PointNet. In this ﬁgure, we visualize all the points\np that give h(p) > 0.5 with function values color-coded by the\nbrightness of the voxel. We randomly pick 15 point functions and\nvisualize the activation regions for them.\nH. More Visualizations\nClassiﬁcation Visualization\nWe use t-SNE[15] to embed\npoint cloud global signature (1024-dim) from our classiﬁcation PointNet into a 2D space. Fig 20 shows the embedding\nspace of ModelNet 40 test split shapes. Similar shapes are\nclustered together according to their semantic categories.\nSegmentation Visualization\nWe present more segmentation results on both complete CAD models and simulated\nKinect partial scans. We also visualize failure cases with\nerror analysis. Fig 21 and Fig 22 show more segmentation\nresults generated on complete CAD models and their\nsimulated Kinect scans.\nFig 23 illustrates some failure\ncases. Please read the caption for the error analysis.\nScene Semantic Parsing Visualization\nWe give a visualization of semantic parsing in Fig 24 where we show input\npoint cloud, prediction and ground truth for both semantic\nsegmentation and object detection for two ofﬁce rooms and\none conference room. The area and the rooms are unseen in\nthe training set.\nPoint Function Visualization\nOur classiﬁcation PointNet computes K (we take K = 1024 in this visualization)\ndimension point features for each point and aggregates\nall the per-point local features via a max pooling layer\ninto a single K-dim vector, which forms the global shape\ndescriptor.\nTo gain more insights on what the learnt per-point\nfunctions h’s detect, we visualize the points pi’s that\ngive high per-point function value f(pi) in Fig 19. This\nvisualization clearly shows that different point functions\nlearn to detect for points in different regions with various\nshapes scattered in the whole space.",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 15
    },
    {
      "page_number": 16,
      "text": "Figure 20. 2D embedding of learnt shape global features. We use t-SNE technique to visualize the learnt global shape features for the\nshapes in ModelNet40 test split.",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 1
    },
    {
      "page_number": 17,
      "text": "airplane\nbag\ncap\ncar\nearphone chair\nguitar\nknife\nrocket\npistol\ntable\nskate\nboard\nmotor\nbike\nmug\nlaptop\nlamp\nFigure 21. PointNet segmentation results on complete CAD models.\nairplane\nbag\ncap\ncar\nearphone chair\nguitar\nknife\nrocket\npistol\ntable\nskate\nboard\nmotor\nbike\nmug\nlaptop\nlamp\nFigure 22. PointNet segmentation results on simulated Kinect scans.",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 96
    },
    {
      "page_number": 18,
      "text": "(d)\n(b)\n(c)\n(a)\n(e)\n(f)\nFigure 23. PointNet segmentation failure cases. In this ﬁgure, we summarize six types of common errors in our segmentation application.\nThe prediction and the ground-truth segmentations are given in the ﬁrst and second columns, while the difference maps are computed and\nshown in the third columns. The red dots correspond to the wrongly labeled points in the given point clouds. (a) illustrates the most\ncommon failure cases: the points on the boundary are wrongly labeled. In the examples, the label predictions for the points near the\nintersections between the table/chair legs and the tops are not accurate. However, most segmentation algorithms suffer from this error. (b)\nshows the errors on exotic shapes. For examples, the chandelier and the airplane shown in the ﬁgure are very rare in the data set. (c) shows\nthat small parts can be overwritten by nearby large parts. For example, the jet engines for airplanes (yellow in the ﬁgure) are mistakenly\nclassiﬁed as body (green) or the plane wing (purple). (d) shows the error caused by the inherent ambiguity of shape parts. For example,\nthe two bottoms of the two tables in the ﬁgure are classiﬁed as table legs and table bases (category other in [29]), while ground-truth\nsegmentation is the opposite. (e) illustrates the error introduced by the incompleteness of the partial scans. For the two caps in the ﬁgure,\nalmost half of the point clouds are missing. (f) shows the failure cases when some object categories have too less training data to cover\nenough variety. There are only 54 bags and 39 caps in the whole dataset for the two categories shown here.",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 36
    },
    {
      "page_number": 19,
      "text": "Figure 24. Examples of semantic segmentation and object detection. First row is input point cloud, where walls and ceiling are hided\nfor clarity. Second and third rows are prediction and ground-truth of semantic segmentation on points, where points belonging to different\nsemantic regions are colored differently (chairs in red, tables in purple, sofa in orange, board in gray, bookcase in green, ﬂoors in blue,\nwindows in violet, beam in yellow, column in magenta, doors in khaki and clutters in black). The last two rows are object detection with\nbounding boxes, where predicted boxes are from connected components based on semantic segmentation prediction.",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 1
    }
  ],
  "sections": {
    "title": "Figure 22. PointNet segmentation results on simulated Kinect scans.\n\n(d)\n(b)\n(c)\n(a)\n(e)\n(f)\nFigure 23. PointNet segmentation failure cases. In this ﬁgure, we summarize six types of common errors in our segmentation application.\nThe prediction and the ground-truth segmentations are given in the ﬁrst and second columns, while the difference maps are computed and\nshown in the third columns. The red dots correspond to the wrongly labeled points in the given point clouds. (a) illustrates the most\ncommon failure cases: the points on the boundary are wrongly labeled. In the examples, the label predictions for the points near the\nintersections between the table/chair legs and the tops are not accurate. However, most segmentation algorithms suffer from this error. (b)\nshows the errors on exotic shapes. For examples, the chandelier and the airplane shown in the ﬁgure are very rare in the data set. (c) shows\nthat small parts can be overwritten by nearby large parts. For example, the jet engines for airplanes (yellow in the ﬁgure) are mistakenly\nclassiﬁed as body (green) or the plane wing (purple). (d) shows the error caused by the inherent ambiguity of shape parts. For example,\nthe two bottoms of the two tables in the ﬁgure are classiﬁed as table legs and table bases (category other in [29]), while ground-truth\nsegmentation is the opposite. (e) illustrates the error introduced by the incompleteness of the partial scans. For the two caps in the ﬁgure,\nalmost half of the point clouds are missing. (f) shows the failure cases when some object categories have too less training data to cover\nenough variety. There are only 54 bags and 39 caps in the whole dataset for the two categories shown here.\n\nFigure 24. Examples of semantic segmentation and object detection. First row is input point cloud, where walls and ceiling are hided\nfor clarity. Second and third rows are prediction and ground-truth of semantic segmentation on points, where points belonging to different\nsemantic regions are colored differently (chairs in red, tables in purple, sofa in orange, board in gray, bookcase in green, ﬂoors in blue,\nwindows in violet, beam in yellow, column in magenta, doors in khaki and clutters in black). The last two rows are object detection with\nbounding boxes, where predicted boxes are from connected components based on semantic segmentation prediction.",
    "abstract": "PointNet: Deep Learning on Point Sets for 3D Classiﬁcation and Segmentation\nCharles R. Qi*\nHao Su*\nKaichun Mo\nLeonidas J. Guibas\nStanford University",
    "introduction": "In this paper we explore deep learning architectures\ncapable of reasoning about 3D geometric data such as\npoint clouds or meshes. Typical convolutional architectures\nrequire highly regular input data formats, like those of\nimage grids or 3D voxels, in order to perform weight\nsharing and other kernel optimizations. Since point clouds\nor meshes are not in a regular format, most researchers\ntypically transform such data to regular 3D voxel grids or\ncollections of images (e.g, views) before feeding them to\na deep net architecture. This data representation transformation, however, renders the resulting data unnecessarily\nvoluminous - while also introducing quantization artifacts\nthat can obscure natural invariances of the data.\nFor this reason we focus on a different input representation for 3D geometry using simply point clouds\n- and name our resulting deep nets PointNets.\nPoint\nclouds are simple and uniﬁed structures that avoid the\ncombinatorial irregularities and complexities of meshes,\nand thus are easier to learn from. The PointNet, however,\n* indicates equal contributions.\nmug?\ntable?\ncar?\nClassification\nPart Segmentation\nPointNet\nSemantic Segmentation\nInput Point Cloud (point set representation)\nFigure 1. Applications of PointNet. We propose a novel deep net\narchitecture that consumes raw point cloud (set of points) without\nvoxelization or rendering. It is a uniﬁed architecture that learns\nboth global and local point features, providing a simple, efﬁcient\nand effective approach for a number of 3D recognition tasks.\nstill has to respect the fact that a point cloud is just a\nset of points and therefore invariant to permutations of its\nmembers, necessitating certain symmetrizations in the net\ncomputation. Further invariances to rigid motions also need\nto be considered.\nOur PointNet is a uniﬁed architecture that directly\ntakes point clouds as input and outputs either class labels\nfor the entire input or per point segment/part labels for\neach point of the input.\nThe basic architecture of our\nnetwork is surprisingly simple as in the initial stages each\npoint is processed identically and independently.\nIn the\nbasic setting each point is represented by just its three\ncoordinates (x, y, z). Additional dimensions may be added\nby computing normals and other local or global features.\nKey to our approach is the use of a single symmetric\nfunction, max pooling.\nEffectively the network learns a\nset of optimization functions/criteria that select interesting\nor informative points of the point cloud and encode the\nreason for their selection. The ﬁnal fully connected layers\nof the network aggregate these learnt optimal values into the\nglobal descriptor for the entire shape as mentioned above\n(shape classiﬁcation) or are used to predict per point labels\n(shape segmentation).\nOur input format is easy to apply rigid or afﬁne transformations to, as each point transforms independently. Thus\nwe can add a data-dependent spatial transformer network\nthat attempts to canonicalize the data before the PointNet\nprocesses them, so as to further improve the results.\narXiv:1612.00593v2 [cs.CV] 10 Apr 2017\n\nWe provide both a theoretical analysis and an experimental evaluation of our approach.\nWe show that\nour network can approximate any set function that is\ncontinuous. More interestingly, it turns out that our network\nlearns to summarize an input point cloud by a sparse set of\nkey points, which roughly corresponds to the skeleton of\nobjects according to visualization. The theoretical analysis\nprovides an understanding why our PointNet is highly\nrobust to small perturbation of input points as well as\nto corruption through point insertion (outliers) or deletion\n(missing data).\nOn a number of benchmark datasets ranging from shape\nclassiﬁcation, part segmentation to scene segmentation,\nwe experimentally compare our PointNet with state-ofthe-art approaches based upon multi-view and volumetric\nrepresentations. Under a uniﬁed architecture, not only is\nour PointNet much faster in speed, but it also exhibits strong\nperformance on par or even better than state of the art.\nThe key contributions of our work are as follows:\n• We design a novel deep net architecture suitable for\nconsuming unordered point sets in 3D;\n• We show how such a net can be trained to perform\n3D shape classiﬁcation, shape part segmentation and\nscene semantic parsing tasks;\n• We provide thorough empirical and theoretical analysis on the stability and efﬁciency of our method;\n• We illustrate the 3D features computed by the selected\nneurons in the net and develop intuitive explanations\nfor its performance.\nThe problem of processing unordered sets by neural nets\nis a very general and fundamental problem - we expect that\nour ideas can be transferred to other domains as well.",
    "literature_review": "",
    "methodology": "",
    "results": "",
    "discussion": "",
    "conclusion": "In this work, we propose a novel deep neural network\nPointNet that directly consumes point cloud. Our network\nprovides a uniﬁed approach to a number of 3D recognition\ntasks including object classiﬁcation, part segmentation and\nsemantic segmentation, while obtaining on par or better\nresults than state of the arts on standard benchmarks. We\nalso provide theoretical analysis and visualizations towards\nunderstanding of our network.\nAcknowledgement.\nThe authors gratefully acknowledge\nthe support of a Samsung GRO grant, ONR MURI N0001413-1-0341 grant, NSF grant IIS-1528025, a Google Focused Research Award, a gift from the Adobe corporation\nand hardware donations by NVIDIA.",
    "references": "[1] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis,\nM. Fischer, and S. Savarese.\n3d semantic parsing of\nlarge-scale indoor spaces.\nIn Proceedings of the IEEE\nInternational Conference on Computer Vision and Pattern\nRecognition, 2016. 6, 7\n[2] M. Aubry, U. Schlickewei, and D. Cremers.\nThe wave\nkernel signature: A quantum mechanical approach to shape\nanalysis. In Computer Vision Workshops (ICCV Workshops),\n2011 IEEE International Conference on, pages 1626-1633.\nIEEE, 2011. 2\n[3] M. M. Bronstein and I. Kokkinos.\nScale-invariant heat\nkernel signatures for non-rigid shape recognition.\nIn\nComputer Vision and Pattern Recognition (CVPR), 2010\nIEEE Conference on, pages 1704-1711. IEEE, 2010. 2\n[4] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral\nnetworks and locally connected networks on graphs. arXiv\npreprint arXiv:1312.6203, 2013. 2\n[5] D.-Y. Chen, X.-P. Tian, Y.-T. Shen, and M. Ouhyoung. On\nvisual similarity based 3d model retrieval.\nIn Computer\ngraphics forum, volume 22, pages 223-232. Wiley Online\nLibrary, 2003. 2\n[6] Y. Fang, J. Xie, G. Dai, M. Wang, F. Zhu, T. Xu, and\nE. Wong.\n3d deep shape descriptor.\nIn Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 2319-2328, 2015. 2\n[7] M. Gschwandtner, R. Kwitt, A. Uhl, and W. Pree. BlenSor:\nBlender Sensor Simulation Toolbox Advances in Visual\nComputing.\nvolume 6939 of Lecture Notes in Computer\nScience, chapter 20, pages 199-208. Springer Berlin /\nHeidelberg, Berlin, Heidelberg, 2011. 6\n[8] K. Guo, D. Zou, and X. Chen.\n3d mesh labeling via\ndeep convolutional neural networks. ACM Transactions on\nGraphics (TOG), 35(1):3, 2015. 2\n[9] M. Jaderberg, K. Simonyan, A. Zisserman, et al.\nSpatial\ntransformer networks. In NIPS 2015. 4\n[10] A. E. Johnson and M. Hebert. Using spin images for efﬁcient\nobject recognition in cluttered 3d scenes. IEEE Transactions\non pattern analysis and machine intelligence, 21(5):433-\n449, 1999. 2\n[11] M. Kazhdan, T. Funkhouser, and S. Rusinkiewicz. Rotation\ninvariant spherical harmonic representation of 3 d shape descriptors. In Symposium on geometry processing, volume 6,\npages 156-164, 2003. 5\n[12] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. 13\n[13] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas.\nFpnn:\nField probing neural networks for 3d data. arXiv preprint\narXiv:1605.06240, 2016. 2\n[14] H. Ling and D. W. Jacobs. Shape classiﬁcation using the\ninner-distance. IEEE transactions on pattern analysis and\nmachine intelligence, 29(2):286-299, 2007. 2\n[15] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne.\nJournal of Machine Learning Research, 9(Nov):2579-2605,\n2008. 15\n[16] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst.\nGeodesic convolutional neural networks on riemannian manifolds. In Proceedings of the IEEE International Conference\non Computer Vision Workshops, pages 37-45, 2015. 2\n[17] D. Maturana and S. Scherer. Voxnet: A 3d convolutional\nneural network for real-time object recognition. In IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems,\nSeptember 2015. 2, 5, 10, 11\n[18] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. Guibas.\nVolumetric and multi-view cnns for object classiﬁcation on\n3d data. In Proc. Computer Vision and Pattern Recognition\n(CVPR), IEEE, 2016. 2, 5, 8\n[19] R. B. Rusu, N. Blodow, and M. Beetz. Fast point feature\nhistograms (fpfh) for 3d registration.\nIn Robotics and\nAutomation, 2009. ICRA’09. IEEE International Conference\non, pages 3212-3217. IEEE, 2009. 2\n[20] R. B. Rusu, N. Blodow, Z. C. Marton, and M. Beetz. Aligning point cloud views using persistent feature histograms.\nIn 2008 IEEE/RSJ International Conference on Intelligent\nRobots and Systems, pages 3384-3391. IEEE, 2008. 2\n[21] M. Savva, F. Yu, H. Su, M. Aono, B. Chen, D. Cohen-Or,\nW. Deng, H. Su, S. Bai, X. Bai, et al. Shrec16 track largescale 3d shape retrieval from shapenet core55. 2\n[22] P. Y. Simard, D. Steinkraus, and J. C. Platt. Best practices for\nconvolutional neural networks applied to visual document\nanalysis. In ICDAR, volume 3, pages 958-962, 2003. 13\n[23] H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller.\nMulti-view convolutional neural networks for 3d shape\nrecognition. In Proc. ICCV, to appear, 2015. 2, 5, 6, 8\n[24] J. Sun, M. Ovsjanikov, and L. Guibas.\nA concise and\nprovably informative multi-scale signature based on heat\ndiffusion. In Computer graphics forum, volume 28, pages\n1383-1392. Wiley Online Library, 2009. 2\n[25] O. Vinyals, S. Bengio, and M. Kudlur.\nOrder matters:\nSequence to sequence for sets.\narXiv preprint\narXiv:1511.06391, 2015. 2, 4, 7\n[26] D. Z. Wang and I. Posner. Voting for voting in online point\ncloud object detection. Proceedings of the Robotics: Science\nand Systems, Rome, Italy, 1317, 2015. 2\n[27] Z. Wu, R. Shou, Y. Wang, and X. Liu. Interactive shape cosegmentation via label propagation. Computers & Graphics,\n38:248-254, 2014. 6, 10\n[28] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and\nJ. Xiao. 3d shapenets: A deep representation for volumetric\nshapes. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 1912-1920, 2015. 2,\n5, 11\n[29] L. Yi, V. G. Kim, D. Ceylan, I.-C. Shen, M. Yan, H. Su,\nC. Lu, Q. Huang, A. Sheffer, and L. Guibas. A scalable active\nframework for region annotation in 3d shape collections.\nSIGGRAPH Asia, 2016. 6, 10, 18\n\nSupplementary\nA. Overview\nThis document provides additional quantitative results,\ntechnical details and more qualitative test examples to the\nmain paper.\nIn Sec B we extend the robustness test to compare\nPointNet with VoxNet on incomplete input.\nIn Sec C\nwe provide more details on neural network architectures,\ntraining parameters and in Sec D we describe our detection\npipeline in scenes. Then Sec E illustrates more applications\nof PointNet, while Sec F shows more analysis experiments.\nSec G provides a proof for our theory on PointNet. At last,\nwe show more visualization results in Sec H.\nB. Comparison between PointNet and VoxNet\n(Sec 5.2)\nWe extend the experiments in Sec 5.2 Robustness Test\nto compare PointNet and VoxNet [17] (a representative\narchitecture for volumetric representation) on robustness to\nmissing data in the input point cloud. Both networks are\ntrained on the same train test split with 1024 number of\npoints as input. For VoxNet we voxelize the point cloud\nto 32 × 32 × 32 occupancy grids and augment the training\ndata by random rotation around up-axis and jittering.\nAt test time, input points are randomly dropped out\nby a certain ratio.\nAs VoxNet is sensitive to rotations,\nits prediction uses average scores from 12 viewpoints of\na point cloud.\nAs shown in Fig 8, we see that our\nPointNet is much more robust to missing points. VoxNet’s\naccuracy dramatically drops when half of the input points\nare missing, from 86.3% to 46.0% with a 40.3% difference,\nwhile our PointNet only has a 3.7% performance drop. This\ncan be explained by the theoretical analysis and explanation\nof our PointNet - it is learning to use a collection of critical\npoints to summarize the shape, thus it is very robust to\nmissing data.\nC. Network Architecture and Training Details\n(Sec 5.1)\nPointNet Classiﬁcation Network\nAs the basic architecture is already illustrated in the main paper, here we\nprovides more details on the joint alignment/transformation\nnetwork and training parameters.\nThe ﬁrst transformation network is a mini-PointNet that\ntakes raw point cloud as input and regresses to a 3 × 3\nmatrix.\nIt’s composed of a shared MLP(64, 128, 1024)\nnetwork (with layer output sizes 64, 128, 1024) on each\npoint, a max pooling across points and two fully connected\nlayers with output sizes 512, 256.\nThe output matrix is\ninitialized as an identity matrix. All layers, except the last\none, include ReLU and batch normalization. The second\nPointNet\nVoxNet\n87.1\n86.3\n0.5\n83.3\n0.75\n18.5\n0.875\n59.2\n13.3\n0.9375\n33.2\n10.2\n20 \n60 \n100 \n0.2 \n0.4 \n0.6 \n0.8 \nAccuracy (%) \nMissing Data Ratio \nPointNet \nVoxNet \nFigure 8. PointNet v.s. VoxNet [17] on incomplete input data.\nMetric is overall classiﬁcation accurcacy on ModelNet40 test set.\nNote that VoxNet is using 12 viewpoints averaging while PointNet\nis using only one view of the point cloud. Evidently PointNet\npresents much stronger robustness to missing points.\ntransformation network has the same architecture as the ﬁrst\none except that the output is a 64 × 64 matrix. The matrix\nis also initialized as an identity. A regularization loss (with\nweight 0.001) is added to the softmax classiﬁcation loss to\nmake the matrix close to orthogonal.\nWe use dropout with keep ratio 0.7 on the last fully\nconnected layer, whose output dimension 256, before class\nscore prediction. The decay rate for batch normalization\nstarts with 0.5 and is gradually increased to 0.99. We use\nadam optimizer with initial learning rate 0.001, momentum\n0.9 and batch size 32. The learning rate is divided by 2\nevery 20 epochs. Training on ModelNet takes 3-6 hours to\nconverge with TensorFlow and a GTX1080 GPU.\nPointNet Segmentation Network\nThe segmentation network is an extension to the classiﬁcation PointNet. Local\npoint features (the output after the second transformation\nnetwork) and global feature (output of the max pooling)\nare concatenated for each point. No dropout is used for\nsegmentation network. Training parameters are the same\nas the classiﬁcation network.\nAs to the task of shape part segmentation, we made\na few modiﬁcations to the basic segmentation network\narchitecture (Fig 2 in main paper) in order to achieve best\nperformance, as illustrated in Fig 9.\nWe add a one-hot\nvector indicating the class of the input and concatenate it\nwith the max pooling layer’s output.\nWe also increase\nneurons in some layers and add skip links to collect local\npoint features in different layers and concatenate them to\nform point feature input to the segmentation network.\nWhile [27] and [29] deal with each object category\nindependently, due to the lack of training data for some\ncategories (the total number of shapes for all the categories\nin the data set are shown in the ﬁrst line), we train our\nPointNet across categories (but with one-hot vector input to\nindicate category). To allow fair comparison, when testing\n\ninput points\npoint features\noutput scores\nmax\npool\nshared \nshared \nshared \nnx3\nnx3\nnx64\nnx64\nnx1024\nn x 1088\nnx128\nmlp (64,128,128)\nmlp (64,128,1024)\ninput\ntransform\nfeature\ntransform\nmlp\n(512,256,k)\nglobal feature\nmlp (512,256,128)\nT-Net\nmatrix\nmultiply\n3x3\ntransform\nT-Net\nmatrix\nmultiply\n64x64\ntransform\nshared \nmlp (128,m)\noutput scores\nnxm\nk\nClassification Network\nSegmentation Network\nnx3\nnx3\nT1\nnx64\nnx128\nnx128\nnx128\nT2\nnx512\nnx2048\nnx64\nnx128\nnx128\nnx128\nnx512\nn x 3024\n(256,256,128)\nnx50\none-hot\ninput points\npart scores\nFigure 9. Network architecture for part segmentation. T1 and\nT2 are alignment/transformation networks for input points and\nfeatures. FC is fully connected layer operating on each point. MLP\nis multi-layer perceptron on each point. One-hot is a vector of size\n16 indicating category of the input shape.\n32 filters \nof stride 1\n32\n32\n32\n5\n32 filters \nof stride 1\n32 filters \nof stride 1\n32 filters \nof stride 1\n32 filters \nof stride 1\n1\n64 filters \nof stride 1\n1\n64 filters \nof stride 1\n32\n1\n50 filters \nof stride 1\nin-category\nprediction\nFigure 10. Baseline 3D CNN segmentation network.\nThe\nnetwork is fully convolutional and predicts part scores for each\nvoxel.\nthese two models, we only predict part labels for the given\nspeciﬁc object category.\nAs to semantic segmentation task, we used the architecture as in Fig 2 in the main paper.\nIt takes around six to twelve hours to train the model on\nShapeNet part dataset and around half a day to train on the\nStanford semantic parsing dataset.\nBaseline 3D CNN Segmentation Network\nIn ShapeNet\npart segmentation experiment, we compare our proposed\nsegmentation version PointNet to two traditional methods\nas well as a 3D volumetric CNN network baseline.\nIn\nFig 10, we show the baseline 3D volumetric CNN network\nwe use. We generalize the well-known 3D CNN architectures, such as VoxNet [17] and 3DShapeNets [28] to a fully\nconvolutional 3D CNN segmentation network.\nFor a given point cloud, we ﬁrst convert it to the volumetric representation as a occupancy grid with resolution\n32 × 32 × 32. Then, ﬁve 3D convolution operations each\nwith 32 output channels and stride of 1 are sequentially\napplied to extract features. The receptive ﬁeld is 19 for each\nvoxel. Finally, a sequence of 3D convolutional layers with\nkernel size 1 × 1 × 1 is appended to the computed feature\nmap to predict segmentation label for each voxel. ReLU and\nbatch normalization are used for all layers except the last\none. The network is trained across categories, however, in\norder to compare with other baseline methods where object\ncategory is given, we only consider output scores in the\ngiven object category.\nD. Details on Detection Pipeline (Sec 5.1)\nWe build a simple 3D object detection system based on\nthe semantic segmentation results and our object classiﬁcation PointNet.\nWe use connected component with segmentation scores\nto get object proposals in scenes. Starting from a random\npoint in the scene, we ﬁnd its predicted label and use\nBFS to search nearby points with the same label, with\na search radius of 0.2 meter.\nIf the resulted cluster has\nmore than 200 points (assuming a 4096 point sample in\na 1m by 1m area), the cluster’s bounding box is marked\nas one object proposal.\nFor each proposed object, it’s\ndetection score is computed as the average point score for\nthat category. Before evaluation, proposals with extremely\nsmall areas/volumes are pruned.\nFor tables, chairs and\nsofas, the bounding boxes are extended to the ﬂoor in case\nthe legs are separated with the seat/surface.\nWe observe that in some rooms such as auditoriums\nlots of objects (e.g. chairs) are close to each other, where\nconnected component would fail to correctly segment out\nindividual ones. Therefore we leverage our classiﬁcation\nnetwork and uses sliding shape method to alleviate the\nproblem for the chair class. We train a binary classiﬁcation\nnetwork for each category and use the classiﬁer for sliding\nwindow detection.\nThe resulted boxes are pruned by\nnon-maximum suppression.\nThe proposed boxes from\nconnected component and sliding shapes are combined for\nﬁnal evaluation.\nIn Fig 11, we show the precision-recall curves for object\ndetection. We trained six models, where each one of them\nis trained on ﬁve areas and tested on the left area. At test\nphase, each model is tested on the area it has never seen.\nThe test results for all six areas are aggregated for the PR\ncurve generation.\nE. More Applications (Sec 5.1)\nModel Retrieval from Point Cloud\nOur PointNet learns\na global shape signature for every given input point cloud.\nWe expect geometrically similar shapes have similar global\nsignature.\nIn this section, we test our conjecture on the\nshape retrieval application. To be more speciﬁc, for every\ngiven query shape from ModelNet test split, we compute\nits global signature (output of the layer before the score\nprediction layer) given by our classiﬁcation PointNet and\nretrieve similar shapes in the train split by nearest neighbor\nsearch. Results are shown in Fig 12.\n\nFigure 11. Precision-recall curves for object detection in 3D\npoint cloud. We evaluated on all six areas for four categories:\ntable, chair, sofa and board. IoU threshold is 0.5 in volume.\nQuery\nPoint Cloud\nTop-5 Retrieval CAD Models\nFigure 12. Model retrieval from point cloud.\nFor every\ngiven point cloud, we retrieve the top-5 similar shapes from the\nModelNet test split. From top to bottom rows, we show examples\nof chair, plant, nightstand and bathtub queries. Retrieved results\nthat are in wrong category are marked by red boxes.\nShape Correspondence\nIn this section, we show that\npoint features learnt by PointNet can be potentially used\nto compute shape correspondences. Given two shapes, we\ncompute the correspondence between their critical point\nsets CS’s by matching the pairs of points that activate\nthe same dimensions in the global features.\nFig 13 and\nFig 14 show the detected shape correspondence between\ntwo similar chairs and tables.\nF. More Architecture Analysis (Sec 5.2)\nEffects of Bottleneck Dimension and Number of Input\nPoints\nHere we show our model’s performance change\nwith regard to the size of the ﬁrst max layer output as\nwell as the number of input points. In Fig 15 we see that\nperformance grows as we increase the number of points\nhowever it saturates at around 1K points. The max layer\nsize plays an important role, increasing the layer size from\nFigure 13. Shape correspondence between two chairs. For the\nclarity of the visualization, we only show 20 randomly picked\ncorrespondence pairs.\nFigure 14. Shape correspondence between two tables. For the\nclarity of the visualization, we only show 20 randomly picked\ncorrespondence pairs.\n64 to 1024 results in a 2−4% performance gain. It indicates\nthat we need enough point feature functions to cover the 3D\nspace in order to discriminate different shapes.\nIt’s worth notice that even with 64 points as input\n(obtained from furthest point sampling on meshes), our\nnetwork can achieve decent performance.\n82 \n84 \n86 \n88 \n200 \n600 \n1000 \nAccuracy (%) \nBottleneck size \n128 \n1024 \n#points\nFigure 15. Effects of bottleneck size and number of input\npoints. The metric is overall classiﬁcation accuracy on ModelNet40 test set.\nMNIST Digit Classiﬁcation\nWhile we focus on 3D point\ncloud learning, a sanity check experiment is to apply our\nnetwork on a 2D point clouds - pixel sets.\nTo convert an MNIST image into a 2D point set we\nthreshold pixel values and add the pixel (represented as a\n\npoint with (x, y) coordinate in the image) with values larger\nthan 128 to the set. We use a set size of 256. If there are\nmore than 256 pixels int he set, we randomly sub-sample it;\nif there are less, we pad the set with the one of the pixels in\nthe set (due to our max operation, which point to use for the\npadding will not affect outcome).\nAs seen in Table 7, we compare with a few baselines\nincluding multi-layer perceptron that considers input image\nas an ordered vector, a RNN that consider input as sequence\nfrom pixel (0,0) to pixel (27,27), and a vanilla version CNN.\nWhile the best performing model on MNIST is still well\nengineered CNNs (achieving less than 0.3% error rate),\nit’s interesting to see that our PointNet model can achieve\nreasonable performance by considering image as a 2D point\nset.\ninput\nerror (%)\nMulti-layer perceptron [22]\nvector\n1.60\nLeNet5 [12]\nimage\n0.80\nOurs PointNet\npoint set\n0.78\nTable 7. MNIST classiﬁcation results. We compare with vanilla\nversions of other deep architectures to show that our network based\non point sets input is achieving reasonable performance on this\ntraditional task.\nNormal Estimation\nIn segmentation version of PointNet,\nlocal point features and global feature are concatenated\nin order to provide context to local points.\nHowever,\nit’s unclear whether the context is learnt through this\nconcatenation. In this experiment, we validate our design\nby showing that our segmentation network can be trained\nto predict point normals, a local geometric property that is\ndetermined by a point’s neighborhood.\nWe train a modiﬁed version of our segmentation PointNet in a supervised manner to regress to the groundtruth point normals. We just change the last layer of our\nsegmentation PointNet to predict normal vector for each\npoint. We use absolute value of cosine distance as loss.\nFig. 16 compares our PointNet normal prediction results\n(the left columns) to the ground-truth normals computed\nfrom the mesh (the right columns).\nWe observe a\nreasonable normal reconstruction.\nOur predictions are\nmore smooth and continuous than the ground-truth which\nincludes ﬂipped normal directions in some region.\nSegmentation Robustness\nAs discussed in Sec 5.2 and\nSec B, our PointNet is less sensitive to data corruption and\nmissing points for classiﬁcation tasks since the global shape\nfeature is extracted from a collection of critical points from\nthe given input point cloud. In this section, we show that the\nrobustness holds for segmentation tasks too. The per-point\npart labels are predicted based on the combination of perpoint features and the learnt global shape feature. In Fig 17,\nGround-truth\nPrediction\nFigure 16. PointNet normal reconstrution results. In this ﬁgure,\nwe show the reconstructed normals for all the points in some\nsample point clouds and the ground-truth normals computed on\nthe mesh.\nwe illustrate the segmentation results for the given input\npoint clouds S (the left-most column), the critical point sets\nCS (the middle column) and the upper-bound shapes NS.\nNetwork Generalizability to Unseen Shape Categories\nIn Fig 18, we visualize the critical point sets and the upperbound shapes for new shapes from unseen categories (face,\nhouse, rabbit, teapot) that are not present in ModelNet or\nShapeNet. It shows that the learnt per-point functions are\ngeneralizable.\nHowever, since we train mostly on manmade objects with lots of planar structures, the reconstructed upper-bound shape in novel categories also contain\nmore planar surfaces.\nG. Proof of Theorem (Sec 4.3)\nLet X = {S : S ⊆ [0, 1] and |S| = n}.\nf : X → R is a continuous function on X w.r.t to\nHausdorff distance dH(·, ·) if the following condition is\nsatisﬁed:\n∀ϵ > 0, ∃δ > 0, for any S, S′ ∈ X, if dH(S, S′) < δ,\nthen |f(S) − f(S′)| < ϵ.\nWe show that f can be approximated arbitrarily by\ncomposing a symmetric function and a continuous function.\n\nInput Point Cloud\nCritical Point Sets\nUpper-bound Shapes\nFigure 17. The consistency of segmentation results.\nWe\nillustrate the segmentation results for some sample given point\nclouds S, their critical point sets CS and upper-bound shapes NS.\nWe observe that the shape family between the CS and NS share a\nconsistent segmentation results.\nOriginal Shape\nCritical Point Sets\nUpper-bound Shapes\nFigure 18. The critical point sets and the upper-bound shapes\nfor unseen objects. We visualize the critical point sets and the\nupper-bound shapes for teapot, bunny, hand and human body,\nwhich are not in the ModelNet or ShapeNet shape repository to\ntest the generalizability of the learnt per-point functions of our\nPointNet on other unseen objects. The images are color-coded\nto reﬂect the depth information.\nTheorem 1. Suppose f\n:\nX\n→\nR is a continuous\nset function w.r.t Hausdorff distance dH(·, ·).\n∀ϵ\n>\n0, ∃ a continuous function h and a symmetric function\ng(x1, . . . , xn) = γ◦MAX, where γ is a continuous function,\nMAX is a vector max operator that takes n vectors as input\nand returns a new vector of the element-wise maximum,\nsuch that for any S ∈ X,\n|f(S) − γ(MAX(h(x1), . . . , h(xn)))| < ϵ\nwhere x1, . . . , xn are the elements of S extracted in certain\norder,\nProof. By the continuity of f, we take δϵ so that |f(S) −\nf(S′)| < ϵ for any S, S′ ∈ X if dH(S, S′) < δϵ.\nDeﬁne K = ⌈1/δϵ⌉, which split [0, 1] into K intervals\nevenly and deﬁne an auxiliary function that maps a point to\nthe left end of the interval it lies in:\nσ(x) = ⌊Kx⌋\nK\nLet ˜S = {σ(x) : x ∈ S}, then\n|f(S) − f( ˜S)| < ϵ\nbecause dH(S, ˜S) < 1/K ≤ δϵ.\nLet hk(x) = e−d(x,[ k−1\nK , k\nK ]) be a soft indicator function\nwhere d(x, I) is the point to set (interval) distance. Let\nh(x) = [h1(x); . . . ; hK(x)], then h : R → RK.\nLet vj(x1, . . . , xn) = max{˜hj(x1), . . . , ˜hj(xn)}, indicating the occupancy of the j-th interval by points in S.\nLet v = [v1; . . . ; vK], then v : R × . . . × R\n\n\n\nn\n→ {0, 1}K\nis a symmetric function, indicating the occupancy of each\ninterval by points in S.\nDeﬁne τ : {0, 1}K → X as τ(v) = { k−1\nK\n: vk ≥ 1},\nwhich maps the occupancy vector to a set which contains\nthe left end of each occupied interval. It is easy to show:\nτ(v(x1, . . . , xn)) ≡ ˜S\nwhere x1, . . . , xn are the elements of S extracted in certain\norder.\nLet γ : RK → R be a continuous function such that\nγ(v) = f(τ(v)) for v ∈ {0, 1}K. Then,\n|γ(v(x1, . . . , xn)) − f(S)|\n=|f(τ(v(x1, . . . , xn))) − f(S)| < ϵ\nNote that γ(v(x1, . . . , xn)) can be rewritten as follows:\nγ(v(x1, . . . , xn)) =γ(MAX(h(x1), . . . , h(xn)))\n=(γ ◦ MAX)(h(x1), . . . , h(xn))\nObviously γ ◦ MAX is a symmetric function.\nNext we give the proof of Theorem 2.\nWe deﬁne\nu = MAX\nxi∈S {h(xi)} to be the sub-network of f which\nmaps a point set in [0, 1]m to a K-dimensional vector. The\nfollowing theorem tells us that small corruptions or extra\nnoise points in the input set is not likely to change the output\nof our network:\nTheorem 2. Suppose u : X\n→ RK such that u =\nxi∈S{h(xi)} and f = γ ◦ u. Then,\n\n(a) ∀S, ∃ CS, NS ⊆ X, f(T) = f(S) if CS ⊆ T ⊆ NS;\n(b) |CS| ≤ K\nProof. Obviously, ∀S ∈ X, f(S) is determined by u(S).\nSo we only need to prove that ∀S, ∃ CS, NS ⊆ X, f(T) =\nf(S) if CS ⊆ T ⊆ NS.\nFor the jth dimension as the output of u, there exists at\nleast one xj ∈ X such that hj(xj) = uj, where hj is the\njth dimension of the output vector from h. Take CS as the\nunion of all xj for j = 1, . . . , K. Then, CS satisﬁes the\nabove condition.\nAdding any additional points x such that h(x) ≤ u(S) at\nall dimensions to CS does not change u, hence f. Therefore,\nTS can be obtained adding the union of all such points to\nNS.\nFigure 19. Point function visualization.\nFor each per-point\nfunction h, we calculate the values h(p) for all the points p in a\ncube of diameter two located at the origin, which spatially covers\nthe unit sphere to which our input shapes are normalized when\ntraining our PointNet. In this ﬁgure, we visualize all the points\np that give h(p) > 0.5 with function values color-coded by the\nbrightness of the voxel. We randomly pick 15 point functions and\nvisualize the activation regions for them.\nH. More Visualizations\nClassiﬁcation Visualization\nWe use t-SNE[15] to embed\npoint cloud global signature (1024-dim) from our classiﬁcation PointNet into a 2D space. Fig 20 shows the embedding\nspace of ModelNet 40 test split shapes. Similar shapes are\nclustered together according to their semantic categories.\nSegmentation Visualization\nWe present more segmentation results on both complete CAD models and simulated\nKinect partial scans. We also visualize failure cases with\nerror analysis. Fig 21 and Fig 22 show more segmentation\nresults generated on complete CAD models and their\nsimulated Kinect scans.\nFig 23 illustrates some failure\ncases. Please read the caption for the error analysis.\nScene Semantic Parsing Visualization\nWe give a visualization of semantic parsing in Fig 24 where we show input\npoint cloud, prediction and ground truth for both semantic\nsegmentation and object detection for two ofﬁce rooms and\none conference room. The area and the rooms are unseen in\nthe training set.\nPoint Function Visualization\nOur classiﬁcation PointNet computes K (we take K = 1024 in this visualization)\ndimension point features for each point and aggregates\nall the per-point local features via a max pooling layer\ninto a single K-dim vector, which forms the global shape\ndescriptor.\nTo gain more insights on what the learnt per-point\nfunctions h’s detect, we visualize the points pi’s that\ngive high per-point function value f(pi) in Fig 19. This\nvisualization clearly shows that different point functions\nlearn to detect for points in different regions with various\nshapes scattered in the whole space.\n\nFigure 20. 2D embedding of learnt shape global features. We use t-SNE technique to visualize the learnt global shape features for the\nshapes in ModelNet40 test split.\n\nairplane\nbag\ncap\ncar\nearphone chair\nguitar\nknife\nrocket\npistol\ntable\nskate\nboard\nmotor\nbike\nmug\nlaptop\nlamp\nFigure 21. PointNet segmentation results on complete CAD models.\nairplane\nbag\ncap\ncar\nearphone chair\nguitar\nknife\nrocket\npistol\ntable\nskate\nboard\nmotor\nbike\nmug\nlaptop",
    "other": "",
    "literature": "Point Cloud Features\nMost existing features for point\ncloud are handcrafted towards speciﬁc tasks. Point features\noften encode certain statistical properties of points and are\ndesigned to be invariant to certain transformations, which\nare typically classiﬁed as intrinsic [2, 24, 3] or extrinsic\n[20, 19, 14, 10, 5]. They can also be categorized as local\nfeatures and global features. For a speciﬁc task, it is not\ntrivial to ﬁnd the optimal feature combination.\nDeep Learning on 3D Data\n3D data has multiple popular\nrepresentations, leading to various approaches for learning.\nVolumetric CNNs: [28, 17, 18] are the pioneers applying\n3D convolutional neural networks on voxelized shapes.\nHowever, volumetric representation is constrained by its\nresolution due to data sparsity and computation cost of\n3D convolution.\nFPNN [13] and Vote3D [26] proposed\nspecial methods to deal with the sparsity problem; however,\ntheir operations are still on sparse volumes, it’s challenging\nfor them to process very large point clouds.\nMultiview\nCNNs: [23, 18] have tried to render 3D point cloud or\nshapes into 2D images and then apply 2D conv nets to\nclassify them.\nWith well engineered image CNNs, this\nline of methods have achieved dominating performance on\nshape classiﬁcation and retrieval tasks [21]. However, it’s\nnontrivial to extend them to scene understanding or other\n3D tasks such as point classiﬁcation and shape completion.\nSpectral CNNs: Some latest works [4, 16] use spectral\nCNNs on meshes. However, these methods are currently\nconstrained on manifold meshes such as organic objects\nand it’s not obvious how to extend them to non-isometric\nshapes such as furniture.\nFeature-based DNNs: [6, 8]\nﬁrstly convert the 3D data into a vector, by extracting\ntraditional shape features and then use a fully connected net\nto classify the shape. We think they are constrained by the\nrepresentation power of the features extracted.\nDeep Learning on Unordered Sets\nFrom a data structure\npoint of view, a point cloud is an unordered set of vectors.\nWhile most works in deep learning focus on regular input\nrepresentations like sequences (in speech and language\nprocessing), images and volumes (video or 3D data), not\nmuch work has been done in deep learning on point sets.\nOne recent work from Oriol Vinyals et al [25] looks\ninto this problem. They use a read-process-write network\nwith attention mechanism to consume unordered input sets\nand show that their network has the ability to sort numbers.\nHowever, since their work focuses on generic sets and NLP\napplications, there lacks the role of geometry in the sets.\n3. Problem Statement\nWe design a deep learning framework that directly\nconsumes unordered point sets as inputs. A point cloud is\nrepresented as a set of 3D points {Pi| i = 1, ..., n}, where\neach point Pi is a vector of its (x, y, z) coordinate plus extra\nfeature channels such as color, normal etc. For simplicity\nand clarity, unless otherwise noted, we only use the (x, y, z)\ncoordinate as our point’s channels.\nFor the object classiﬁcation task, the input point cloud is\neither directly sampled from a shape or pre-segmented from\na scene point cloud. Our proposed deep network outputs\nk scores for all the k candidate classes.\nFor semantic\nsegmentation, the input can be a single object for part region\nsegmentation, or a sub-volume from a 3D scene for object\nregion segmentation. Our model will output n × m scores\nfor each of the n points and each of the m semantic subcategories.\n\ninput points\npoint features\noutput scores\nmax\npool\nshared \nshared \nshared \nnx3\nnx3\nnx64\nnx64\nnx1024\nn x 1088\nnx128\nmlp (64,64)\nmlp (64,128,1024)\ninput\ntransform\nfeature\ntransform\nmlp\n(512,256,k)\nglobal feature\nmlp (512,256,128)\nT-Net\nmatrix\nmultiply\n3x3\ntransform\nT-Net\nmatrix\nmultiply\n64x64\ntransform\nshared \nmlp (128,m)\noutput scores\nnxm\nk\nClassification Network\nSegmentation Network\nFigure 2. PointNet Architecture. The classiﬁcation network takes n points as input, applies input and feature transformations, and then\naggregates point features by max pooling. The output is classiﬁcation scores for k classes. The segmentation network is an extension to the\nclassiﬁcation net. It concatenates global and local features and outputs per point scores. “mlp” stands for multi-layer perceptron, numbers\nin bracket are layer sizes. Batchnorm is used for all layers with ReLU. Dropout layers are used for the last mlp in classiﬁcation net.\n4. Deep Learning on Point Sets\nThe architecture of our network (Sec 4.2) is inspired by\nthe properties of point sets in Rn (Sec 4.1).\n4.1. Properties of Point Sets in Rn\nOur input is a subset of points from an Euclidean space.\nIt has three main properties:\n• Unordered.\nUnlike pixel arrays in images or voxel\narrays in volumetric grids, point cloud is a set of points\nwithout speciﬁc order. In other words, a network that\nconsumes N 3D point sets needs to be invariant to N!\npermutations of the input set in data feeding order.\n• Interaction among points. The points are from a space\nwith a distance metric. It means that points are not\nisolated, and neighboring points form a meaningful\nsubset.\nTherefore, the model needs to be able to\ncapture local structures from nearby points, and the\ncombinatorial interactions among local structures.\n• Invariance under transformations.\nAs a geometric\nobject, the learned representation of the point set\nshould be invariant to certain transformations.\nFor\nexample, rotating and translating points all together\nshould not modify the global point cloud category nor\nthe segmentation of the points.\n4.2. PointNet Architecture\nOur full network architecture is visualized in Fig 2,\nwhere the classiﬁcation network and the segmentation\nnetwork share a great portion of structures. Please read the\ncaption of Fig 2 for the pipeline.\nOur network has three key modules: the max pooling\nlayer as a symmetric function to aggregate information from\nall the points, a local and global information combination\nstructure, and two joint alignment networks that align both\ninput points and point features.\nWe will discuss our reason behind these design choices\nin separate paragraphs below.\nSymmetry Function for Unordered Input\nIn order\nto make a model invariant to input permutation, three\nstrategies exist: 1) sort input into a canonical order; 2) treat\nthe input as a sequence to train an RNN, but augment the\ntraining data by all kinds of permutations; 3) use a simple\nsymmetric function to aggregate the information from each\npoint. Here, a symmetric function takes n vectors as input\nand outputs a new vector that is invariant to the input\norder. For example, + and ∗ operators are symmetric binary\nfunctions.\nWhile sorting sounds like a simple solution, in high\ndimensional space there in fact does not exist an ordering\nthat is stable w.r.t.\npoint perturbations in the general\nsense.\nThis can be easily shown by contradiction.\nIf\nsuch an ordering strategy exists, it deﬁnes a bijection map\nbetween a high-dimensional space and a 1d real line. It\nis not hard to see, to require an ordering to be stable w.r.t\npoint perturbations is equivalent to requiring that this map\npreserves spatial proximity as the dimension reduces, a task\nthat cannot be achieved in the general case.\nTherefore,\nsorting does not fully resolve the ordering issue, and it’s\nhard for a network to learn a consistent mapping from\ninput to output as the ordering issue persists. As shown in\nexperiments (Fig 5), we ﬁnd that applying a MLP directly\non the sorted point set performs poorly, though slightly\nbetter than directly processing an unsorted input.\nThe idea to use RNN considers the point set as a\nsequential signal and hopes that by training the RNN\n\nwith randomly permuted sequences, the RNN will become\ninvariant to input order. However in “OrderMatters” [25]\nthe authors have shown that order does matter and cannot be\ntotally omitted. While RNN has relatively good robustness\nto input ordering for sequences with small length (dozens),\nit’s hard to scale to thousands of input elements, which is\nthe common size for point sets. Empirically, we have also\nshown that model based on RNN does not perform as well\nas our proposed method (Fig 5).\nOur idea is to approximate a general function deﬁned on\na point set by applying a symmetric function on transformed\nelements in the set:\nf({x1, . . . , xn}) ≈ g(h(x1), . . . , h(xn)),\n(1)\nwhere f\n:\n2RN\n→\nR, h\n:\n→\nRK and g\n:\nRK × · · · × RK\n\n\n\nn\n→ R is a symmetric function.\nEmpirically, our basic module is very simple:\nwe\napproximate h by a multi-layer perceptron network and\ng by a composition of a single variable function and a\nmax pooling function.\nThis is found to work well by\nexperiments. Through a collection of h, we can learn a\nnumber of f’s to capture different properties of the set.\nWhile our key module seems simple, it has interesting\nproperties (see Sec 5.3) and can achieve strong performace\n(see Sec 5.1) in a few different applications. Due to the\nsimplicity of our module, we are also able to provide\ntheoretical analysis as in Sec 4.3.\nLocal and Global Information Aggregation\nThe output\nfrom the above section forms a vector [f1, . . . , fK], which\nis a global signature of the input set.\nWe can easily\ntrain a SVM or multi-layer perceptron classiﬁer on the\nshape global features for classiﬁcation.\nHowever, point\nsegmentation requires a combination of local and global\nknowledge. We can achieve this by a simple yet highly\neffective manner.\nOur solution can be seen in Fig 2 (Segmentation Network). After computing the global point cloud feature vector, we feed it back to per point features by concatenating\nthe global feature with each of the point features. Then we\nextract new per point features based on the combined point\nfeatures - this time the per point feature is aware of both the\nlocal and global information.\nWith this modiﬁcation our network is able to predict\nper point quantities that rely on both local geometry and\nglobal semantics. For example we can accurately predict\nper-point normals (ﬁg in supplementary), validating that the\nnetwork is able to summarize information from the point’s\nlocal neighborhood. In experiment session, we also show\nthat our model can achieve state-of-the-art performance on\nshape part segmentation and scene segmentation.\nJoint Alignment Network\nThe semantic labeling of a\npoint cloud has to be invariant if the point cloud undergoes\ncertain geometric transformations, such as rigid transformation. We therefore expect that the learnt representation by\nour point set is invariant to these transformations.\nA natural solution is to align all input set to a canonical\nspace before feature extraction.\nJaderberg et al. [9]\nintroduces the idea of spatial transformer to align 2D\nimages through sampling and interpolation, achieved by a\nspeciﬁcally tailored layer implemented on GPU.\nOur input form of point clouds allows us to achieve this\ngoal in a much simpler way compared with [9]. We do not\nneed to invent any new layers and no alias is introduced as in\nthe image case. We predict an afﬁne transformation matrix\nby a mini-network (T-net in Fig 2) and directly apply this\ntransformation to the coordinates of input points. The mininetwork itself resembles the big network and is composed\nby basic modules of point independent feature extraction,\nmax pooling and fully connected layers. More details about\nthe T-net are in the supplementary.\nThis idea can be further extended to the alignment of\nfeature space, as well. We can insert another alignment network on point features and predict a feature transformation\nmatrix to align features from different input point clouds.\nHowever, transformation matrix in the feature space has\nmuch higher dimension than the spatial transform matrix,\nwhich greatly increases the difﬁculty of optimization. We\ntherefore add a regularization term to our softmax training\nloss. We constrain the feature transformation matrix to be\nclose to orthogonal matrix:\nLreg = ∥I − AAT ∥2\nF ,\n(2)\nwhere A is the feature alignment matrix predicted by a\nmini-network. An orthogonal transformation will not lose\ninformation in the input, thus is desired. We ﬁnd that by\nadding the regularization term, the optimization becomes\nmore stable and our model achieves better performance.\n4.3. Theoretical Analysis\nUniversal approximation\nWe ﬁrst show the universal\napproximation ability of our neural network to continuous\nset functions. By the continuity of set functions, intuitively,\na small perturbation to the input point set should not\ngreatly change the function values, such as classiﬁcation or\nsegmentation scores.\nFormally, let X = {S : S ⊆ [0, 1]m and |S| = n}, f :\nX → R is a continuous set function on X w.r.t to Hausdorff\ndistance dH(·, ·), i.e., ∀ϵ > 0, ∃δ > 0, for any S, S′ ∈ X,\nif dH(S, S′) < δ, then |f(S) − f(S′)| < ϵ. Our theorem\nsays that f can be arbitrarily approximated by our network\ngiven enough neurons at the max pooling layer, i.e., K in\n(1) is sufﬁciently large.\n\nPartial Inputs\nComplete Inputs\nairplane\ncar\nchair\nlamp\nguitar\nmotorbike\nmug\ntable\nbag\nrocket\nearphone\nlaptop\ncap\nknife\npistol\nskateboard\nFigure 3. Qualitative results for part segmentation.\nWe\nvisualize the CAD part segmentation results across all 16 object\ncategories. We show both results for partial simulated Kinect scans\n(left block) and complete ShapeNet CAD models (right block).\nTheorem 1. Suppose f\n:\nX\n→\nR is a continuous\nset function w.r.t Hausdorff distance dH(·, ·).\n∀ϵ\n>\n0, ∃ a continuous function h and a symmetric function\ng(x1, . . . , xn) = γ ◦ MAX, such that for any S ∈ X,\nf(S) − γ\n\nxi∈S {h(xi)}\n < ϵ\nwhere x1, . . . , xn is the full list of elements in S ordered\narbitrarily, γ is a continuous function, and MAX is a vector\nmax operator that takes n vectors as input and returns a\nnew vector of the element-wise maximum.\nThe proof to this theorem can be found in our supplementary material. The key idea is that in the worst case the\nnetwork can learn to convert a point cloud into a volumetric\nrepresentation, by partitioning the space into equal-sized\nvoxels. In practice, however, the network learns a much\nsmarter strategy to probe the space, as we shall see in point\nfunction visualizations.\nBottleneck dimension and stability\nTheoretically and\nexperimentally we ﬁnd that the expressiveness of our\nnetwork is strongly affected by the dimension of the max\npooling layer, i.e., K in (1). Here we provide an analysis,\nwhich also reveals properties related to the stability of our\nmodel.\nWe deﬁne u = MAX\nxi∈S {h(xi)} to be the sub-network of f\nwhich maps a point set in [0, 1]m to a K-dimensional vector.\nThe following theorem tells us that small corruptions or\nextra noise points in the input set are not likely to change\nthe output of our network:\nTheorem 2. Suppose u : X\n→ RK such that u =\nxi∈S{h(xi)} and f = γ ◦ u. Then,\n(a) ∀S, ∃ CS, NS ⊆ X, f(T) = f(S) if CS ⊆ T ⊆ NS;\n(b) |CS| ≤ K\ninput\n#views\naccuracy\naccuracy\navg. class\noverall\nSPH [11]\nmesh\n-\n68.2\n-\n3DShapeNets [28]\nvolume\n77.3\n84.7\nVoxNet [17]\nvolume\n83.0\n85.9\nSubvolume [18]\nvolume\n86.0\n89.2\nLFD [28]\nimage\n75.5\n-\nMVCNN [23]\nimage\n90.1\n-\nOurs baseline\npoint\n-\n72.6\n77.4\nOurs PointNet\npoint\n86.2\n89.2\nTable 1. Classiﬁcation results on ModelNet40. Our net achieves\nstate-of-the-art among deep nets on 3D input.\nWe explain the implications of the theorem. (a) says that\nf(S) is unchanged up to the input corruption if all points\nin CS are preserved; it is also unchanged with extra noise\npoints up to NS. (b) says that CS only contains a bounded\nnumber of points, determined by K in (1). In other words,\nf(S) is in fact totally determined by a ﬁnite subset CS ⊆ S\nof less or equal to K elements. We therefore call CS the\ncritical point set of S and K the bottleneck dimension of f.\nCombined with the continuity of h, this explains the\nrobustness of our model w.r.t point perturbation, corruption\nand extra noise points. The robustness is gained in analogy\nto the sparsity principle in machine learning models.\nIntuitively, our network learns to summarize a shape by\na sparse set of key points. In experiment section we see\nthat the key points form the skeleton of an object.\n5. Experiment\nExperiments are divided into four parts. First, we show\nPointNets can be applied to multiple 3D recognition tasks\n(Sec 5.1).\nSecond, we provide detailed experiments to\nvalidate our network design (Sec 5.2). At last we visualize\nwhat the network learns (Sec 5.3) and analyze time and\nspace complexity (Sec 5.4).\n5.1. Applications\nIn this section we show how our network can be\ntrained to perform 3D object classiﬁcation, object part\nsegmentation and semantic scene segmentation 1.\nEven\nthough we are working on a brand new data representation\n(point sets), we are able to achieve comparable or even\nbetter performance on benchmarks for several tasks.\n3D Object Classiﬁcation\nOur network learns global\npoint cloud feature that can be used for object classiﬁcation.\nWe evaluate our model on the ModelNet40 [28] shape\nclassiﬁcation benchmark. There are 12,311 CAD models\nfrom 40 man-made object categories, split into 9,843 for\n1More application examples such as correspondence and point cloud\nbased CAD model retrieval are included in supplementary material.\n\nmean\naero\nbag\ncap\ncar\nchair\near\nguitar knife\nlamp\nlaptop motor\nmug pistol rocket skate\ntable\nphone\nboard\n# shapes\n76\n898\n69\n392\n451\n184 283\n152\nWu [27]\n-\n63.2\n-\n-\n-\n73.5\n-\n-\n-\n74.4\n-\n-\n-\n-\n-\n-\n74.8\nYi [29]\n81.4\n81.0\n78.4\n77.7\n75.7\n87.6\n61.9\n92.0\n85.4\n82.5\n95.7\n70.6\n91.9 85.9\n53.1\n69.8\n75.3\n3DCNN\n79.4\n75.1\n72.8\n73.3\n70.0\n87.2\n63.5\n88.4\n79.6\n74.4\n93.9\n58.7\n91.8 76.4\n51.2\n65.3\n77.1\nOurs\n83.7\n83.4\n78.7\n82.5\n74.9\n89.6\n73.0\n91.5\n85.9\n80.8\n95.3\n65.2\n93.0 81.2\n57.9\n72.8\n80.6\nTable 2. Segmentation results on ShapeNet part dataset. Metric is mIoU(%) on points. We compare with two traditional methods [27]\nand [29] and a 3D fully convolutional network baseline proposed by us. Our PointNet method achieved the state-of-the-art in mIoU.\ntraining and 2,468 for testing.\nWhile previous methods\nfocus on volumetric and mult-view image representations,\nwe are the ﬁrst to directly work on raw point cloud.\nWe uniformly sample 1024 points on mesh faces according to face area and normalize them into a unit sphere.\nDuring training we augment the point cloud on-the-ﬂy by\nrandomly rotating the object along the up-axis and jitter the\nposition of each points by a Gaussian noise with zero mean\nand 0.02 standard deviation.\nIn Table 1, we compare our model with previous works\nas well as our baseline using MLP on traditional features\nextracted from point cloud (point density, D2, shape contour\netc.).\nOur model achieved state-of-the-art performance\namong methods based on 3D input (volumetric and point\ncloud). With only fully connected layers and max pooling,\nour net gains a strong lead in inference speed and can be\neasily parallelized in CPU as well. There is still a small\ngap between our method and multi-view based method\n(MVCNN [23]), which we think is due to the loss of ﬁne\ngeometry details that can be captured by rendered images.\n3D Object Part Segmentation\nPart segmentation is a\nchallenging ﬁne-grained 3D recognition task. Given a 3D\nscan or a mesh model, the task is to assign part category\nlabel (e.g. chair leg, cup handle) to each point or face.\nWe evaluate on ShapeNet part data set from [29], which\ncontains 16,881 shapes from 16 categories, annotated with\n50 parts in total. Most object categories are labeled with\ntwo to ﬁve parts. Ground truth annotations are labeled on\nsampled points on the shapes.\nWe formulate part segmentation as a per-point classiﬁcation problem. Evaluation metric is mIoU on points. For\neach shape S of category C, to calculate the shape’s mIoU:\nFor each part type in category C, compute IoU between\ngroundtruth and prediction. If the union of groundtruth and\nprediction points is empty, then count part IoU as 1. Then\nwe average IoUs for all part types in category C to get mIoU\nfor that shape. To calculate mIoU for the category, we take\naverage of mIoUs for all shapes in that category.\nIn this section, we compare our segmentation version\nPointNet (a modiﬁed version of Fig 2, Segmentation\nNetwork) with two traditional methods [27] and [29] that\nboth take advantage of point-wise geometry features and\ncorrespondences between shapes, as well as our own\n3D CNN baseline.\nSee supplementary for the detailed\nmodiﬁcations and network architecture for the 3D CNN.\nIn Table 2, we report per-category and mean IoU(%)\nscores. We observe a 2.3% mean IoU improvement and our\nnet beats the baseline methods in most categories.\nWe also perform experiments on simulated Kinect scans\nto test the robustness of these methods. For every CAD\nmodel in the ShapeNet part data set, we use Blensor Kinect\nSimulator [7] to generate incomplete point clouds from six\nrandom viewpoints. We train our PointNet on the complete\nshapes and partial scans with the same network architecture\nand training setting. Results show that we lose only 5.3%\nmean IoU. In Fig 3, we present qualitative results on both\ncomplete and partial data. One can see that though partial\ndata is fairly challenging, our predictions are reasonable.\nSemantic Segmentation in Scenes\nOur network on part\nsegmentation can be easily extended to semantic scene\nsegmentation, where point labels become semantic object\nclasses instead of object part labels.\nWe experiment on the Stanford 3D semantic parsing data\nset [1].\nThe dataset contains 3D scans from Matterport\nscanners in 6 areas including 271 rooms. Each point in the\nscan is annotated with one of the semantic labels from 13\ncategories (chair, table, ﬂoor, wall etc. plus clutter).\nTo prepare training data, we ﬁrstly split points by room,\nand then sample rooms into blocks with area 1m by 1m.\nWe train our segmentation version of PointNet to predict\nmean IoU\noverall accuracy\nOurs baseline\n20.12\n53.19\nOurs PointNet\n47.71\n78.62\nTable 3. Results on semantic segmentation in scenes. Metric is\naverage IoU over 13 classes (structural and furniture elements plus\nclutter) and classiﬁcation accuracy calculated on points.\ntable\nchair\nsofa\nboard\nmean\n# instance\n1363\n137\nArmeni et al. [1]\n46.02\n16.15\n6.78\n3.91\n18.22\nOurs\n46.67\n33.80\n4.76\n11.72\n24.24\nTable 4. Results on 3D object detection in scenes. Metric is\naverage precision with threshold IoU 0.5 computed in 3D volumes.\n\nInput\nOutput\nFigure 4. Qualitative results for semantic segmentation. Top\nrow is input point cloud with color. Bottom row is output semantic\nsegmentation result (on points) displayed in the same camera\nviewpoint as input.\nper point class in each block. Each point is represented by\na 9-dim vector of XYZ, RGB and normalized location as\nto the room (from 0 to 1). At training time, we randomly\nsample 4096 points in each block on-the-ﬂy. At test time,\nwe test on all the points. We follow the same protocol as [1]\nto use k-fold strategy for train and test.\nWe compare our method with a baseline using handcrafted point features. The baseline extracts the same 9dim local features and three additional ones: local point\ndensity, local curvature and normal. We use standard MLP\nas the classiﬁer.\nResults are shown in Table 3, where\nour PointNet method signiﬁcantly outperforms the baseline\nmethod. In Fig 4, we show qualitative segmentation results.\nOur network is able to output smooth predictions and is\nrobust to missing points and occlusions.\nBased on the semantic segmentation output from our\nnetwork, we further build a 3D object detection system\nusing connected component for object proposal (see supplementary for details). We compare with previous stateof-the-art method in Table 4. The previous method is based\non a sliding shape method (with CRF post processing) with\nSVMs trained on local geometric features and global room\ncontext feature in voxel grids. Our method outperforms it\nby a large margin on the furniture categories reported.\n5.2. Architecture Design Analysis\nIn this section we validate our design choices by control\nexperiments. We also show the effects of our network’s\nhyperparameters.\nComparison with Alternative Order-invariant Methods\nAs mentioned in Sec 4.2, there are at least three options for\nconsuming unordered set inputs. We use the ModelNet40\nshape classiﬁcation problem as a test bed for comparisons\nof those options, the following two control experiment will\nalso use this task.\nThe baselines (illustrated in Fig 5) we compared with\ninclude multi-layer perceptron on unsorted and sorted\n(1,2,3)\n(2,3,4)\n(1,3,1)\nrnn \ncell\nrnn \ncell\nrnn \ncell\n...\n(1,2,3)\n(2,3,4)\n(1,3,1)\n...\n...\n(1,2,3)\n(1,3,1)\n(2,3,4)\n...\nsorting\nsequential model\nsymmetry function\nsorted\nFigure 5. Three approaches to achieve order invariance. Multilayer perceptron (MLP) applied on points consists of 5 hidden\nlayers with neuron sizes 64,64,64,128,1024, all points share a\nsingle copy of MLP. The MLP close to the output consists of two\nlayers with sizes 512,256.\npoints as n×3 arrays, RNN model that considers input point\nas a sequence, and a model based on symmetry functions.\nThe symmetry operation we experimented include max\npooling, average pooling and an attention based weighted\nsum. The attention method is similar to that in [25], where\na scalar score is predicted from each point feature, then the\nscore is normalized across points by computing a softmax.\nThe weighted sum is then computed on the normalized\nscores and the point features. As shown in Fig 5, maxpooling operation achieves the best performance by a large\nwinning margin, which validates our choice.\nEffectiveness of Input and Feature Transformations\nIn\nTable 5 we demonstrate the positive effects of our input\nand feature transformations (for alignment). It’s interesting\nto see that the most basic architecture already achieves\nquite reasonable results. Using input transformation gives\na 0.8% performance boost.\nThe regularization loss is\nnecessary for the higher dimension transform to work.\nBy combining both transformations and the regularization\nterm, we achieve the best performance.\nRobustness Test\nWe show our PointNet, while simple\nand effective, is robust to various kinds of input corruptions.\nWe use the same architecture as in Fig 5’s max pooling\nnetwork. Input points are normalized into a unit sphere.\nResults are in Fig 6.\nAs to missing points, when there are 50% points missing,\nthe accuracy only drops by 2.4% and 3.8% w.r.t. furthest\nand random input sampling. Our net is also robust to outlier\nTransform\naccuracy\nnone\n87.1\ninput (3x3)\n87.9\nfeature (64x64)\n86.9\nfeature (64x64) + reg.\n87.4\nboth\n89.2\nTable 5. Effects of input feature transforms. Metric is overall\nclassiﬁcation accuracy on ModelNet40 test set.\n\n30 \n50 \n70 \n90 \n0.05 \n0.1 \nAccuracy (%) \nPerturbation noise std \n40 \n60 \n80 \n100 \n0.2 \n0.4 \n0.6 \n0.8 \nAccuracy (%) \nMissing data ratio \nFurthest \nRandom \n30 \n50 \n70 \n90 \n0.1 \n0.2 \n0.3 \n0.4 \n0.5 \nAccuracy (%) \nOutlier ratio \nXYZ+density \nFigure 6. PointNet robustness test.\nThe metric is overall\nclassiﬁcation accuracy on ModelNet40 test set.\nLeft: Delete\npoints. Furthest means the original 1024 points are sampled with\nfurthest sampling. Middle: Insertion. Outliers uniformly scattered\nin the unit sphere. Right: Perturbation. Add Gaussian noise to\neach point independently.\npoints, if it has seen those during training. We evaluate two\nmodels: one trained on points with (x, y, z) coordinates; the\nother on (x, y, z) plus point density. The net has more than\n80% accuracy even when 20% of the points are outliers.\nFig 6 right shows the net is robust to point perturbations.\n5.3. Visualizing PointNet\nIn Fig 7, we visualize critical point sets CS and upperbound shapes NS (as discussed in Thm 2) for some sample\nshapes S. The point sets between the two shapes will give\nexactly the same global shape feature f(S).\nWe can see clearly from Fig 7 that the critical point\nsets CS, those contributed to the max pooled feature,\nsummarizes the skeleton of the shape. The upper-bound\nshapes NS illustrates the largest possible point cloud that\ngive the same global shape feature f(S) as the input point\ncloud S. CS and NS reﬂect the robustness of PointNet,\nmeaning that losing some non-critical points does not\nchange the global shape signature f(S) at all.\nThe NS is constructed by forwarding all the points in a\nedge-length-2 cube through the network and select points p\nwhose point function values (h1(p), h2(p), · · · , hK(p)) are\nno larger than the global shape descriptor.\nOriginal Shape\nCritical Point Sets\nUpper-bound Shapes\nFigure 7. Critical points and upper bound shape. While critical\npoints jointly determine the global shape feature for a given shape,\nany point cloud that falls between the critical points set and the\nupper bound shape gives exactly the same feature. We color-code\nall ﬁgures to show the depth information.\n5.4. Time and Space Complexity Analysis\nTable 6 summarizes space (number of parameters in\nthe network) and time (ﬂoating-point operations/sample)\ncomplexity of our classiﬁcation PointNet. We also compare\nPointNet to a representative set of volumetric and multiview based architectures in previous works.\nWhile MVCNN [23] and Subvolume (3D CNN) [18]\nachieve high performance, PointNet is orders more efﬁcient\nin computational cost (measured in FLOPs/sample: 141x\nand 8x more efﬁcient, respectively).\nBesides, PointNet\nis much more space efﬁcient than MVCNN in terms of\n#param in the network (17x less parameters). Moreover,\nPointNet is much more scalable - it’s space and time\ncomplexity is O(N) - linear in the number of input points.\nHowever, since convolution dominates computing time,\nmulti-view method’s time complexity grows squarely on\nimage resolution and volumetric convolution based method\ngrows cubically with the volume size.\nEmpirically, PointNet is able to process more than\none million points per second for point cloud classiﬁcation (around 1K objects/second) or semantic segmentation\n(around 2 rooms/second) with a 1080X GPU on TensorFlow, showing great potential for real-time applications.\n#params\nFLOPs/sample\nPointNet (vanilla)\n0.8M\n148M\nPointNet\n3.5M\n440M\nSubvolume [18]\n16.6M\n3633M\nMVCNN [23]\n60.0M\n62057M\nTable 6. Time and space complexity of deep architectures for\n3D data classiﬁcation.\nPointNet (vanilla) is the classiﬁcation\nPointNet without input and feature transformations.\nstands for ﬂoating-point operation. The “M” stands for million.\nSubvolume and MVCNN used pooling on input data from multiple\nrotations or views, without which they have much inferior\nperformance."
  }
}