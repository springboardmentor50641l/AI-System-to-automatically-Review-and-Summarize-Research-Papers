{
  "file_name": "Xception_Deep_Learning_with_Depthwise_Separable_Co_20260129.pdf",
  "file_path": "C:\\Users\\ASUS\\Documents\\GitHub\\AI-System-to-automatically-Review-and-Summarize-Research-Papers\\data\\papers\\Xception_Deep_Learning_with_Depthwise_Separable_Co_20260129.pdf",
  "file_size": 804466,
  "extraction_date": "2026-01-29T22:01:09.612929",
  "pdf_type": "text_based",
  "total_pages": 8,
  "total_chars": 28999,
  "total_words": 4401,
  "extraction_method": "pymupdf",
  "pages": [
    {
      "page_number": 1,
      "text": "Xception: Deep Learning with Depthwise Separable Convolutions\nFranc¸ois Chollet\nGoogle, Inc.\n\nAbstract\nWe present an interpretation of Inception modules in convolutional neural networks as being an intermediate step\nin-between regular convolution and the depthwise separable\nconvolution operation (a depthwise convolution followed by\na pointwise convolution). In this light, a depthwise separable\nconvolution can be understood as an Inception module with\na maximally large number of towers. This observation leads\nus to propose a novel deep convolutional neural network\narchitecture inspired by Inception, where Inception modules\nhave been replaced with depthwise separable convolutions.\nWe show that this architecture, dubbed Xception, slightly\noutperforms Inception V3 on the ImageNet dataset (which\nInception V3 was designed for), and signiﬁcantly outperforms Inception V3 on a larger image classiﬁcation dataset\ncomprising 350 million images and 17,000 classes. Since\nthe Xception architecture has the same number of parameters as Inception V3, the performance gains are not due\nto increased capacity but rather to a more efﬁcient use of\nmodel parameters.\n1. Introduction\nConvolutional neural networks have emerged as the master algorithm in computer vision in recent years, and developing recipes for designing them has been a subject of\nconsiderable attention. The history of convolutional neural\nnetwork design started with LeNet-style models [10], which\nwere simple stacks of convolutions for feature extraction\nand max-pooling operations for spatial sub-sampling. In\n2012, these ideas were reﬁned into the AlexNet architecture [9], where convolution operations were being repeated\nmultiple times in-between max-pooling operations, allowing\nthe network to learn richer features at every spatial scale.\nWhat followed was a trend to make this style of network\nincreasingly deeper, mostly driven by the yearly ILSVRC\ncompetition; ﬁrst with Zeiler and Fergus in 2013 [25] and\nthen with the VGG architecture in 2014 [18].\nAt this point a new style of network emerged, the Inception architecture, introduced by Szegedy et al. in 2014 [20]\nas GoogLeNet (Inception V1), later reﬁned as Inception V2\n[7], Inception V3 [21], and most recently Inception-ResNet\n[19]. Inception itself was inspired by the earlier NetworkIn-Network architecture [11]. Since its ﬁrst introduction,\nInception has been one of the best performing family of\nmodels on the ImageNet dataset [14], as well as internal\ndatasets in use at Google, in particular JFT [5].\nThe fundamental building block of Inception-style models is the Inception module, of which several different versions exist. In ﬁgure 1 we show the canonical form of an\nInception module, as found in the Inception V3 architecture. An Inception model can be understood as a stack of\nsuch modules. This is a departure from earlier VGG-style\nnetworks which were stacks of simple convolution layers.\nWhile Inception modules are conceptually similar to convolutions (they are convolutional feature extractors), they\nempirically appear to be capable of learning richer representations with less parameters. How do they work, and\nhow do they differ from regular convolutions? What design\nstrategies come after Inception?\n1.1. The Inception hypothesis\nA convolution layer attempts to learn ﬁlters in a 3D space,\nwith 2 spatial dimensions (width and height) and a channel dimension; thus a single convolution kernel is tasked\nwith simultaneously mapping cross-channel correlations and\nspatial correlations.\nThis idea behind the Inception module is to make this\nprocess easier and more efﬁcient by explicitly factoring it\ninto a series of operations that would independently look at\ncross-channel correlations and at spatial correlations. More\nprecisely, the typical Inception module ﬁrst looks at crosschannel correlations via a set of 1x1 convolutions, mapping\nthe input data into 3 or 4 separate spaces that are smaller than\nthe original input space, and then maps all correlations in\nthese smaller 3D spaces, via regular 3x3 or 5x5 convolutions.\nThis is illustrated in ﬁgure 1. In effect, the fundamental hypothesis behind Inception is that cross-channel correlations\nand spatial correlations are sufﬁciently decoupled that it is\npreferable not to map them jointly 1.\n1A variant of the process is to independently look at width-wise correarXiv:1610.02357v3 [cs.CV] 4 Apr 2017",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": false,
      "image_count": 0
    },
    {
      "page_number": 2,
      "text": "Consider a simpliﬁed version of an Inception module that\nonly uses one size of convolution (e.g. 3x3) and does not\ninclude an average pooling tower (ﬁgure 2). This Inception module can be reformulated as a large 1x1 convolution\nfollowed by spatial convolutions that would operate on nonoverlapping segments of the output channels (ﬁgure 3). This\nobservation naturally raises the question: what is the effect of the number of segments in the partition (and their\nsize)? Would it be reasonable to make a much stronger\nhypothesis than the Inception hypothesis, and assume that\ncross-channel correlations and spatial correlations can be\nmapped completely separately?\nFigure 1. A canonical Inception module (Inception V3).\nFigure 2. A simpliﬁed Inception module.\n1.2. The continuum between convolutions and separable convolutions\nAn “extreme” version of an Inception module, based on\nthis stronger hypothesis, would ﬁrst use a 1x1 convolution to\nmap cross-channel correlations, and would then separately\nmap the spatial correlations of every output channel. This\nis shown in ﬁgure 4. We remark that this extreme form of\nan Inception module is almost identical to a depthwise separable convolution, an operation that has been used in neural\nlations and height-wise correlations. This is implemented by some of the\nmodules found in Inception V3, which alternate 7x1 and 1x7 convolutions.\nThe use of such spatially separable convolutions has a long history in image processing and has been used in some convolutional neural network\nimplementations since at least 2012 (possibly earlier).\nFigure 3. A strictly equivalent reformulation of the simpliﬁed Inception module.\nFigure 4. An “extreme” version of our Inception module, with one\nspatial convolution per output channel of the 1x1 convolution.\nnetwork design as early as 2014 [15] and has become more\npopular since its inclusion in the TensorFlow framework [1]\nin 2016.\nA depthwise separable convolution, commonly called\n“separable convolution” in deep learning frameworks such as\nTensorFlow and Keras, consists in a depthwise convolution,\ni.e. a spatial convolution performed independently over each\nchannel of an input, followed by a pointwise convolution,\ni.e. a 1x1 convolution, projecting the channels output by the\ndepthwise convolution onto a new channel space. This is\nnot to be confused with a spatially separable convolution,\nwhich is also commonly called “separable convolution” in\nthe image processing community.\nTwo minor differences between and “extreme” version of\nan Inception module and a depthwise separable convolution\nwould be:\n• The order of the operations: depthwise separable convolutions as usually implemented (e.g. in TensorFlow)\nperform ﬁrst channel-wise spatial convolution and then\nperform 1x1 convolution, whereas Inception performs\nthe 1x1 convolution ﬁrst.\n• The presence or absence of a non-linearity after the\nﬁrst operation. In Inception, both operations are followed by a ReLU non-linearity, however depthwise",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 4
    },
    {
      "page_number": 3,
      "text": "separable convolutions are usually implemented without non-linearities.\nWe argue that the ﬁrst difference is unimportant, in particular because these operations are meant to be used in a\nstacked setting. The second difference might matter, and we\ninvestigate it in the experimental section (in particular see\nﬁgure 10).\nWe also note that other intermediate formulations of Inception modules that lie in between regular Inception modules and depthwise separable convolutions are also possible:\nin effect, there is a discrete spectrum between regular convolutions and depthwise separable convolutions, parametrized\nby the number of independent channel-space segments used\nfor performing spatial convolutions. A regular convolution\n(preceded by a 1x1 convolution), at one extreme of this\nspectrum, corresponds to the single-segment case; a depthwise separable convolution corresponds to the other extreme\nwhere there is one segment per channel; Inception modules\nlie in between, dividing a few hundreds of channels into 3\nor 4 segments. The properties of such intermediate modules\nappear not to have been explored yet.\nHaving made these observations, we suggest that it may\nbe possible to improve upon the Inception family of architectures by replacing Inception modules with depthwise separable convolutions, i.e. by building models that would be\nstacks of depthwise separable convolutions. This is made\npractical by the efﬁcient depthwise convolution implementation available in TensorFlow. In what follows, we present a\nconvolutional neural network architecture based on this idea,\nwith a similar number of parameters as Inception V3, and\nwe evaluate its performance against Inception V3 on two\nlarge-scale image classiﬁcation task.\n2. Prior work\nThe present work relies heavily on prior efforts in the\nfollowing areas:\n• Convolutional neural networks [10, 9, 25], in particular\nthe VGG-16 architecture [18], which is schematically\nsimilar to our proposed architecture in a few respects.\n• The Inception architecture family of convolutional neural networks [20, 7, 21, 19], which ﬁrst demonstrated\nthe advantages of factoring convolutions into multiple\nbranches operating successively on channels and then\non space.\n• Depthwise separable convolutions, which our proposed\narchitecture is entirely based upon. While the use of spatially separable convolutions in neural networks has a\nlong history, going back to at least 2012 [12] (but likely\neven earlier), the depthwise version is more recent. Laurent Sifre developed depthwise separable convolutions\nduring an internship at Google Brain in 2013, and used\nthem in AlexNet to obtain small gains in accuracy and\nlarge gains in convergence speed, as well as a signiﬁcant\nreduction in model size. An overview of his work was\nﬁrst made public in a presentation at ICLR 2014 [23].\nDetailed experimental results are reported in Sifre’s thesis, section 6.2 [15]. This initial work on depthwise separable convolutions was inspired by prior research from\nSifre and Mallat on transformation-invariant scattering\n[16, 15]. Later, a depthwise separable convolution was\nused as the ﬁrst layer of Inception V1 and Inception\nV2 [20, 7]. Within Google, Andrew Howard [6] has\nintroduced efﬁcient mobile models called MobileNets\nusing depthwise separable convolutions. Jin et al. in\n2014 [8] and Wang et al. in 2016 [24] also did related\nwork aiming at reducing the size and computational\ncost of convolutional neural networks using separable\nconvolutions. Additionally, our work is only possible\ndue to the inclusion of an efﬁcient implementation of\ndepthwise separable convolutions in the TensorFlow\nframework [1].\n• Residual connections, introduced by He et al. in [4],\nwhich our proposed architecture uses extensively.\n3. The Xception architecture\nWe propose a convolutional neural network architecture\nbased entirely on depthwise separable convolution layers.\nIn effect, we make the following hypothesis: that the mapping of cross-channels correlations and spatial correlations\nin the feature maps of convolutional neural networks can be\nentirely decoupled. Because this hypothesis is a stronger version of the hypothesis underlying the Inception architecture,\nwe name our proposed architecture Xception, which stands\nfor “Extreme Inception”.\nA complete description of the speciﬁcations of the network is given in ﬁgure 5. The Xception architecture has\n36 convolutional layers forming the feature extraction base\nof the network. In our experimental evaluation we will exclusively investigate image classiﬁcation and therefore our\nconvolutional base will be followed by a logistic regression\nlayer. Optionally one may insert fully-connected layers before the logistic regression layer, which is explored in the\nexperimental evaluation section (in particular, see ﬁgures\n7 and 8). The 36 convolutional layers are structured into\n14 modules, all of which have linear residual connections\naround them, except for the ﬁrst and last modules.\nIn short, the Xception architecture is a linear stack of\ndepthwise separable convolution layers with residual connections. This makes the architecture very easy to deﬁne\nand modify; it takes only 30 to 40 lines of code using a highlevel library such as Keras [2] or TensorFlow-Slim [17], not\nunlike an architecture such as VGG-16 [18], but rather un-",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": false,
      "image_count": 0
    },
    {
      "page_number": 4,
      "text": "like architectures such as Inception V2 or V3 which are far\nmore complex to deﬁne. An open-source implementation of\nXception using Keras and TensorFlow is provided as part of\nthe Keras Applications module2, under the MIT license.\n4. Experimental evaluation\nWe choose to compare Xception to the Inception V3 architecture, due to their similarity of scale: Xception and\nInception V3 have nearly the same number of parameters\n(table 3), and thus any performance gap could not be attributed to a difference in network capacity. We conduct\nour comparison on two image classiﬁcation tasks: one is\nthe well-known 1000-class single-label classiﬁcation task on\nthe ImageNet dataset [14], and the other is a 17,000-class\nmulti-label classiﬁcation task on the large-scale JFT dataset.\n4.1. The JFT dataset\nJFT is an internal Google dataset for large-scale image\nclassiﬁcation dataset, ﬁrst introduced by Hinton et al. in [5],\nwhich comprises over 350 million high-resolution images\nannotated with labels from a set of 17,000 classes. To evaluate the performance of a model trained on JFT, we use an\nauxiliary dataset, FastEval14k.\nFastEval14k is a dataset of 14,000 images with dense\nannotations from about 6,000 classes (36.5 labels per image on average). On this dataset we evaluate performance\nusing Mean Average Precision for top 100 predictions\n and we weight the contribution of each class\nto  with a score estimating how common (and\ntherefore important) the class is among social media images.\nThis evaluation procedure is meant to capture performance\non frequently occurring labels from social media, which is\ncrucial for production models at Google.\n4.2. Optimization conﬁguration\nA different optimization conﬁguration was used for ImageNet and JFT:\n• On ImageNet:\n- Optimizer: SGD\n- Momentum: 0.9\n- Initial learning rate: 0.045\n- Learning rate decay: decay of rate 0.94 every 2\nepochs\n• On JFT:\n- Optimizer: RMSprop [22]\n- Momentum: 0.9\n- Initial learning rate: 0.001\n2\n- Learning rate decay: decay of rate 0.9 every\n3,000,000 samples\nFor both datasets, the same exact same optimization conﬁguration was used for both Xception and Inception V3.\nNote that this conﬁguration was tuned for best performance\nwith Inception V3; we did not attempt to tune optimization\nhyperparameters for Xception. Since the networks have different training proﬁles (ﬁgure 6), this may be suboptimal, especially on the ImageNet dataset, on which the optimization\nconﬁguration used had been carefully tuned for Inception\nV3.\nAdditionally, all models were evaluated using Polyak\naveraging [13] at inference time.\n4.3. Regularization conﬁguration\n• Weight decay: The Inception V3 model uses a weight\ndecay (L2 regularization) rate of 4e − 5, which has\nbeen carefully tuned for performance on ImageNet. We\nfound this rate to be quite suboptimal for Xception\nand instead settled for 1e − 5. We did not perform\nan extensive search for the optimal weight decay rate.\nThe same weight decay rates were used both for the\nImageNet experiments and the JFT experiments.\n• Dropout: For the ImageNet experiments, both models\ninclude a dropout layer of rate 0.5 before the logistic\nregression layer. For the JFT experiments, no dropout\nwas included due to the large size of the dataset which\nmade overﬁtting unlikely in any reasonable amount of\ntime.\n• Auxiliary loss tower: The Inception V3 architecture\nmay optionally include an auxiliary tower which backpropagates the classiﬁcation loss earlier in the network,\nserving as an additional regularization mechanism. For\nsimplicity, we choose not to include this auxiliary tower\nin any of our models.\n4.4. Training infrastructure\nAll networks were implemented using the TensorFlow\nframework [1] and trained on 60 NVIDIA K80 GPUs each.\nFor the ImageNet experiments, we used data parallelism\nwith synchronous gradient descent to achieve the best classiﬁcation performance, while for JFT we used asynchronous\ngradient descent so as to speed up training. The ImageNet\nexperiments took approximately 3 days each, while the JFT\nexperiments took over one month each. The JFT models\nwere not trained to full convergence, which would have\ntaken over three month per experiment.",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": false,
      "image_count": 0
    },
    {
      "page_number": 5,
      "text": "Figure 5. The Xception architecture: the data ﬁrst goes through the entry ﬂow, then through the middle ﬂow which is repeated eight times,\nand ﬁnally through the exit ﬂow. Note that all Convolution and SeparableConvolution layers are followed by batch normalization [7] (not\nincluded in the diagram). All SeparableConvolution layers use a depth multiplier of 1 (no depth expansion).\n4.5. Comparison with Inception V3\n4.5.1\nClassiﬁcation performance\nAll evaluations were run with a single crop of the inputs\nimages and a single model. ImageNet results are reported\non the validation set rather than the test set (i.e. on the\nnon-blacklisted images from the validation set of ILSVRC\n2012). JFT results are reported after 30 million iterations\n(one month of training) rather than after full convergence.\nResults are provided in table 1 and table 2, as well as ﬁgure\n6, ﬁgure 7, ﬁgure 8. On JFT, we tested both versions of our\nnetworks that did not include any fully-connected layers, and\nversions that included two fully-connected layers of 4096\nunits each before the logistic regression layer.\nOn ImageNet, Xception shows marginally better results\nthan Inception V3. On JFT, Xception shows a 4.3% relative improvement on the FastEval14k  metric.\nWe also note that Xception outperforms ImageNet results\nreported by He et al. for ResNet-50, ResNet-101 and ResNet152 [4].\nTable 1. Classiﬁcation performance comparison on ImageNet (single crop, single model). VGG-16 and ResNet-152 numbers are\nonly included as a reminder. The version of Inception V3 being\nbenchmarked does not include the auxiliary tower.\nTop-1 accuracy\nTop-5 accuracy\nVGG-16\n0.715\n0.901\nResNet-152\n0.770\n0.933\nInception V3\n0.782\n0.941\nXception\n0.790\n0.945\nThe Xception architecture shows a much larger performance improvement on the JFT dataset compared to the\nImageNet dataset. We believe this may be due to the fact\nthat Inception V3 was developed with a focus on ImageNet\nand may thus be by design over-ﬁt to this speciﬁc task. On\nthe other hand, neither architecture was tuned for JFT. It is\nlikely that a search for better hyperparameters for Xception\non ImageNet (in particular optimization parameters and reg-",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 1
    },
    {
      "page_number": 6,
      "text": "Table 2. Classiﬁcation performance comparison on JFT (single\ncrop, single model).\nFastEval14k \nInception V3 - no FC layers\n6.36\nXception - no FC layers\n6.70\nInception V3 with FC layers\n6.50\nXception with FC layers\n6.78\nFigure 6. Training proﬁle on ImageNet\nFigure 7. Training proﬁle on JFT, without fully-connected layers\nularization parameters) would yield signiﬁcant additional\nimprovement.\n4.5.2\nSize and speed\nTable 3. Size and training speed comparison.\nParameter count\nSteps/second\nInception V3\n23,626,728\nXception\n22,855,952\nIn table 3 we compare the size and speed of Inception\nFigure 8. Training proﬁle on JFT, with fully-connected layers\nV3 and Xception. Parameter count is reported on ImageNet\n(1000 classes, no fully-connected layers) and the number of\ntraining steps (gradient updates) per second is reported on\nImageNet with 60 K80 GPUs running synchronous gradient\ndescent. Both architectures have approximately the same\nsize (within 3.5%), and Xception is marginally slower. We\nexpect that engineering optimizations at the level of the\ndepthwise convolution operations can make Xception faster\nthan Inception V3 in the near future. The fact that both\narchitectures have almost the same number of parameters\nindicates that the improvement seen on ImageNet and JFT\ndoes not come from added capacity but rather from a more\nefﬁcient use of the model parameters.\n4.6. Effect of the residual connections\nFigure 9. Training proﬁle with and without residual connections.\nTo quantify the beneﬁts of residual connections in the\nXception architecture, we benchmarked on ImageNet a modiﬁed version of Xception that does not include any residual",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 4
    },
    {
      "page_number": 7,
      "text": "connections. Results are shown in ﬁgure 9. Residual connections are clearly essential in helping with convergence,\nboth in terms of speed and ﬁnal classiﬁcation performance.\nHowever we will note that benchmarking the non-residual\nmodel with the same optimization conﬁguration as the residual model may be uncharitable and that better optimization\nconﬁgurations might yield more competitive results.\nAdditionally, let us note that this result merely shows the\nimportance of residual connections for this speciﬁc architecture, and that residual connections are in no way required\nin order to build models that are stacks of depthwise separable convolutions. We also obtained excellent results with\nnon-residual VGG-style models where all convolution layers\nwere replaced with depthwise separable convolutions (with\na depth multiplier of 1), superior to Inception V3 on JFT at\nequal parameter count.\n4.7. Effect of an intermediate activation after pointwise convolutions\nFigure 10. Training proﬁle with different activations between the\ndepthwise and pointwise operations of the separable convolution\nlayers.\nWe mentioned earlier that the analogy between depthwise separable convolutions and Inception modules suggests\nthat depthwise separable convolutions should potentially include a non-linearity between the depthwise and pointwise\noperations. In the experiments reported so far, no such nonlinearity was included. However we also experimentally\ntested the inclusion of either ReLU or ELU [3] as intermediate non-linearity. Results are reported on ImageNet in ﬁgure\n10, and show that the absence of any non-linearity leads to\nboth faster convergence and better ﬁnal performance.\nThis is a remarkable observation, since Szegedy et al. report the opposite result in [21] for Inception modules. It may\nbe that the depth of the intermediate feature spaces on which\nspatial convolutions are applied is critical to the usefulness\nof the non-linearity: for deep feature spaces (e.g. those\nfound in Inception modules) the non-linearity is helpful, but\nfor shallow ones (e.g. the 1-channel deep feature spaces\nof depthwise separable convolutions) it becomes harmful,\npossibly due to a loss of information.\n5. Future directions\nWe noted earlier the existence of a discrete spectrum between regular convolutions and depthwise separable convolutions, parametrized by the number of independent channelspace segments used for performing spatial convolutions. Inception modules are one point on this spectrum. We showed\nin our empirical evaluation that the extreme formulation of\nan Inception module, the depthwise separable convolution,\nmay have advantages over regular a regular Inception module. However, there is no reason to believe that depthwise\nseparable convolutions are optimal. It may be that intermediate points on the spectrum, lying between regular Inception\nmodules and depthwise separable convolutions, hold further\nadvantages. This question is left for future investigation.\n6. Conclusions\nWe showed how convolutions and depthwise separable\nconvolutions lie at both extremes of a discrete spectrum,\nwith Inception modules being an intermediate point in between. This observation has led to us to propose replacing\nInception modules with depthwise separable convolutions in\nneural computer vision architectures. We presented a novel\narchitecture based on this idea, named Xception, which has\na similar parameter count as Inception V3. Compared to\nInception V3, Xception shows small gains in classiﬁcation\nperformance on the ImageNet dataset and large gains on the\nJFT dataset. We expect depthwise separable convolutions\nto become a cornerstone of convolutional neural network\narchitecture design in the future, since they offer similar\nproperties as Inception modules, yet are as easy to use as\nregular convolution layers.\nReferences\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,\nR. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,\nR. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,\nJ. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. Warden,\nM. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorﬂow.org.\n[2] F. Chollet. Keras.  2015.\n[3] D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and\naccurate deep network learning by exponential linear units\n(elus). arXiv preprint arXiv:1511.07289, 2015.",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": true,
      "image_count": 1
    },
    {
      "page_number": 8,
      "text": "[4] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition.\narXiv preprint arXiv:1512.03385,\n2015.\n[5] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge\nin a neural network, 2015.\n[6] A. Howard. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. Forthcoming.\n[7] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift.\nIn Proceedings of The 32nd International Conference on\nMachine Learning, pages 448-456, 2015.\n[8] J. Jin, A. Dundar, and E. Culurciello. Flattened convolutional\nneural networks for feedforward acceleration. arXiv preprint\narXiv:1412.5474, 2014.\n[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\nclassiﬁcation with deep convolutional neural networks. In\nAdvances in neural information processing systems, pages\n1097-1105, 2012.\n[10] Y. LeCun, L. Jackel, L. Bottou, C. Cortes, J. S. Denker,\nH. Drucker, I. Guyon, U. Muller, E. Sackinger, P. Simard,\net al. Learning algorithms for classiﬁcation: A comparison on\nhandwritten digit recognition. Neural networks: the statistical\nmechanics perspective, 261:276, 1995.\n[11] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv\npreprint arXiv:1312.4400, 2013.\n[12] F. Mamalet and C. Garcia. Simplifying ConvNets for Fast\nLearning. In International Conference on Artiﬁcial Neural\nNetworks (ICANN 2012), pages 58-65. Springer, 2012.\n[13] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM J. Control Optim.,\n30(4):838-855, July 1992.\n[14] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. 2014.\n[15] L. Sifre. Rigid-motion scattering for image classiﬁcation,\n2014. Ph.D. thesis.\n[16] L. Sifre and S. Mallat. Rotation, scaling and deformation\ninvariant scattering for texture discrimination. In 2013 IEEE\nConference on Computer Vision and Pattern Recognition,\nPortland, OR, USA, June 23-28, 2013, pages 1233-1240,\n2013.\n[17] N. Silberman and S. Guadarrama. Tf-slim, 2016.\n[18] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014.\n[19] C. Szegedy, S. Ioffe, and V. Vanhoucke.\nInception-v4,\ninception-resnet and the impact of residual connections on\nlearning. arXiv preprint arXiv:1602.07261, 2016.\n[20] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper\nwith convolutions. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 1-9, 2015.\n[21] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.\nRethinking the inception architecture for computer vision.\narXiv preprint arXiv:1512.00567, 2015.\n[22] T. Tieleman and G. Hinton. Divide the gradient by a running average of its recent magnitude. COURSERA: Neural\nNetworks for Machine Learning, 4, 2012. Accessed: 201511-05.\n[23] V. Vanhoucke. Learning visual representations at scale. ICLR,\n2014.\n[24] M. Wang, B. Liu, and H. Foroosh. Factorized convolutional\nneural networks. arXiv preprint arXiv:1608.04337, 2016.\n[25] M. D. Zeiler and R. Fergus. Visualizing and understanding\nconvolutional networks. In Computer Vision-ECCV 2014,\npages 818-833. Springer, 2014.",
      "method": "pymupdf",
      "page_width": 612.0,
      "page_height": 792.0,
      "has_images": false,
      "image_count": 0
    }
  ],
  "sections": {
    "title": "(elus). arXiv preprint arXiv:1511.07289, 2015.\n\n[4] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition.\narXiv preprint arXiv:1512.03385,\n2015.\n[5] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge\nin a neural network, 2015.\n[6] A. Howard. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. Forthcoming.\n[7] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift.\nIn Proceedings of The 32nd International Conference on\nMachine Learning, pages 448-456, 2015.\n[8] J. Jin, A. Dundar, and E. Culurciello. Flattened convolutional\nneural networks for feedforward acceleration. arXiv preprint\narXiv:1412.5474, 2014.\n[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\nclassiﬁcation with deep convolutional neural networks. In\nAdvances in neural information processing systems, pages\n1097-1105, 2012.\n[10] Y. LeCun, L. Jackel, L. Bottou, C. Cortes, J. S. Denker,\nH. Drucker, I. Guyon, U. Muller, E. Sackinger, P. Simard,\net al. Learning algorithms for classiﬁcation: A comparison on\nhandwritten digit recognition. Neural networks: the statistical\nmechanics perspective, 261:276, 1995.\n[11] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv\npreprint arXiv:1312.4400, 2013.\n[12] F. Mamalet and C. Garcia. Simplifying ConvNets for Fast\nLearning. In International Conference on Artiﬁcial Neural\nNetworks (ICANN 2012), pages 58-65. Springer, 2012.\n[13] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM J. Control Optim.,\n30(4):838-855, July 1992.\n[14] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. 2014.\n[15] L. Sifre. Rigid-motion scattering for image classiﬁcation,\n2014. Ph.D. thesis.\n[16] L. Sifre and S. Mallat. Rotation, scaling and deformation\ninvariant scattering for texture discrimination. In 2013 IEEE\nConference on Computer Vision and Pattern Recognition,\nPortland, OR, USA, June 23-28, 2013, pages 1233-1240,\n2013.\n[17] N. Silberman and S. Guadarrama. Tf-slim, 2016.\n[18] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014.\n[19] C. Szegedy, S. Ioffe, and V. Vanhoucke.\nInception-v4,\ninception-resnet and the impact of residual connections on\nlearning. arXiv preprint arXiv:1602.07261, 2016.\n[20] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper\nwith convolutions. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 1-9, 2015.\n[21] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.\nRethinking the inception architecture for computer vision.\narXiv preprint arXiv:1512.00567, 2015.\n[22] T. Tieleman and G. Hinton. Divide the gradient by a running average of its recent magnitude. COURSERA: Neural\nNetworks for Machine Learning, 4, 2012. Accessed: 201511-05.\n[23] V. Vanhoucke. Learning visual representations at scale. ICLR,\n2014.\n[24] M. Wang, B. Liu, and H. Foroosh. Factorized convolutional\nneural networks. arXiv preprint arXiv:1608.04337, 2016.\n[25] M. D. Zeiler and R. Fergus. Visualizing and understanding\nconvolutional networks. In Computer Vision-ECCV 2014,\npages 818-833. Springer, 2014.",
    "abstract": "Xception: Deep Learning with Depthwise Separable Convolutions\nFranc¸ois Chollet\nGoogle, Inc.",
    "introduction": "Convolutional neural networks have emerged as the master algorithm in computer vision in recent years, and developing recipes for designing them has been a subject of\nconsiderable attention. The history of convolutional neural\nnetwork design started with LeNet-style models [10], which\nwere simple stacks of convolutions for feature extraction\nand max-pooling operations for spatial sub-sampling. In\n2012, these ideas were reﬁned into the AlexNet architecture [9], where convolution operations were being repeated\nmultiple times in-between max-pooling operations, allowing\nthe network to learn richer features at every spatial scale.\nWhat followed was a trend to make this style of network\nincreasingly deeper, mostly driven by the yearly ILSVRC\ncompetition; ﬁrst with Zeiler and Fergus in 2013 [25] and\nthen with the VGG architecture in 2014 [18].\nAt this point a new style of network emerged, the Inception architecture, introduced by Szegedy et al. in 2014 [20]\nas GoogLeNet (Inception V1), later reﬁned as Inception V2\n[7], Inception V3 [21], and most recently Inception-ResNet\n[19]. Inception itself was inspired by the earlier NetworkIn-Network architecture [11]. Since its ﬁrst introduction,\nInception has been one of the best performing family of\nmodels on the ImageNet dataset [14], as well as internal\ndatasets in use at Google, in particular JFT [5].\nThe fundamental building block of Inception-style models is the Inception module, of which several different versions exist. In ﬁgure 1 we show the canonical form of an\nInception module, as found in the Inception V3 architecture. An Inception model can be understood as a stack of\nsuch modules. This is a departure from earlier VGG-style\nnetworks which were stacks of simple convolution layers.\nWhile Inception modules are conceptually similar to convolutions (they are convolutional feature extractors), they\nempirically appear to be capable of learning richer representations with less parameters. How do they work, and\nhow do they differ from regular convolutions? What design\nstrategies come after Inception?\n1.1. The Inception hypothesis\nA convolution layer attempts to learn ﬁlters in a 3D space,\nwith 2 spatial dimensions (width and height) and a channel dimension; thus a single convolution kernel is tasked\nwith simultaneously mapping cross-channel correlations and\nspatial correlations.\nThis idea behind the Inception module is to make this\nprocess easier and more efﬁcient by explicitly factoring it\ninto a series of operations that would independently look at\ncross-channel correlations and at spatial correlations. More\nprecisely, the typical Inception module ﬁrst looks at crosschannel correlations via a set of 1x1 convolutions, mapping\nthe input data into 3 or 4 separate spaces that are smaller than\nthe original input space, and then maps all correlations in\nthese smaller 3D spaces, via regular 3x3 or 5x5 convolutions.\nThis is illustrated in ﬁgure 1. In effect, the fundamental hypothesis behind Inception is that cross-channel correlations\nand spatial correlations are sufﬁciently decoupled that it is\npreferable not to map them jointly 1.\n1A variant of the process is to independently look at width-wise correarXiv:1610.02357v3 [cs.CV] 4 Apr 2017\n\nConsider a simpliﬁed version of an Inception module that\nonly uses one size of convolution (e.g. 3x3) and does not\ninclude an average pooling tower (ﬁgure 2). This Inception module can be reformulated as a large 1x1 convolution\nfollowed by spatial convolutions that would operate on nonoverlapping segments of the output channels (ﬁgure 3). This\nobservation naturally raises the question: what is the effect of the number of segments in the partition (and their\nsize)? Would it be reasonable to make a much stronger\nhypothesis than the Inception hypothesis, and assume that\ncross-channel correlations and spatial correlations can be\nmapped completely separately?\nFigure 1. A canonical Inception module (Inception V3).\nFigure 2. A simpliﬁed Inception module.\n1.2. The continuum between convolutions and separable convolutions\nAn “extreme” version of an Inception module, based on\nthis stronger hypothesis, would ﬁrst use a 1x1 convolution to\nmap cross-channel correlations, and would then separately\nmap the spatial correlations of every output channel. This\nis shown in ﬁgure 4. We remark that this extreme form of\nan Inception module is almost identical to a depthwise separable convolution, an operation that has been used in neural\nlations and height-wise correlations. This is implemented by some of the\nmodules found in Inception V3, which alternate 7x1 and 1x7 convolutions.\nThe use of such spatially separable convolutions has a long history in image processing and has been used in some convolutional neural network\nimplementations since at least 2012 (possibly earlier).\nFigure 3. A strictly equivalent reformulation of the simpliﬁed Inception module.\nFigure 4. An “extreme” version of our Inception module, with one\nspatial convolution per output channel of the 1x1 convolution.\nnetwork design as early as 2014 [15] and has become more\npopular since its inclusion in the TensorFlow framework [1]\nin 2016.\nA depthwise separable convolution, commonly called\n“separable convolution” in deep learning frameworks such as\nTensorFlow and Keras, consists in a depthwise convolution,\ni.e. a spatial convolution performed independently over each\nchannel of an input, followed by a pointwise convolution,\ni.e. a 1x1 convolution, projecting the channels output by the\ndepthwise convolution onto a new channel space. This is\nnot to be confused with a spatially separable convolution,\nwhich is also commonly called “separable convolution” in\nthe image processing community.\nTwo minor differences between and “extreme” version of\nan Inception module and a depthwise separable convolution\nwould be:\n• The order of the operations: depthwise separable convolutions as usually implemented (e.g. in TensorFlow)\nperform ﬁrst channel-wise spatial convolution and then\nperform 1x1 convolution, whereas Inception performs\nthe 1x1 convolution ﬁrst.\n• The presence or absence of a non-linearity after the\nﬁrst operation. In Inception, both operations are followed by a ReLU non-linearity, however depthwise\n\nseparable convolutions are usually implemented without non-linearities.\nWe argue that the ﬁrst difference is unimportant, in particular because these operations are meant to be used in a\nstacked setting. The second difference might matter, and we\ninvestigate it in the experimental section (in particular see\nﬁgure 10).\nWe also note that other intermediate formulations of Inception modules that lie in between regular Inception modules and depthwise separable convolutions are also possible:\nin effect, there is a discrete spectrum between regular convolutions and depthwise separable convolutions, parametrized\nby the number of independent channel-space segments used\nfor performing spatial convolutions. A regular convolution\n(preceded by a 1x1 convolution), at one extreme of this\nspectrum, corresponds to the single-segment case; a depthwise separable convolution corresponds to the other extreme\nwhere there is one segment per channel; Inception modules\nlie in between, dividing a few hundreds of channels into 3\nor 4 segments. The properties of such intermediate modules\nappear not to have been explored yet.\nHaving made these observations, we suggest that it may\nbe possible to improve upon the Inception family of architectures by replacing Inception modules with depthwise separable convolutions, i.e. by building models that would be\nstacks of depthwise separable convolutions. This is made\npractical by the efﬁcient depthwise convolution implementation available in TensorFlow. In what follows, we present a\nconvolutional neural network architecture based on this idea,\nwith a similar number of parameters as Inception V3, and\nwe evaluate its performance against Inception V3 on two\nlarge-scale image classiﬁcation task.\n2. Prior work\nThe present work relies heavily on prior efforts in the\nfollowing areas:\n• Convolutional neural networks [10, 9, 25], in particular\nthe VGG-16 architecture [18], which is schematically\nsimilar to our proposed architecture in a few respects.\n• The Inception architecture family of convolutional neural networks [20, 7, 21, 19], which ﬁrst demonstrated\nthe advantages of factoring convolutions into multiple\nbranches operating successively on channels and then\non space.\n• Depthwise separable convolutions, which our proposed\narchitecture is entirely based upon. While the use of spatially separable convolutions in neural networks has a\nlong history, going back to at least 2012 [12] (but likely\neven earlier), the depthwise version is more recent. Laurent Sifre developed depthwise separable convolutions\nduring an internship at Google Brain in 2013, and used\nthem in AlexNet to obtain small gains in accuracy and\nlarge gains in convergence speed, as well as a signiﬁcant\nreduction in model size. An overview of his work was\nﬁrst made public in a presentation at ICLR 2014 [23].\nDetailed experimental results are reported in Sifre’s thesis, section 6.2 [15]. This initial work on depthwise separable convolutions was inspired by prior research from\nSifre and Mallat on transformation-invariant scattering\n[16, 15]. Later, a depthwise separable convolution was\nused as the ﬁrst layer of Inception V1 and Inception\nV2 [20, 7]. Within Google, Andrew Howard [6] has\nintroduced efﬁcient mobile models called MobileNets\nusing depthwise separable convolutions. Jin et al. in\n2014 [8] and Wang et al. in 2016 [24] also did related\nwork aiming at reducing the size and computational\ncost of convolutional neural networks using separable\nconvolutions. Additionally, our work is only possible\ndue to the inclusion of an efﬁcient implementation of\ndepthwise separable convolutions in the TensorFlow\nframework [1].\n• Residual connections, introduced by He et al. in [4],\nwhich our proposed architecture uses extensively.\n3. The Xception architecture\nWe propose a convolutional neural network architecture\nbased entirely on depthwise separable convolution layers.\nIn effect, we make the following hypothesis: that the mapping of cross-channels correlations and spatial correlations\nin the feature maps of convolutional neural networks can be\nentirely decoupled. Because this hypothesis is a stronger version of the hypothesis underlying the Inception architecture,\nwe name our proposed architecture Xception, which stands\nfor “Extreme Inception”.\nA complete description of the speciﬁcations of the network is given in ﬁgure 5. The Xception architecture has\n36 convolutional layers forming the feature extraction base\nof the network. In our experimental evaluation we will exclusively investigate image classiﬁcation and therefore our\nconvolutional base will be followed by a logistic regression\nlayer. Optionally one may insert fully-connected layers before the logistic regression layer, which is explored in the\nexperimental evaluation section (in particular, see ﬁgures\n7 and 8). The 36 convolutional layers are structured into\n14 modules, all of which have linear residual connections\naround them, except for the ﬁrst and last modules.\nIn short, the Xception architecture is a linear stack of\ndepthwise separable convolution layers with residual connections. This makes the architecture very easy to deﬁne\nand modify; it takes only 30 to 40 lines of code using a highlevel library such as Keras [2] or TensorFlow-Slim [17], not\nunlike an architecture such as VGG-16 [18], but rather un-\n\nlike architectures such as Inception V2 or V3 which are far\nmore complex to deﬁne. An open-source implementation of\nXception using Keras and TensorFlow is provided as part of\nthe Keras Applications module2, under the MIT license.\n4. Experimental evaluation\nWe choose to compare Xception to the Inception V3 architecture, due to their similarity of scale: Xception and\nInception V3 have nearly the same number of parameters\n(table 3), and thus any performance gap could not be attributed to a difference in network capacity. We conduct\nour comparison on two image classiﬁcation tasks: one is\nthe well-known 1000-class single-label classiﬁcation task on\nthe ImageNet dataset [14], and the other is a 17,000-class\nmulti-label classiﬁcation task on the large-scale JFT dataset.\n4.1. The JFT dataset\nJFT is an internal Google dataset for large-scale image\nclassiﬁcation dataset, ﬁrst introduced by Hinton et al. in [5],\nwhich comprises over 350 million high-resolution images\nannotated with labels from a set of 17,000 classes. To evaluate the performance of a model trained on JFT, we use an\nauxiliary dataset, FastEval14k.\nFastEval14k is a dataset of 14,000 images with dense\nannotations from about 6,000 classes (36.5 labels per image on average). On this dataset we evaluate performance\nusing Mean Average Precision for top 100 predictions\n and we weight the contribution of each class\nto  with a score estimating how common (and\ntherefore important) the class is among social media images.\nThis evaluation procedure is meant to capture performance\non frequently occurring labels from social media, which is\ncrucial for production models at Google.\n4.2. Optimization conﬁguration\nA different optimization conﬁguration was used for ImageNet and JFT:\n• On ImageNet:\n- Optimizer: SGD\n- Momentum: 0.9\n- Initial learning rate: 0.045\n- Learning rate decay: decay of rate 0.94 every 2\nepochs\n• On JFT:\n- Optimizer: RMSprop [22]\n- Momentum: 0.9\n- Initial learning rate: 0.001\n2\n- Learning rate decay: decay of rate 0.9 every\n3,000,000 samples\nFor both datasets, the same exact same optimization conﬁguration was used for both Xception and Inception V3.\nNote that this conﬁguration was tuned for best performance\nwith Inception V3; we did not attempt to tune optimization\nhyperparameters for Xception. Since the networks have different training proﬁles (ﬁgure 6), this may be suboptimal, especially on the ImageNet dataset, on which the optimization\nconﬁguration used had been carefully tuned for Inception\nV3.\nAdditionally, all models were evaluated using Polyak\naveraging [13] at inference time.\n4.3. Regularization conﬁguration\n• Weight decay: The Inception V3 model uses a weight\ndecay (L2 regularization) rate of 4e − 5, which has\nbeen carefully tuned for performance on ImageNet. We\nfound this rate to be quite suboptimal for Xception\nand instead settled for 1e − 5. We did not perform\nan extensive search for the optimal weight decay rate.\nThe same weight decay rates were used both for the\nImageNet experiments and the JFT experiments.\n• Dropout: For the ImageNet experiments, both models\ninclude a dropout layer of rate 0.5 before the logistic\nregression layer. For the JFT experiments, no dropout\nwas included due to the large size of the dataset which\nmade overﬁtting unlikely in any reasonable amount of\ntime.\n• Auxiliary loss tower: The Inception V3 architecture\nmay optionally include an auxiliary tower which backpropagates the classiﬁcation loss earlier in the network,\nserving as an additional regularization mechanism. For\nsimplicity, we choose not to include this auxiliary tower\nin any of our models.\n4.4. Training infrastructure\nAll networks were implemented using the TensorFlow\nframework [1] and trained on 60 NVIDIA K80 GPUs each.\nFor the ImageNet experiments, we used data parallelism\nwith synchronous gradient descent to achieve the best classiﬁcation performance, while for JFT we used asynchronous\ngradient descent so as to speed up training. The ImageNet\nexperiments took approximately 3 days each, while the JFT\nexperiments took over one month each. The JFT models\nwere not trained to full convergence, which would have\ntaken over three month per experiment.\n\nFigure 5. The Xception architecture: the data ﬁrst goes through the entry ﬂow, then through the middle ﬂow which is repeated eight times,\nand ﬁnally through the exit ﬂow. Note that all Convolution and SeparableConvolution layers are followed by batch normalization [7] (not\nincluded in the diagram). All SeparableConvolution layers use a depth multiplier of 1 (no depth expansion).\n4.5. Comparison with Inception V3\n4.5.1\nClassiﬁcation performance\nAll evaluations were run with a single crop of the inputs\nimages and a single model. ImageNet results are reported\non the validation set rather than the test set (i.e. on the\nnon-blacklisted images from the validation set of ILSVRC\n2012). JFT results are reported after 30 million iterations\n(one month of training) rather than after full convergence.\nResults are provided in table 1 and table 2, as well as ﬁgure\n6, ﬁgure 7, ﬁgure 8. On JFT, we tested both versions of our\nnetworks that did not include any fully-connected layers, and\nversions that included two fully-connected layers of 4096\nunits each before the logistic regression layer.\nOn ImageNet, Xception shows marginally better results\nthan Inception V3. On JFT, Xception shows a 4.3% relative improvement on the FastEval14k  metric.\nWe also note that Xception outperforms ImageNet results\nreported by He et al. for ResNet-50, ResNet-101 and ResNet152 [4].\nTable 1. Classiﬁcation performance comparison on ImageNet (single crop, single model). VGG-16 and ResNet-152 numbers are\nonly included as a reminder. The version of Inception V3 being\nbenchmarked does not include the auxiliary tower.\nTop-1 accuracy\nTop-5 accuracy\nVGG-16\n0.715\n0.901\nResNet-152\n0.770\n0.933\nInception V3\n0.782\n0.941\nXception\n0.790\n0.945\nThe Xception architecture shows a much larger performance improvement on the JFT dataset compared to the\nImageNet dataset. We believe this may be due to the fact\nthat Inception V3 was developed with a focus on ImageNet\nand may thus be by design over-ﬁt to this speciﬁc task. On\nthe other hand, neither architecture was tuned for JFT. It is\nlikely that a search for better hyperparameters for Xception\non ImageNet (in particular optimization parameters and reg-\n\nTable 2. Classiﬁcation performance comparison on JFT (single\ncrop, single model).\nFastEval14k \nInception V3 - no FC layers\n6.36\nXception - no FC layers\n6.70\nInception V3 with FC layers\n6.50\nXception with FC layers\n6.78\nFigure 6. Training proﬁle on ImageNet\nFigure 7. Training proﬁle on JFT, without fully-connected layers\nularization parameters) would yield signiﬁcant additional\nimprovement.\n4.5.2\nSize and speed\nTable 3. Size and training speed comparison.\nParameter count\nSteps/second\nInception V3\n23,626,728\nXception\n22,855,952\nIn table 3 we compare the size and speed of Inception\nFigure 8. Training proﬁle on JFT, with fully-connected layers\nV3 and Xception. Parameter count is reported on ImageNet\n(1000 classes, no fully-connected layers) and the number of\ntraining steps (gradient updates) per second is reported on\nImageNet with 60 K80 GPUs running synchronous gradient\ndescent. Both architectures have approximately the same\nsize (within 3.5%), and Xception is marginally slower. We\nexpect that engineering optimizations at the level of the\ndepthwise convolution operations can make Xception faster\nthan Inception V3 in the near future. The fact that both\narchitectures have almost the same number of parameters\nindicates that the improvement seen on ImageNet and JFT\ndoes not come from added capacity but rather from a more\nefﬁcient use of the model parameters.\n4.6. Effect of the residual connections\nFigure 9. Training proﬁle with and without residual connections.\nTo quantify the beneﬁts of residual connections in the\nXception architecture, we benchmarked on ImageNet a modiﬁed version of Xception that does not include any residual\n\nconnections. Results are shown in ﬁgure 9. Residual connections are clearly essential in helping with convergence,\nboth in terms of speed and ﬁnal classiﬁcation performance.\nHowever we will note that benchmarking the non-residual\nmodel with the same optimization conﬁguration as the residual model may be uncharitable and that better optimization\nconﬁgurations might yield more competitive results.\nAdditionally, let us note that this result merely shows the\nimportance of residual connections for this speciﬁc architecture, and that residual connections are in no way required\nin order to build models that are stacks of depthwise separable convolutions. We also obtained excellent results with\nnon-residual VGG-style models where all convolution layers\nwere replaced with depthwise separable convolutions (with\na depth multiplier of 1), superior to Inception V3 on JFT at\nequal parameter count.\n4.7. Effect of an intermediate activation after pointwise convolutions\nFigure 10. Training proﬁle with different activations between the\ndepthwise and pointwise operations of the separable convolution\nlayers.\nWe mentioned earlier that the analogy between depthwise separable convolutions and Inception modules suggests\nthat depthwise separable convolutions should potentially include a non-linearity between the depthwise and pointwise\noperations. In the experiments reported so far, no such nonlinearity was included. However we also experimentally\ntested the inclusion of either ReLU or ELU [3] as intermediate non-linearity. Results are reported on ImageNet in ﬁgure\n10, and show that the absence of any non-linearity leads to\nboth faster convergence and better ﬁnal performance.\nThis is a remarkable observation, since Szegedy et al. report the opposite result in [21] for Inception modules. It may\nbe that the depth of the intermediate feature spaces on which\nspatial convolutions are applied is critical to the usefulness\nof the non-linearity: for deep feature spaces (e.g. those\nfound in Inception modules) the non-linearity is helpful, but\nfor shallow ones (e.g. the 1-channel deep feature spaces\nof depthwise separable convolutions) it becomes harmful,\npossibly due to a loss of information.\n5. Future directions\nWe noted earlier the existence of a discrete spectrum between regular convolutions and depthwise separable convolutions, parametrized by the number of independent channelspace segments used for performing spatial convolutions. Inception modules are one point on this spectrum. We showed\nin our empirical evaluation that the extreme formulation of\nan Inception module, the depthwise separable convolution,\nmay have advantages over regular a regular Inception module. However, there is no reason to believe that depthwise\nseparable convolutions are optimal. It may be that intermediate points on the spectrum, lying between regular Inception\nmodules and depthwise separable convolutions, hold further\nadvantages. This question is left for future investigation.",
    "literature_review": "",
    "methodology": "",
    "results": "",
    "discussion": "",
    "conclusion": "We showed how convolutions and depthwise separable\nconvolutions lie at both extremes of a discrete spectrum,\nwith Inception modules being an intermediate point in between. This observation has led to us to propose replacing\nInception modules with depthwise separable convolutions in\nneural computer vision architectures. We presented a novel\narchitecture based on this idea, named Xception, which has\na similar parameter count as Inception V3. Compared to\nInception V3, Xception shows small gains in classiﬁcation\nperformance on the ImageNet dataset and large gains on the\nJFT dataset. We expect depthwise separable convolutions\nto become a cornerstone of convolutional neural network\narchitecture design in the future, since they offer similar\nproperties as Inception modules, yet are as easy to use as\nregular convolution layers.",
    "references": "[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,\nR. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,\nR. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,\nJ. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. Warden,\nM. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorﬂow.org.\n[2] F. Chollet. Keras.  2015.\n[3] D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and",
    "other": ""
  }
}