{
  "abstract": "Convolutional networks are at the core of most state-\nof-the-art computer vision solutions for a wide variety of\ntasks. Since 2014 very deep convolutional networks started\nto become mainstream, yielding substantial gains in vari-\nous benchmarks. Although increased model size and com-\nputational cost tend to translate to immediate quality gains\nfor most tasks (as long as enough labeled data is provided\nfor training), computational efﬁciency and low parameter\ncount are still enabling factors for various use cases such as\nmobile vision and big-data scenarios. Here we are explor-\ning ways to scale up networks in ways that aim at utilizing\nthe added computation as efﬁciently as possible by suitably\nfactorized convolutions and aggressive regularization. We\nbenchmark our methods on the ILSVRC 2012 classiﬁcation\nchallenge validation set demonstrate substantial gains over\nthe state of the art: 21.2% top-1 and 5.6% top-5 error for\nsingle frame evaluation using a network with a computa-\ntional cost of 5 billion multiply-adds per inference and with\nusing less than 25 million parameters. With an ensemble of\n4 models and multi-crop evaluation, we report 3.5% top-5\nerror and 17.3% top-1 error.",
  "introduction": "Since the 2012 ImageNet competition [16] winning en-\ntry by Krizhevsky et al [9], their network “AlexNet” has\nbeen successfully applied to a larger variety of computer\nvision tasks, for example to object-detection [5], segmen-\ntation [12], human pose estimation [22], video classiﬁca-\ntion [8], object tracking [23], and superresolution [3].\nThese successes spurred a new line of research that fo-\ncused on ﬁnding higher performing convolutional neural\nnetworks. Starting in 2014, the quality of network architec-\ntures signiﬁcantly improved by utilizing deeper and wider\nnetworks. VGGNet [18] and GoogLeNet [20] yielded simi-\n larly high performance in the 2014 ILSVRC [16] classiﬁca-\ntion challenge. One interesting observation was that gains\nin the classiﬁcation performance tend to transfer to signiﬁ-\ncant quality gains in a wide variety of application domains.\nThis means that architectural improvements in deep con-\nvolutional architecture can be utilized for improving perfor-\nmance for most other computer vision tasks that are increas-\ningly reliant on high quality, learned visual features. Also,\nimprovements in the network quality resulted in new appli-\ncation domains for convolutional networks in cases where\nAlexNet features could not compete with hand engineered,\ncrafted solutions, e.g. proposal generation in detection[4].\n Although VGGNet [18] has the compelling feature of\narchitectural simplicity, this comes at a high cost: evalu-\nating the network requires a lot of computation. On the\nother hand, the Inception architecture of GoogLeNet [20]\nwas also designed to perform well even under strict con-\nstraints on memory and computational budget. For exam-\nple, GoogleNet employed only 5 million parameters, which\nrepresented a 12× reduction with respect to its predeces-\nsor AlexNet, which used 60 million parameters. Further-\nmore, VGGNet employed about 3x more parameters than\nAlexNet.\n The computational cost of Inception is also much lower\nthan VGGNet or its higher performing successors [6]. This\nhas made it feasible to utilize Inception networks in big-data\nscenarios[17], [13], where huge amount of data needed to\nbe processed at reasonable cost or scenarios where memory\nor computational capacity is inherently limited, for example\nin mobile vision settings. It is certainly possible to mitigate\nparts of these issues by applying specialized solutions to tar-\nget memory use [2], [15] or by optimizing the execution of\ncertain operations via computational tricks [10]. However,\nthese methods add extra complexity. Furthermore, these\nmethods could be applied to optimize the Inception archi-\ntecture as well, widening the efﬁciency gap again.\n Still, the complexity of the Inception architecture makes\n 1\n arXiv:1512.00567v3 [cs.CV] 11 Dec 2015\n\nit more difﬁcult to make changes to the network. If the ar-\nchitecture is scaled up naively, large parts of the computa-\ntional gains can be immediately lost. Also, [20] does not\nprovide a clear description about the contributing factors\nthat lead to the various design decisions of the GoogLeNet\narchitecture. This makes it much harder to adapt it to new\nuse-cases while maintaining its efﬁciency.\nFor example,\nif it is deemed necessary to increase the capacity of some\nInception-style model, the simple transformation of just\ndoubling the number of all ﬁlter bank sizes will lead to a\n4x increase in both computational cost and number of pa-\nrameters. This might prove prohibitive or unreasonable in a\nlot of practical scenarios, especially if the associated gains\nare modest. In this paper, we start with describing a few\ngeneral principles and optimization ideas that that proved\nto be useful for scaling up convolution networks in efﬁcient\nways. Although our principles are not limited to Inception-\ntype networks, they are easier to observe in that context as\nthe generic structure of the Inception style building blocks\nis ﬂexible enough to incorporate those constraints naturally.\nThis is enabled by the generous use of dimensional reduc-\ntion and parallel structures of the Inception modules which\nallows for mitigating the impact of structural changes on\nnearby components. Still, one needs to be cautious about\ndoing so, as some guiding principles should be observed to\nmaintain high quality of the models.",
  "background": "",
  "related work": "",
  "methodology": "We have trained our networks with stochastic gradient\nutilizing the TensorFlow [1] distributed machine learning\nsystem using 50 replicas running each on a NVidia Kepler\nGPU with batch size 32 for 100 epochs. Our earlier experi-\nments used momentum [19] with a decay of 0.9, while our\nbest models were achieved using RMSProp [21] with de-\ncay of 0.9 and ϵ = 1.0. We used a learning rate of 0.045,\ndecayed every two epoch using an exponential rate of 0.94.\nIn addition, gradient clipping [14] with threshold 2.0 was\nfound to be useful to stabilize the training. Model evalua-\ntions are performed using a running average of the parame-\nters computed over time.",
  "proposed solution": "",
  "results": "A typical use-case of vision networks is for the the post-\nclassiﬁcation of detection, for example in the Multibox [4]\ncontext. This includes the analysis of a relative small patch\nof the image containing a single object with some context.\nThe tasks is to decide whether the center part of the patch\ncorresponds to some object and determine the class of the\nobject if it does. The challenge is that objects tend to be\nrelatively small and low-resolution. This raises the question\nof how to properly deal with lower resolution input.\nThe common wisdom is that models employing higher\nresolution receptive ﬁelds tend to result in signiﬁcantly im-\nproved recognition performance. However it is important to\ndistinguish between the effect of the increased resolution of\nthe ﬁrst layer receptive ﬁeld and the effects of larger model\ncapacitance and computation. If we just change the reso-\nlution of the input without further adjustment to the model,\nthen we end up using computationally much cheaper mod-\nels to solve more difﬁcult tasks. Of course, it is natural,\nthat these solutions loose out already because of the reduced\ncomputational effort. In order to make an accurate assess-\nment, the model needs to analyze vague hints in order to\nbe able to “hallucinate” the ﬁne details. This is computa-\ntionally costly. The question remains therefore: how much\n\nReceptive Field Size\nTop-1 Accuracy (single frame)\n79 × 79\n75.2%\n151 × 151\n76.4%\n299 × 299\n76.6%\n Table 2. Comparison of recognition performance when the size of\nthe receptive ﬁeld varies, but the computational cost is constant.\n does higher input resolution helps if the computational ef-\nfort is kept constant. One simple way to ensure constant\neffort is to reduce the strides of the ﬁrst two layer in the\ncase of lower resolution input, or by simply removing the\nﬁrst pooling layer of the network.\nFor this purpose we have performed the following three\nexperiments:\n 1. 299 × 299 receptive ﬁeld with stride 2 and maximum\npooling after the ﬁrst layer.\n 2. 151 × 151 receptive ﬁeld with stride 1 and maximum\npooling after the ﬁrst layer.\n 3. 79 × 79 receptive ﬁeld with stride 1 and without pool-\ning after the ﬁrst layer.\n All three networks have almost identical computational\ncost. Although the third network is slightly cheaper, the\ncost of the pooling layer is marginal and (within 1% of the\ntotal cost of the)network. In each case, the networks were\ntrained until convergence and their quality was measured on\nthe validation set of the ImageNet ILSVRC 2012 classiﬁca-\ntion benchmark. The results can be seen in table 2. Al-\nthough the lower-resolution networks take longer to train,\nthe quality of the ﬁnal result is quite close to that of their\nhigher resolution counterparts.\nHowever, if one would just naively reduce the network\nsize according to the input resolution, then network would\nperform much more poorly. However this would an unfair\ncomparison as we would are comparing a 16 times cheaper\nmodel on a more difﬁcult task.\nAlso these results of table 2 suggest, one might con-\nsider using dedicated high-cost low resolution networks for\nsmaller objects in the R-CNN [5] context.\n Table 3 shows the experimental results about the recog-\nnition performance of our proposed architecture (Inception-\nv2) as described in Section 6. Each Inception-v2 line shows\nthe result of the cumulative changes including the high-\nlighted new modiﬁcation plus all the earlier ones. Label\nSmoothing refers to method described in Section 7. Fac-\ntorized 7 × 7 includes a change that factorizes the ﬁrst\n7 × 7 convolutional layer into a sequence of 3 × 3 convo-\nlutional layers. BN-auxiliary refers to the version in which\n Network\nTop-1\n Error\n Top-5\n Error\n Cost\nBn Ops\nGoogLeNet [20]\n29%\n9.2%\n1.5\nBN-GoogLeNet\n26.8%\n-\n1.5\nBN-Inception [7]\n25.2%\n7.8\n2.0\nInception-v2\n23.4%\n-\n3.8\nInception-v2\nRMSProp\n23.1%\n6.3\n3.8\nInception-v2\nLabel Smoothing\n22.8%\n6.1\n3.8\nInception-v2\nFactorized 7 × 7\n21.6%\n5.8\n4.8\nInception-v2\nBN-auxiliary\n21.2%\n5.6%\n4.8\n Table 3. Single crop experimental results comparing the cumula-\ntive effects on the various contributing factors. We compare our\nnumbers with the best published single-crop inference for Ioffe at\nal [7]. For the “Inception-v2” lines, the changes are cumulative\nand each subsequent line includes the new change in addition to\nthe previous ones. The last line is referring to all the changes is\nwhat we refer to as “Inception-v3” below. Unfortunately, He et\nal [6] reports the only 10-crop evaluation results, but not single\ncrop results, which is reported in the Table 4 below.\n Network\nCrops\nEvaluated\n Top-5\nError\n Top-1\nError\nGoogLeNet [20]\n10\n-\n9.15%\nGoogLeNet [20]\n144\n-\n7.89%\nVGG [18]\n-\n24.4%\n6.8%\nBN-Inception [7]\n144\n22%\n5.82%\nPReLU [6]\n10\n24.27%\n7.38%\nPReLU [6]\n-\n21.59%\n5.71%\nInception-v3\n12\n19.47%\n4.48%\nInception-v3\n144\n18.77%\n4.2%\n Table 4. Single-model, multi-crop experimental results compar-\ning the cumulative effects on the various contributing factors. We\ncompare our numbers with the best published single-model infer-\nence results on the ILSVRC 2012 classiﬁcation benchmark.\n the fully connected layer of the auxiliary classiﬁer is also\nbatch-normalized, not just the convolutions. We are refer-\nring to the model in last row of Table 3 as Inception-v3 and\nevaluate its performance in the multi-crop and ensemble set-\ntings.\n All our evaluations are done on the 48238 non-\nblacklisted examples on the ILSVRC-2012 validation set,\nas suggested by [16]. We have evaluated all the 50000 ex-\namples as well and the results were roughly 0.1% worse in\ntop-5 error and around 0.2% in top-1 error. In the upcom-\ning version of this paper, we will verify our ensemble result\non the test set, but at the time of our last evaluation of BN-\nInception in spring [7] indicates that the test and validation\nset error tends to correlate very well.\n\nNetwork\nModels\nEvaluated\n Crops\nEvaluated\n Top-1\nError\n Top-5\nError\nVGGNet [18]\n2\n-\n23.7%\n6.8%\nGoogLeNet [20]\n7\n144\n-\n6.67%\nPReLU [6]\n-\n-\n-\n4.94%\nBN-Inception [7]\n6\n144\n20.1%\n4.9%\nInception-v3\n4\n144\n17.2%\n3.58%∗\n Table 5. Ensemble evaluation results comparing multi-model,\nmulti-crop reported results. Our numbers are compared with the\nbest published ensemble inference results on the ILSVRC 2012\nclassiﬁcation benchmark.\n∗All results, but the top-5 ensemble\nresult reported are on the validation set. The ensemble yielded\n3.46% top-5 error on the validation set.",
  "discussion": "",
  "conclusion": "We have provided several design principles to scale up\nconvolutional networks and studied them in the context of\nthe Inception architecture. This guidance can lead to high\nperformance vision networks that have a relatively mod-\nest computation cost compared to simpler, more monolithic\narchitectures. Our highest quality version of Inception-v3\nreaches 21.2%, top-1 and 5.6% top-5 error for single crop\nevaluation on the ILSVR 2012 classiﬁcation, setting a new\nstate of the art.\nThis is achieved with relatively modest\n(2.5×) increase in computational cost compared to the net-\nwork described in Ioffe et al [7]. Still our solution uses\nmuch less computation than the best published results based\non denser networks: our model outperforms the results of\nHe et al [6] – cutting the top-5 (top-1) error by 25% (14%)\nrelative, respectively – while being six times cheaper com-\nputationally and using at least ﬁve times less parameters\n(estimated).\nOur ensemble of four Inception-v3 models\nreaches 3.5% with multi-crop evaluation reaches 3.5% top-\n5 error which represents an over 25% reduction to the best\npublished results and is almost half of the error of ILSVRC\n2014 winining GoogLeNet ensemble.\nWe have also demonstrated that high quality results can\nbe reached with receptive ﬁeld resolution as low as 79×79.\nThis might prove to be helpful in systems for detecting rel-\natively small objects. We have studied how factorizing con-\nvolutions and aggressive dimension reductions inside neural\nnetwork can result in networks with relatively low computa-\ntional cost while maintaining high quality. The combination\nof lower parameter count and additional regularization with\nbatch-normalized auxiliary classiﬁers and label-smoothing\nallows for training high quality networks on relatively mod-\nest sized training sets."
}